NVDA
 
2026
 
Q1
 
Earnings
 
Call
 
Transcript
 
28
 
May
 
2025
 
 
 
Participants
 
Toshiya
 
Hari
 
executive
 
Colett e
 
Kress
 
executive
 
Jensen
 
Huang
 
executive
 
Joseph
 
Moor e
 
analyst
 
Vivek
 
Arya
 
analyst
 
Christ opher
 
Muse
 
analyst
 
Benjamin
 
Reitz es
 
analyst
 
Timothy
 
Arcuri
 
analyst
 
Jacob
 
Wilhelm
 
analyst
 
Call
 
transcript
 
Oper ator
 
Good
 
afternoon.
 
My
 
name
 
is
 
Sarah,
 
and
 
I
 
will
 
be
 
your
 
confer ence
 
operator
 
today .
 
At
 
this
 
time,
 
I
 
would
 
like
 
to
 
welcome
 
everyone
 
to
 
NVIDIA's
 
First
 
Quar ter
 
Fiscal
 
2026
 
Financial
 
Results
 
Confer ence
 
Call.
 
[Oper ator
 
Instructions]
 
Toshiya
 
Hari,
 
you
 
may
 
begin
 
your
 
confer ence.
 
Toshiya
 
Hari
 
Thank
 
you.
 
Good
 
afternoon,
 
everyone,
 
and
 
welcome
 
to
 
NVIDIA's
 
confer ence
 
call
 
for
 
the
 
ﬁrst
 
quarter
 
of
 
ﬁscal
 
2026.
 
With
 
me
 
today
 
from
 
NVIDIA
 
are
 
Jensen
 
Huang,
 
President
 
and
 
Chief
 
Executive
 
Oﬃcer;
 
and
 
Colett e
 
Kress,
 
Executive
 
Vice
 
President
 
and
 
Chief
 
Financial
 
Oﬃcer .
 
I'd
 
like
 
to
 
remind
 
you
 
that
 
our
 
call
 
is
 
being
 
webcast
 
live
 
on
 
NVIDIA's
 
Invest or
 
Relations
 
websit e.
 
The
 
webcast
 
will
 
be
 
available
 
for
 
replay
 
until
 
the
 
confer ence
 
call
 
to
 
discuss
 
our
 
ﬁnancial
 
results
 
for
 
the
 
second
 
quarter
 
of
 
ﬁscal
 
2026.
 
The
 
content
 
of
 
today's
 
call
 
is
 
NVIDIA's
 
property.
 
It
 
can
 
not
 
be
 
reproduced
 
or
 
transcribed
 
without
 
our
 
prior
 
written
 
consent.
 
During
 
this
 
call,
 
we
 
may
 
make
 
forward-looking
 
statements
 
based
 
on
 
current
 
expectations.
 
These
 
are
 
subject
 
to
 
a
 
number
 
of
 
signiﬁcant
 
risks
 
and
 
uncer tainties,
 
and
 
our
 
actual
 
results
 
may
 
diﬀer
 
materially .
 
For
 
a
 
discussion
 
of
 
factors
 
that
 
could
 
aﬀect
 
our
 
future
 
ﬁnancial
 
results
 
and
 
business,
 
please
 
refer
 
to
 
the
 
disclosur e
 
in
 
today's
 
earnings
 
release,
 
our
 
most
 
recent
 
Forms
 
10-K
 
and
 
10-Q
 
and
 
the
 
reports
 
that
 
we
 
may
 
ﬁle
 
on
 
Form
 
8-K
 
with
 
the
 
Securities
 
and
 
Exchange
 
Commission.
 
All
 
our
 
statements
 
are
 
made
 
as
 
of
 
today,
 
May
 
28,
 
2025,
 
based
 
on
 
information
 
currently
 
available
 
to
 
us.
 
Except
 
as
 
requir ed
 
by
 
law,
 
we
 
assume
 
no
 
obligation
 
to
 
updat e
 
any
 
such
 
statements.
 
During
 
this
 
call,
 
we
 
will
 
discuss
 
non-GAAP
 
ﬁnancial
 
measur es.
 
You
 
can
 
ﬁnd
 
a
 
reconciliation
 
of
 
these
 
non-GAAP
 
ﬁnancial
 
measur es
 
to
 
GAAP
 
ﬁnancial
 
measur es
 
in
 
our
 
CFO
 
commentar y,
 
which
 
is
 
posted
 
on
 
our
 
websit e.
 
With
 
that,
 
let
 
me
 
turn
 
the
 
call
 
over
 
to
 
Colett e.
 
Colett e
 
Kress
 
Thank
 
you,
 
Toshiya.
 
We
 
deliver ed
 
another
 
strong
 
quarter
 
with
 
revenue
 
of
 
$44
 
billion,
 
up
 
69%
 
year-over -year ,
 
exceeding
 
our
 
outlook
 
in
 
what
 
proved
 
to
 
be
 
a
 
challenging
 
operating
 
envir onment.
 
Data
 
Cent er
 
revenue
 
of
 
$39
 
billion
 
grew
 
73%
 
year-on-year .
 
AR
 
workloads
 
have
 
transitioned
 
strongly
 
to
 
inference
 
and
 
AI
 
factory
 
build-outs
 
are
 
driving
 
signiﬁcant
 
revenue.
 
Our
 
customers'
 
commitments
 
are
 
ﬁrm.
 
On
 
April
 
9,
 
the
 
U.S.
 
government
 
issued
 
new
 
export
 
contr ols
 
on
 
H20,
 
our
 
data
 
center
 
GPU
 
designed
 
speciﬁcally
 
for
 
the
 
China
 
mark et.
 
We
 
sold
 
H20
 
with
 
the
 
approval
 
of
 
the
 
previous
 
administr ation.
 
Although
 
our
 
H20
 
has
 
been
 
in
 
the
 
mark et
 
for
 
over
 
a
 
year
 
and
 
does
 
not
 
have
 
a
 
mark et
 
outside
 
of
 
China,
 
the
 
new
 
export
 
contr ols
 
on
 
H20
 
did
 
not
 
provide
 
a
 
grace
 
period
 
to
 
allow
 
us
 
to
 
sell
 
through
 
our
 
invent ory.
 
In
 
Q1,
 
we
 
recogniz ed
 
$4.6
 
billion
 
in
 
H20
 
revenue,
 
which
 
occurr ed
 
prior
 
to
 
April
 
9,
 
but
 
also
 
recogniz ed
 
a
 
$4.5
 
billion
 
charge
 
as
 
we
 
wrote
 
down
 
invent ory
 
and
 
purchase
 
obligations
 
tied
 
to
 
orders
 
we
 
had
 
received
 
prior
 
to
 
April
 
9.
 
We
 
were
 
unable
 
to
 
ship
 
$2.5
 
billion
 
in
 
H20
 
revenue
 
in
 
the
 
ﬁrst
 
quarter
 
due
 
to
 
the
 
new
 
export
 
contr ols.
 
The
 
$4.5
 
billion
 
charge
 
was
 
less
 
than
 
what
 
we
 
initially
 
anticipat ed
 
as
 
we
 
were
 
able
 
to
 
reuse
 
certain
 
materials.
 
We
 
are
 
still
 
evaluating
 
our
 
limited
 
options
 
to
 
supply
 
data
 
center
 
comput e
 
products
 
compliant
 
with
 
the
 
U.S.
 
government's
 
revised
 
export
 
contr ol
 
rules.
 
Losing
 
access
 
to
 
the
 
China
 
AI
 
acceler ator
 
mark et,
 
which
 
we
 
believe
 
will
 
grow
 
to
 
nearly
 
$50
 
billion,
 
would
 
have
 
a
 
material
 
adverse
 
impact
 
on
 
our
 
business
 
going
 
forward
 
and
 
beneﬁt
 
our
 
foreign
 
competit ors
 
in
 
China
 
and
 
worldwide.
 
Our
 
Blackwell
 
ramp,
 
the
 
fastest
 
in
 
our
 
company's
 
history,
 
drove
 
a
 
73%
 
year-on-year
 
increase
 
in
 
Data
 
Cent er
 
revenue.
 
Blackwell
 
contribut ed
 
nearly
 
70%
 
of
 
Data
 
Cent er
 
comput e
 
revenue
 
in
 
the
 
quarter
 
with
 
the
 
transition
 
from
 
Hopper
 
nearly
 
complet e.
 
The
 
introduction
 
of
 
GB200
 
NVL
 
was
 
a
 
fundamental
 
architectur al
 
change
 
to
 
enable
 
data
 
center-scale
 
workloads
 
and
 
to
 
achieve
 
the
 
lowest
 
cost
 
per
 
inference
 
token.
 
While
 
these
 
systems
 
are
 
comple x
 
to
 
build,
 
we
 
have
 
seen
 
a
 
signiﬁcant
 
improvement
 
in
 
manufacturing
 
yields,
 
and
 
rack
 
shipments
 
are
 
moving
 
to
 
strong
 
rates
 
to
 
end
 
customers.
 
GB200
 
NVL
 
racks
 
are
 
now
 
gener ally
 
available
 
for
 
motor
 
builders,
 
enterprises
 
and
 
sover eign
 
customers
 
to
 
develop
 
and
 
deploy
 
AI.
 
On
 
average,
 
major
 
hyperscalers
 
are
 
each
 
deploying
 
nearly
 
1,000
 
NVL72
 
racks
 
or
 
72,000
 
Blackwell
 
GPUs
 
per
 
week
 
and
 
are
 
on
 
track
 
to
 
further
 
ramp
 
output
 
this
 
quarter.
 
Microsoft,
 
for
 
example,
 
has
 
already
 
deployed
 
tens
 
of
 
thousands
 
of
 
Blackwell
 
GPUs
 
and
 
is
 
expect ed
 
to
 
ramp
 
to
 
hundr eds
 
of
 
thousands
 
of
 
GB200s
 
with
 
OpenAI
 
as
 
one
 
of
 
its
 
key
 
customers.
 
Key
 
learnings
 
from
 
the
 
GB200
 
ramp
 
will
 
allow
 
for
 
a
 
smooth
 
transition
 
to
 
the
 
next
 
phase
 
of
 
our
 
product
 
road
 
map,
 
Blackwell
 
Ultra.
 
Sampling
 
of
 
GB300
 
systems
 
began
 
earlier
 
this
 
month
 
at
 
the
 
major
 
CSPs,
 
and
 
we
 
expect
 
production
 
shipments
 
to
 
commence
 
later
 
this
 
quarter.
 
GB300
 
will
 
lever age
 
the
 
same
 
architectur e,
 
same
 
physical
 
footprint
 
and
 
the
 
same
 
electrical
 
and
 
mechanical
 
speciﬁcations
 
as
 
GB200.
 
The
 
GB300
 
drop-in
 
design
 
will
 
allow
 
CSPs
 
to
 
seamlessly
 
transition
 
their
 
systems
 
and
 
manufacturing
 
used
 
for
 
GB200
 
while
 
maintaining
 
high
 
yields.
 
GB300
 
GPUs
 
with
 
50%
 
more
 
HBM
 
will
 
deliver
 
another
 
50%
 
increase
 
in
 
dense
 
FP4
 
inference
 
comput e
 
performance
 
compar ed
 
to
 
the
 
B200.
 
We
 
remain
 
committ ed
 
to
 
our
 
annual
 
product
 
cadence
 
with
 
our
 
road
 
map
 
extending
 
through
 
2028,
 
tightly
 
aligned
 
with
 
the
 
multiple
 
year
 
planning
 
cycles
 
of
 
our
 
customers.
 
We
 
are
 
witnessing
 
a
 
sharp
 
jump
 
in
 
inference
 
demand.
 
OpenAI,
 
Microsoft
 
and
 
Google
 
are
 
seeing
 
a
 
step
 
function
 
leap
 
in
 
token
 
gener ation.
 
Microsoft
 
processed
 
over
 
100
 
trillion
 
tokens
 
in
 
Q1,
 
a
 
ﬁvefold
 
increase
 
on
 
a
 
year-over -year
 
basis.
 
This
 
exponential
 
growth
 
in
 
Azure
 
OpenAI
 
is
 
representative
 
of
 
strong
 
demand
 
for
 
Azure
 
AI
 
Foundr y
 
as
 
well
 
as
 
other
 
AI
 
services
 
across
 
Microsoft's
 
platform.
 
Inference
 
serving
 
startups
 
are
 
now
 
serving
 
models
 
using
 
B200,
 
tripling
 
their
 
token
 
gener ation
 
rate
 
and
 
corresponding
 
revenues
 
for
 
high-value
 
reasoning
 
models
 
such
 
as
 
DeepSeek-R1
 
as
 
reported
 
by
 
artiﬁcial
 
analysis.
 
NVIDIA
 
Dynamo
 
on
 
Blackwell
 
NVL72
 
turbochar ges
 
AI
 
inference
 
throughput
 
by
 
30x
 
for
 
the
 
new
 
reasoning
 
models
 
sweeping
 
the
 
industr y.
 
Developer
 
engagements
 
increased
 
with
 
adoption
 
ranging
 
from
 
LLM
 
providers
 
such
 
as
 
Perple xity
 
to
 
ﬁnancial
 
services
 
institutions
 
such
 
as
 
Capital
 
One,
 
who
 
reduced
 
agentic
 
chatbo x
 
latency
 
by
 
5x
 
with
 
Dynamo.
 
In
 
the
 
latest
 
ELMO
 
Perf
 
inference
 
results,
 
we
 
submitt ed
 
our
 
ﬁrst
 
results
 
using
 
GB200
 
NVL72,
 
delivering
 
up
 
to
 
30x
 
higher
 
inference
 
throughput
 
compar ed
 
to
 
our
 
[
 
8-GPU
 
]
 
H200
 
submission
 
on
 
the
 
challenging
 
Llama
 
3.1
 
benchmark.
 
This
 
feat
 
was
 
achieved
 
through
 
a
 
combination
 
of
 
tripling
 
the
 
performance
 
for
 
GPU
 
as
 
well
 
as
 
9x
 
more
 
GPUs
 
all
 
connect ed
 
on
 
a
 
single
 
NVLink
 
domain.
 
And
 
while
 
Blackwell
 
is
 
still
 
early
 
in
 
its
 
life
 
cycle,
 
softwar e
 
optimizations
 
have
 
already
 
improved
 
its
 
performance
 
by
 
1.5x
 
in
 
the
 
last
 
month
 
alone.
 
We
 
expect
 
to
 
continue
 
improving
 
the
 
performance
 
of
 
Blackwell
 
through
 
its
 
operational
 
life
 
as
 
we
 
have
 
done
 
with
 
Hopper
 
and
 
AMP
 
Pro.
 
For
 
example,
 
we
 
increased
 
the
 
inference
 
performance
 
of
 
Hopper
 
by
 
4x
 
over
 
2
 
years.
 
This
 
is
 
the
 
beneﬁt
 
of
 
NVIDIA's
 
programmable
 
CUD A
 
architectur e
 
and
 
rich
 
ecosyst em.
 
The
 
pace
 
and
 
scale
 
of
 
AI
 
factory
 
deployments
 
are
 
acceler ating
 
with
 
nearly
 
100
 
NVIDIA-power ed
 
AI
 
factories
 
in
 
ﬂight
 
this
 
quarter,
 
a
 
twofold
 
increase
 
year-over -year ,
 
with
 
the
 
average
 
number
 
of
 
GPUs
 
powering
 
each
 
factory
 
also
 
doubling
 
in
 
the
 
same
 
period.
 
And
 
more
 
AI
 
factory
 
projects
 
are
 
starting
 
across
 
industries
 
and
 
geogr aphies.
 
NVIDIA's
 
full
 
stack
 
architectur e
 
is
 
underpinning
 
AI
 
factory
 
deployments
 
as
 
industr y
 
leaders
 
like
 
AT&T,
 
BYD,
 
Capital
 
One,
 
Foxconn,
 
MediaT ek,
 
and
 
Telenor ,
 
are
 
strategically
 
vital
 
sover eign
 
clouds
 
like
 
those
 
recently
 
announced
 
in
 
Saudi
 
Arabia,
 
Taiwan
 
and
 
the
 
UAE.
 
We
 
have
 
a
 
line
 
of
 
sight
 
to
 
projects
 
requiring
 
tens
 
of
 
gigawatts
 
of
 
NVIDIA
 
AI
 
infrastructur e
 
in
 
the
 
not-t oo-distant
 
future.
 
The
 
transition
 
from
 
gener ative
 
to
 
agentic
 
AI,
 
AI
 
capable
 
of
 
receiving,
 
reasoning,
 
planning
 
and
 
acting
 
will
 
transform
 
every
 
industr y,
 
every
 
company
 
and
 
countr y.
 
We
 
envision
 
AI
 
agents
 
as
 
a
 
new
 
digital
 
workfor ce
 
capable
 
of
 
handling
 
tasks
 
ranging
 
from
 
customer
 
service
 
to
 
comple x
 
decision-making
 
processes.
 
We
 
introduced
 
the
 
Llama
 
Nemotr on
 
family
 
of
 
open
 
reasoning
 
models
 
designed
 
to
 
super charge
 
agentic
 
AI
 
platforms
 
for
 
enterprises.
 
Built
 
on
 
the
 
Llama
 
architectur e,
 
these
 
models
 
are
 
available
 
as
 
NIMs
 
or
 
NVIDIA
 
inference
 
micro
 
services
 
with
 
multiple
 
sizes
 
to
 
meet
 
diverse
 
deployment
 
needs.
 
Our
 
post
 
training
 
enhancements
 
have
 
yielded
 
a
 
20%
 
accur acy
 
boost
 
and
 
a
 
5x
 
increase
 
in
 
inference
 
speed,
 
leading
 
platform
 
companies,
 
including
 
Accentur e,
 
Cadence,
 
Deloitt e,
 
and
 
Microsoft
 
or
 
transforming
 
work
 
with
 
our
 
reasoning
 
models.
 
NVIDIA
 
NeMo
 
micro
 
services
 
are
 
gener ally
 
available
 
across
 
industries
 
are
 
being
 
lever aged
 
by
 
leading
 
enterprises
 
to
 
build,
 
optimiz e
 
and
 
scale
 
AI
 
applications.
 
With
 
NeMo,
 
Cisco
 
increased
 
model
 
accur acy
 
by
 
40%
 
and
 
improved
 
response
 
time
 
by
 
10x
 
in
 
its
 
code
 
assistant.
 
NASD AQ
 
realized
 
a
 
30%
 
improvement
 
in
 
accur acy
 
and
 
response
 
time
 
in
 
its
 
AI
 
platform's
 
search
 
capabilities.
 
And
 
Shell's
 
Custom
 
LLM
 
achieved
 
a
 
30%
 
increase
 
in
 
accur acy
 
when
 
trained
 
with
 
NVIDIA
 
NeMo.
 
NeMo's
 
parallelism,
 
techniques
 
acceler ated
 
model
 
training
 
time
 
by
 
20%
 
when
 
compar ed
 
to
 
other
 
frameworks.
 
We
 
also
 
announced
 
a
 
partnership
 
with
 
Yum!
 
Brands,
 
the
 
world's
 
largest
 
restaur ant
 
company
 
to
 
bring
 
NVIDIA
 
AI
 
to
 
500
 
of
 
its
 
restaur ants
 
this
 
year
 
and
 
expanding
 
to
 
61,000
 
restaur ants
 
over
 
time
 
to
 
streamline
 
order-taking,
 
optimiz e
 
operations
 
and
 
enhance
 
service
 
across
 
its
 
restaur ants.
 
For
 
AI-power ed
 
cybersecurity
 
leading
 
companies
 
like
 
Check
 
Point,
 
CrowdS trike
 
and
 
Paladin
 
Networks
 
are
 
using
 
NVIDIA's
 
AI
 
security
 
and
 
softwar e
 
stack
 
to
 
build,
 
optimiz e
 
and
 
secur e
 
agentic
 
workﬂows,
 
with
 
CrowdS trike
 
realizing
 
2x
 
faster
 
detection
 
triage
 
with
 
50%
 
less
 
comput e
 
cost.
 
Moving
 
to
 
networking.
 
Sequential
 
growth
 
in
 
networking
 
resumed
 
in
 
Q1
 
with
 
revenue
 
up
 
64%
 
quarter-over -quar ter
 
to
 
$5
 
billion.
 
Our
 
customers
 
continue
 
to
 
lever age
 
our
 
platform
 
to
 
eﬃciently
 
scale
 
up
 
and
 
scale
 
out
 
AI
 
factory
 
workloads.
 
We
 
created
 
the
 
world's
 
fastest
 
switch,
 
NVLink
 
for
 
scale
 
up,
 
our
 
NVLink
 
comput e
 
fabric
 
in
 
its
 
ﬁfth
 
gener ation,
 
oﬀers
 
14x
 
the
 
bandwidth
 
of
 
PCIe
 
Gen
 
5.
 
NVLink
 
72
 
carries
 
130
 
terabytes
 
per
 
second
 
of
 
bandwidth
 
in
 
a
 
single
 
rack,
 
equivalent
 
to
 
the
 
entirety
 
of
 
the
 
world's
 
peak
 
Internet
 
traﬃc.
 
NVLink
 
is
 
a
 
new
 
growth
 
vector
 
and
 
is
 
oﬀ
 
to
 
a
 
great
 
start
 
with
 
Q1
 
shipments
 
exceeding
 
$1
 
billion.
 
At
 
Comput ex,
 
we
 
announced
 
NVLink
 
Fusion.
 
Hyperscale
 
customers
 
can
 
now
 
build
 
semi-cust om
 
CCUs
 
and
 
acceler ators
 
that
 
connect
 
directly
 
to
 
the
 
NVIDIA
 
platform
 
with
 
NVLink.
 
We
 
are
 
now
 
enabling
 
key
 
partners,
 
including
 
ASIC
 
providers
 
such
 
as
 
MediaT ek,
 
Marvell,
 
Alchip
 
Technologies
 
and
 
Astera
 
Labs
 
as
 
well
 
as
 
CPU
 
suppliers,
 
such
 
as
 
Fujitsu
 
and
 
Qualcomm
 
to
 
lever age
 
and
 
relink
 
Fusion
 
to
 
connect
 
our
 
respective
 
ecosyst ems.
 
For
 
scale
 
out,
 
our
 
enhanced
 
Ethernet
 
oﬀerings
 
deliver ed
 
the
 
highest
 
throughput,
 
low
 
in
 
its
 
latency
 
networking
 
for
 
AI.
 
Spectrum-X
 
posted
 
strong
 
sequential
 
and
 
year-on-year
 
growth
 
and
 
is
 
now
 
annualizing
 
over
 
$8
 
billion
 
in
 
revenue.
 
Adoption
 
is
 
widespr ead
 
across
 
major
 
CSPs
 
and
 
consumer
 
Internet
 
companies,
 
including
 
CoreWeave,
 
Microsoft
 
Azure
 
and
 
Oracle
 
Cloud
 
and
 
xAI.
 
This
 
quarter,
 
we
 
added
 
Google
 
Cloud
 
and
 
Meta
 
to
 
the
 
growing
 
list
 
of
 
Spectrum-X
 
customers.
 
We
 
introduced
 
Spectrum-X
 
and
 
Quantum-X
 
silicon
 
photonics
 
switches
 
featuring
 
the
 
world's
 
most
 
advanced
 
co-packaged
 
optics.
 
These
 
platforms
 
will
 
enable
 
next-level
 
AI
 
factory
 
scaling
 
to
 
millions
 
of
 
DPUs
 
through
 
the
 
increasingly
 
power
 
eﬃciency
 
by
 
3.5x
 
and
 
network
 
resiliency
 
by
 
10x,
 
while
 
acceler ating
 
customer
 
time
 
to
 
mark et
 
by
 
1.3x.
 
Transitioning
 
to
 
a
 
quick
 
summar y
 
of
 
our
 
revenue
 
by
 
geogr aphy .
 
China
 
as
 
a
 
percentage
 
of
 
our
 
Data
 
Cent er
 
revenue
 
was
 
slightly
 
below
 
our
 
expectations
 
and
 
down
 
sequentially
 
due
 
to
 
H20
 
export
 
licensing
 
contr ols.
 
For
 
Q2,
 
we
 
expect
 
a
 
meaningful
 
decrease
 
in
 
China
 
data
 
center
 
revenue.
 
As
 
a
 
reminder ,
 
while
 
Singapor e
 
represent ed
 
nearly
 
20%
 
of
 
our
 
Q1
 
build
 
revenue
 
as
 
many
 
of
 
our
 
large
 
customers
 
use
 
Singapor e
 
for
 
centr alized
 
invoicing,
 
our
 
products
 
are
 
almost
 
always
 
shipped
 
elsewher e.
 
Note
 
that
 
over
 
99%
 
of
 
H100,
 
H200,
 
and
 
Blackwell
 
data
 
center
 
comput e
 
revenue
 
billed
 
to
 
Singapor e
 
was
 
for
 
orders
 
from
 
U.S.-based
 
customers.
 
Moving
 
to
 
gaming
 
and
 
AI
 
PCs.
 
Gaming
 
revenue
 
was
 
a
 
record
 
$3.8
 
billion,
 
increasing
 
48%
 
sequentially
 
and
 
42%
 
year-on-year .
 
Strong
 
adoption
 
by
 
gamers,
 
creatives
 
and
 
AI
 
enthusiasts
 
have
 
made
 
Blackwell
 
our
 
fastest
 
ramp
 
ever.
 
Against
 
a
 
backdr op
 
of
 
robust
 
demand,
 
we
 
greatly
 
improved
 
our
 
supply
 
and
 
availability
 
in
 
Q1
 
and
 
expect
 
to
 
continue
 
these
 
eﬀorts
 
in
 
Q2.
 
AI
 
is
 
transforming
 
PC
 
and
 
creator
 
and
 
gamers.
 
With
 
a
 
100
 
million
 
user
 
installed
 
base,
 
represents
 
the
 
largest
 
footprint
 
for
 
PC
 
developers.
 
This
 
quarter,
 
we
 
added
 
to
 
our
 
AI
 
PC
 
laptop
 
oﬀerings,
 
including
 
models
 
capable
 
of
 
running
 
Microsoft's
 
Copilot+.
 
This
 
past
 
quarter,
 
we
 
brought
 
Blackwell
 
architectur e
 
to
 
mainstr eam
 
gaming
 
with
 
its
 
launch
 
of
 
GeForce
 
RTX
 
5060
 
and
 
5060
 
Ti
 
starting
 
at
 
just
 
$299.
 
The
 
RTX
 
5060
 
also
 
debut ed
 
in
 
laptop
 
starting
 
at
 
$1,099.
 
These
 
systems
 
that
 
doubled
 
the
 
frame
 
rate/latency.
 
These
 
GeForce
 
RTX
 
50,
 
60
 
and
 
50-60TI
 
deskt op
 
GPUs
 
and
 
laptops
 
are
 
now
 
available.
 
In
 
console
 
gaming,
 
the
 
recently
 
unveiled
 
Nintendo
 
Switch
 
2
 
lever ages
 
NVIDIA's
 
neuro
 
rendering
 
and
 
AI
 
technologies,
 
including
 
next-gener ation
 
custom
 
RTX
 
GPUs
 
with
 
DLSS
 
technology
 
to
 
deliver
 
a
 
giant
 
leap
 
in
 
gaming
 
performance
 
to
 
millions
 
of
 
players
 
worldwide.
 
Nintendo
 
has
 
shipped
 
over
 
150
 
million
 
switch
 
consoles
 
to
 
date,
 
making
 
it
 
one
 
of
 
the
 
most
 
successful
 
gaming
 
systems
 
in
 
history.
 
Moving
 
to
 
Pro
 
Visualization.
 
Revenue
 
of
 
$509
 
million
 
was
 
ﬂat
 
sequentially
 
and
 
up
 
19%
 
year-on-year .
 
Tariﬀ-r elated
 
uncer tainty
 
tempor arily
 
impact ed
 
Q1
 
systems
 
and
 
demand
 
for
 
our
 
AI
 
workstations
 
is
 
strong,
 
and
 
we
 
expect
 
sequential
 
revenue
 
growth
 
to
 
resume
 
in
 
Q2.
 
NVIDIA
 
DGX
 
Spark
 
and
 
station
 
revolutioniz ed
 
personal
 
computing.
 
By
 
putting
 
the
 
power
 
of
 
an
 
AI
 
super comput er
 
in
 
a
 
deskt op
 
form
 
factor.
 
DGX
 
Spark
 
delivers
 
up
 
to
 
1
 
petaﬂop
 
of
 
AI
 
comput e
 
while
 
DGX
 
Station
 
oﬀers
 
an
 
incredible
 
20
 
petaﬂops
 
and
 
is
 
power ed
 
by
 
the
 
GB300
 
Super
 
Chip.
 
DGX
 
Spark
 
will
 
be
 
available
 
in
 
calendar
 
Q3
 
and
 
DGX
 
Station
 
later
 
this
 
year.
 
We
 
have
 
deepened
 
Omni
 
versus
 
integration
 
and
 
adoption
 
into
 
some
 
of
 
the
 
world's
 
leading
 
softwar e
 
platforms,
 
including
 
Databricks,
 
SAP
 
and
 
Schneider
 
Electric,
 
new
 
Omniverse
 
blueprints
 
such
 
as
 
Mega
 
for
 
at-scale
 
robotic
 
ﬂeet
 
management
 
are
 
being
 
lever aged
 
in
 
Kion
 
Group,
 
Pegatr on,
 
Accentur e
 
and
 
other
 
leading
 
companies
 
to
 
enhance
 
industrial
 
operations.
 
At
 
Comput ex,
 
we
 
showcased
 
Omni
 
versus
 
great
 
traction
 
with
 
technology
 
manufacturing
 
leaders,
 
including
 
TSMC,
 
Quanta,
 
Foxconn,
 
Pegatr on.
 
Using
 
Omniverse,
 
TSMC
 
saves
 
months
 
in
 
work
 
by
 
designing
 
fabs
 
virtually .
 
Foxconn
 
acceler ates
 
thermal
 
simulations
 
by
 
150x,
 
and
 
Pegatr on
 
reduced
 
assembly
 
line
 
defect
 
rates
 
by
 
67%.
 
Lastly
 
with
 
our
 
automotive
 
group.
 
Revenue
 
was
 
$567
 
million,
 
down
 
1%
 
sequentially
 
but
 
up
 
72%
 
year-on-year .
 
Year-on-year
 
growth
 
was
 
driven
 
by
 
the
 
ramp
 
of
 
self-driving
 
across
 
a
 
number
 
of
 
customers
 
and
 
robust
 
end
 
demand
 
for
 
NAVs.
 
We
 
are
 
partnering
 
with
 
GM
 
to
 
build
 
the
 
next-gen
 
vehicles,
 
factories
 
and
 
robots
 
using
 
NVIDIA
 
AI,
 
simulation
 
and
 
acceler ated
 
computing.
 
And
 
we
 
are
 
now
 
in
 
production
 
with
 
our
 
full
 
stack
 
solution
 
for
 
Mercedes-Benz
 
starting
 
with
 
the
 
new
 
CLA
 
hitting
 
roads
 
in
 
the
 
next
 
few
 
months.
 
We
 
announced
 
Isaac
 
Group
 
and
 
one,
 
the
 
world's
 
ﬁrst
 
open
 
fully
 
customizable
 
foundation
 
model
 
for
 
humanoid
 
robots
 
enabling
 
gener alized
 
reasoning
 
and
 
skill
 
development.
 
We
 
also
 
launched
 
new
 
open
 
NVIDIA
 
Cosmo
 
World
 
Foundation
 
models.
 
Leading
 
companies
 
include
 
[
 
OneX
 
],
 
Agility
 
Robots,
 
Robotics,
 
Figur e
 
AI,
 
Uber
 
and
 
Wobi.
 
We've
 
begun
 
integrating
 
Kosmos
 
into
 
their
 
operations
 
for
 
synthetic
 
data
 
gener ation,
 
while
 
Agility
 
Robotics,
 
Boston
 
Dynamics,
 
and
 
Robotics
 
are
 
harnessing
 
Isaac's
 
simulation
 
to
 
advance
 
their
 
humanoid
 
eﬀorts.
 
GE
 
Healthcar e
 
is
 
using
 
the
 
new
 
NVIDIA
 
Isaac
 
platform
 
for
 
health
 
care
 
simulation
 
built
 
on
 
NVIDIA
 
Omniverse
 
and
 
using
 
NVIDIA
 
Cosmos.
 
The
 
platform
 
speed,
 
development
 
of
 
robotic
 
imaging
 
and
 
surgery
 
systems.
 
The
 
era
 
of
 
robotics
 
is
 
here,
 
billions
 
of
 
robots,
 
hundr eds
 
of
 
millions
 
of
 
autonomous
 
vehicles
 
and
 
hundr eds
 
of
 
thousands
 
of
 
robotic
 
factories
 
and
 
warehouses
 
will
 
be
 
developed.
 
All
 
right.
 
Moving
 
to
 
the
 
rest
 
of
 
the
 
P&L.
 
GAAP
 
gross
 
margins
 
and
 
non-GAAP
 
gross
 
margins
 
were
 
60.5%
 
and
 
61%,
 
respectively .
 
Excluding
 
the
 
$4.5
 
billion
 
charge,
 
Q1
 
non-GAAP
 
gross
 
margins
 
would
 
have
 
been
 
71.3%,
 
slightly
 
above
 
our
 
outlook
 
at
 
the
 
beginning
 
of
 
the
 
quarter.
 
Sequentially,
 
GAAP
 
operating
 
expenses
 
were
 
up
 
7%
 
and
 
non-GAAP
 
operating
 
expenses
 
were
 
up
 
6%,
 
reﬂecting
 
higher
 
compensation
 
and
 
employee
 
growth.
 
Our
 
investments
 
include
 
expanding
 
our
 
infrastructur e
 
capabilities
 
and
 
AI
 
solutions,
 
and
 
we
 
plan
 
to
 
grow
 
these
 
investments
 
throughout
 
the
 
ﬁscal
 
year.
 
In
 
Q1,
 
we
 
returned
 
a
 
record
 
$14.3
 
billion
 
to
 
shareholders
 
in
 
the
 
form
 
of
 
share
 
repurchases
 
and
 
cash
 
dividends.
 
Our
 
capital
 
return
 
program
 
continues
 
to
 
be
 
a
 
key
 
element
 
of
 
our
 
capital
 
allocation
 
strategy.
 
Let
 
me
 
turn
 
to
 
the
 
outlook
 
for
 
the
 
second
 
quarter.
 
Total
 
revenue
 
is
 
expect ed
 
to
 
be
 
$45
 
billion,
 
plus
 
or
 
minus
 
2%.
 
We
 
expect
 
modest
 
sequential
 
growth
 
across
 
all
 
of
 
our
 
platforms.
 
In
 
Data
 
Cent er,
 
we
 
anticipat e
 
the
 
continued
 
ramp
 
of
 
Blackwell
 
to
 
be
 
partially
 
oﬀset
 
by
 
a
 
decline
 
in
 
China
 
revenue.
 
Note,
 
our
 
outlook
 
reﬂects
 
a
 
loss
 
in
 
H20
 
revenue
 
of
 
approximat ely
 
$8
 
billion
 
for
 
the
 
second
 
quarter.
 
GAAP
 
and
 
non-GAAP
 
gross
 
margins
 
are
 
expect ed
 
to
 
be
 
71.8%
 
and
 
72%,
 
respectively,
 
plus
 
or
 
minus
 
50
 
basis
 
points.
 
We
 
expect
 
or
 
Blackwell
 
proﬁtability
 
to
 
drive
 
modest
 
sequential
 
improvement
 
in
 
gross
 
margins.
 
We
 
are
 
continuing
 
to
 
work
 
towar ds
 
achieving
 
gross
 
margins
 
in
 
the
 
mid-70s
 
range
 
late
 
this
 
year.
 
GAAP
 
and
 
non-GAAP
 
operating
 
expenses
 
are
 
expect ed
 
to
 
be
 
approximat ely
 
$5.7
 
billion
 
and
 
$4
 
billion,
 
respectively,
 
and
 
we
 
continue
 
to
 
expect
 
full
 
year
 
ﬁscal
 
year
 
'26
 
operating
 
expense
 
growth
 
to
 
be
 
in
 
the
 
mid-30%
 
range.
 
GAAP
 
and
 
non-GAAP
 
other
 
income
 
and
 
expenses
 
are
 
expect ed
 
to
 
be
 
an
 
income
 
of
 
approximat ely
 
$450
 
million,
 
excluding
 
gains
 
and
 
losses
 
from
 
nonmark etable
 
and
 
publicly
 
held
 
equity
 
securities.
 
GAAP
 
and
 
non-GAAP
 
tax
 
rates
 
are
 
expect ed
 
to
 
be
 
16.5%,
 
plus
 
or
 
minus
 
1%,
 
excluding
 
any
 
discr ete
 
items.
 
Further
 
ﬁnancial
 
details
 
are
 
included
 
in
 
the
 
CFO
 
commentar y
 
and
 
other
 
information
 
available
 
on
 
our
 
IR
 
websit e,
 
including
 
a
 
new
 
ﬁnancially
 
information
 
AI
 
agent.
 
Let
 
me
 
highlight
 
upcoming
 
events
 
for
 
the
 
ﬁnancial
 
community .
 
We
 
will
 
be
 
at
 
the
 
BofA
 
Global
 
Technology
 
Confer ence
 
in
 
San
 
Francisco
 
on
 
June
 
4.
 
The
 
Rosenblatt
 
Virtual
 
AI
 
Summit
 
and
 
NASD AQ
 
Invest or
 
Confer ence
 
in
 
London
 
on
 
June
 
10,
 
and
 
GTC
 
Paris
 
at
 
VivaTech
 
on
 
June
 
11
 
in
 
Paris.
 
We
 
look
 
forward
 
to
 
seeing
 
you
 
at
 
these
 
events.
 
Our
 
earnings
 
call
 
to
 
discuss
 
the
 
results
 
of
 
our
 
second
 
quarter
 
of
 
ﬁscal
 
2026
 
is
 
scheduled
 
for
 
August
 
27.
 
Well,
 
now
 
let
 
me
 
turn
 
it
 
over
 
to
 
Jensen
 
to
 
make
 
some
 
remarks.
 
Jensen
 
Huang
 
Thanks,
 
Colett e.
 
We've
 
had
 
a
 
busy
 
and
 
productive
 
year.
 
Let
 
me
 
share
 
my
 
perspective
 
on
 
some
 
topics
 
we're
 
frequently
 
asked.
 
On
 
export
 
contr ol.
 
China
 
is
 
one
 
of
 
the
 
world's
 
largest
 
AI
 
mark ets
 
and
 
a
 
springboar d
 
to
 
global
 
success.
 
With
 
half
 
of
 
the
 
world's
 
AI
 
resear chers
 
based
 
there,
 
the
 
platform
 
that
 
wins
 
China
 
is
 
positioned
 
to
 
lead
 
globally .
 
Today,
 
however ,
 
the
 
$50
 
billion
 
China
 
mark et
 
is
 
eﬀectively
 
closed
 
to
 
U.S.
 
industr y.
 
The
 
H20
 
export
 
ban
 
ended
 
our
 
hopper
 
data
 
center
 
business
 
in
 
China.
 
We
 
cannot
 
reduce
 
hopper
 
further
 
to
 
comply .
 
As
 
a
 
result,
 
we
 
are
 
taking
 
a
 
multibillion-dollar
 
write-oﬀ
 
on
 
invent ory
 
that
 
cannot
 
be
 
sold
 
or
 
repurposed.
 
We
 
are
 
exploring
 
limited
 
ways
 
to
 
compet e,
 
but
 
Hopper
 
is
 
no
 
longer
 
an
 
option.
 
China's
 
AI
 
moves
 
on
 
with
 
or
 
without
 
U.S.
 
chips.
 
It
 
has
 
to
 
comput e
 
to
 
train
 
and
 
deploy
 
advanced
 
models.
 
The
 
question
 
is
 
not
 
whether
 
China
 
will
 
have
 
AI,
 
it
 
already
 
does.
 
The
 
question
 
is
 
whether
 
one
 
of
 
the
 
world's
 
largest
 
AR
 
mark ets
 
will
 
run
 
on
 
American
 
platforms.
 
Shielding
 
Chinese
 
chipmak ers
 
from
 
U.S.
 
competition
 
only
 
strengthens
 
them
 
abroad
 
and
 
weak ens
 
America's
 
position.
 
Expor t
 
restrictions
 
have
 
spurr ed
 
China's
 
innovation
 
and
 
scale.
 
The
 
AI
 
race
 
is
 
not
 
just
 
about
 
chips.
 
It's
 
about
 
which
 
stack
 
the
 
world
 
runs
 
on.
 
As
 
that
 
stack
 
grows
 
to
 
include
 
6G
 
and
 
quantum,
 
U.S.
 
global
 
infrastructur e
 
leadership
 
is
 
at
 
stake.
 
The
 
U.S.
 
has
 
based
 
its
 
policy
 
on
 
the
 
assumption
 
that
 
China
 
cannot
 
make
 
AI
 
chips.
 
That
 
assumption
 
was
 
always
 
questionable
 
and
 
now
 
it's
 
clearly
 
wrong.
 
China
 
has
 
enormous
 
manufacturing
 
capability .
 
In
 
the
 
end,
 
the
 
platform
 
that
 
wins
 
the
 
AI
 
developers
 
win
 
AI
 
--
 
wins
 
AI.
 
Expor t
 
contr ols
 
should
 
strengthen
 
U.S.
 
platforms,
 
not
 
drive
 
half
 
of
 
the
 
world's
 
AI
 
talent
 
to
 
rivals.
 
On
 
DeepSeek,
 
DeepSeek
 
and
 
Q1
 
from
 
China
 
are
 
among
 
the
 
most
 
--
 
among
 
the
 
best
 
open
 
source
 
models.
 
Released
 
freely,
 
they've
 
gained
 
traction
 
across
 
the
 
U.S.,
 
Europe
 
and
 
beyond.
 
DeepSeek
 
R1,
 
like
 
ChatGPT ,
 
introduced
 
reasoning
 
AI
 
that
 
produces
 
better
 
answers,
 
the
 
longer
 
it
 
thinks.
 
Reasoning
 
AI
 
enables
 
step-by-st ep
 
problem
 
solving,
 
planning
 
and
 
tool
 
use,
 
turning
 
models
 
into
 
intelligent
 
agents.
 
Reasoning
 
is
 
comput e-intensive,
 
requir es
 
hundr eds
 
to
 
thousands
 
more
 
thousands
 
of
 
times
 
more
 
tokens
 
per
 
task
 
than
 
previous
 
one-shot
 
inference.
 
Reasoning
 
models
 
are
 
driving
 
a
 
step-function
 
surge
 
in
 
inference
 
demand.
 
AI
 
scaling
 
laws
 
remain
 
ﬁrmly
 
intact,
 
not
 
only
 
for
 
training,
 
but
 
now
 
Inference
 
2
 
requir es
 
massive
 
scale
 
comput e.
 
DeepSeek
 
also
 
underscor es
 
the
 
strategic
 
value
 
of
 
open
 
source
 
AI.
 
When
 
popular
 
models
 
are
 
trained
 
and
 
optimiz ed
 
on
 
U.S.
 
platforms,
 
it
 
drives
 
usage,
 
feedback
 
and
 
continuous
 
improvement,
 
reinfor cing
 
American
 
leadership
 
across
 
the
 
stack.
 
U.S.
 
platforms
 
must
 
remain
 
the
 
preferred
 
platform
 
for
 
open
 
source
 
AI.
 
That
 
means
 
suppor ting
 
collabor ation
 
with
 
top
 
developers
 
globally,
 
including
 
in
 
China.
 
America
 
wins
 
when
 
models
 
like
 
DeepSeek
 
and
 
Q1
 
runs
 
best
 
on
 
American
 
infrastructur e.
 
Regar ding
 
onshor e
 
manufacturing,
 
President
 
Trump
 
has
 
outlined
 
a
 
bold
 
vision
 
to
 
reshor e
 
advanced
 
manufacturing,
 
create
 
jobs
 
and
 
strengthen
 
national
 
security .
 
Future
 
plants
 
will
 
be
 
highly
 
comput erized
 
in
 
robotics.
 
We
 
share
 
this
 
vision.
 
TSMC
 
is
 
building
 
6
 
fabs
 
and
 
2
 
advanced
 
packaging
 
plants
 
in
 
Arizona
 
to
 
make
 
chips
 
for
 
NVIDIA.
 
Process
 
qualiﬁcation
 
is
 
under way
 
with
 
volume
 
production
 
expect ed
 
by
 
year-end.
 
SPIL
 
and
 
Amcor
 
are
 
also
 
investing
 
in
 
Arizona,
 
constructing
 
packaging,
 
assembly
 
and
 
test
 
facilities.
 
In
 
Houst on,
 
we're
 
partnering
 
with
 
Foxconn
 
to
 
construct
 
a
 
1
 
million
 
squar e
 
foot
 
factory
 
to
 
build
 
AI
 
super comput ers.
 
Wistron
 
is
 
building
 
a
 
similar
 
plant
 
in
 
Fort
 
Worth,
 
Texas.
 
To
 
encour age
 
and
 
suppor t
 
these
 
investments,
 
we've
 
made
 
substantial
 
long-t erm
 
purchase
 
commitments
 
a
 
deep
 
investment
 
in
 
America's
 
AI
 
manufacturing
 
future.
 
Our
 
goal
 
from
 
chip
 
to
 
super comput er
 
built
 
in
 
America
 
within
 
a
 
year.
 
Each
 
GB200
 
NVLink72
 
racks
 
contains
 
1.2
 
million
 
components
 
and
 
weighs
 
nearly
 
2
 
tons.
 
No
 
1
 
has
 
produced
 
super comput ers
 
on
 
this
 
scale.
 
Our
 
partners
 
are
 
doing
 
an
 
extraordinar y
 
job.
 
On
 
AI
 
diﬀusion
 
rule,
 
President
 
Trump
 
rescinded
 
the
 
AI
 
diﬀusion
 
rule,
 
calling
 
it
 
count erproductive,
 
and
 
proposed
 
a
 
new
 
policy
 
to
 
promot e
 
U.S.
 
AI
 
tech
 
with
 
trusted
 
partners.
 
On
 
his
 
Middle
 
East
 
tour,
 
he
 
announced
 
historic
 
investments.
 
I
 
was
 
honor ed
 
to
 
join
 
him
 
in
 
announcing
 
a
 
500-megawatt
 
AI
 
infrastructur e
 
project
 
in
 
Saudi
 
Arabia
 
and
 
a
 
5-gigawatt
 
AI
 
campus
 
in
 
the
 
UAE.
 
President
 
Trump
 
wants
 
U.S.
 
tech
 
to
 
lead.
 
The
 
deals
 
he
 
announced
 
are
 
wins
 
for
 
America,
 
creating
 
jobs,
 
advancing
 
infrastructur e,
 
gener ating
 
tax
 
revenue
 
and
 
reducing
 
the
 
U.S.
 
trade
 
deﬁcit.
 
The
 
U.S.
 
will
 
always
 
be
 
NVIDIA's
 
largest
 
mark et
 
and
 
home
 
to
 
the
 
largest
 
installed
 
base
 
of
 
our
 
infrastructur e.
 
Every
 
nation
 
now
 
sees
 
AI
 
as
 
core
 
to
 
the
 
next
 
industrial
 
revolution,
 
a
 
new
 
industr y
 
that
 
produces
 
intelligence
 
and
 
essential
 
infrastructur e
 
for
 
every
 
economy .
 
Countries
 
are
 
racing
 
to
 
build
 
national
 
AI
 
platforms
 
to
 
elevat e
 
their
 
digital
 
capabilities.
 
At
 
Comput ex,
 
we
 
announced
 
Taiwan's
 
ﬁrst
 
AI
 
factory
 
in
 
partnership
 
with
 
Foxconn
 
and
 
the
 
Taiwan
 
government.
 
Last
 
week,
 
I
 
was
 
in
 
Sweden
 
to
 
launch
 
its
 
ﬁrst
 
national
 
AI
 
infrastructur e.
 
Japan,
 
Korea,
 
India,
 
Canada,
 
France,
 
the
 
U.K.,
 
Germany,
 
Italy,
 
Spain,
 
and
 
more
 
are
 
now
 
building
 
national
 
AI
 
factories
 
to
 
empower
 
startups,
 
industries
 
and
 
societies.
 
Sover eign
 
AI
 
is
 
a
 
new
 
growth
 
engine
 
for
 
NVIDIA.
 
Toshiya,
 
back
 
to
 
you.
 
Toshiya
 
Hari
 
Oper ator,
 
we
 
will
 
now
 
open
 
the
 
call
 
for
 
questions.
 
Would
 
you
 
please
 
poll
 
for
 
questions?
 
Oper ator
 
[Oper ator
 
Instructions]
 
Your
 
ﬁrst
 
question
 
comes
 
from
 
the
 
line
 
of
 
Joe
 
Moor e
 
with
 
Morgan
 
Stanley .
 
Joseph
 
Moor e
 
You
 
guys
 
have
 
talked
 
about
 
this
 
scaling
 
up
 
of
 
inference
 
around
 
reasoning
 
models
 
for
 
at
 
least
 
a
 
year
 
now.
 
And
 
we've
 
really
 
seen
 
that
 
come
 
to
 
fruition
 
as
 
you
 
talked
 
about.
 
We've
 
heard
 
it
 
from
 
your
 
customers.
 
Can
 
you
 
give
 
us
 
a
 
sense
 
for
 
how
 
much
 
of
 
that
 
demand
 
you'r e
 
able
 
to
 
serve
 
and
 
give
 
us
 
a
 
sense
 
for
 
maybe
 
how
 
big
 
the
 
inference
 
business
 
is
 
for
 
you
 
guys?
 
And
 
do
 
we
 
need
 
full
 
on
 
NDL72
 
rack
 
scale
 
solutions
 
for
 
reasoning
 
inference
 
going
 
forward?
 
Jensen
 
Huang
 
Well,
 
we
 
would
 
like
 
to
 
serve
 
all
 
of
 
it,
 
and
 
I
 
think
 
we're
 
on
 
track
 
to
 
serve
 
most
 
of
 
it.
 
Grace
 
Blackwell
 
NVLink72
 
is
 
the
 
ideal
 
engine
 
today,
 
the
 
ideal
 
comput er
 
thinking
 
machine,
 
if
 
you
 
will,
 
for
 
reasoning
 
AI.
 
There's
 
a
 
couple
 
of
 
reasons
 
for
 
that.
 
The
 
ﬁrst
 
reason
 
is
 
that
 
the
 
token
 
gener ation
 
amount,
 
the
 
number
 
of
 
tokens
 
reasoning
 
goes
 
through,
 
is
 
100x,
 
1,000x
 
more
 
than
 
a
 
one-shot
 
chatbot.
 
It's
 
essentially
 
thinking
 
to
 
itself,
 
breaking
 
down
 
a
 
problem
 
step-by-st ep.
 
It
 
might
 
be
 
planning
 
multiple
 
paths
 
to
 
an
 
answer .
 
It
 
could
 
be
 
using
 
tools,
 
reading
 
PDFs,
 
reading
 
web
 
pages,
 
watching
 
videos
 
and
 
then
 
producing
 
a
 
result,
 
an
 
answer .
 
The
 
longer
 
it
 
thinks,
 
the
 
better
 
the
 
answer ,
 
the
 
smar ter
 
the
 
answer
 
is.
 
And
 
so
 
what
 
we
 
would
 
like
 
to
 
do,
 
and
 
the
 
reason
 
why
 
Grace
 
Blackwell
 
was
 
designed
 
to
 
give
 
such
 
a
 
giant
 
step-up
 
in
 
inference
 
performance,
 
is
 
so
 
that
 
you
 
could
 
do
 
all
 
this
 
and
 
still
 
get
 
a
 
response
 
as
 
quickly
 
as
 
possible.
 
Compar ed
 
to
 
Hopper ,
 
Grace
 
Blackwell
 
is
 
some
 
40x
 
higher
 
speed
 
and
 
throughput
 
compar ed.
 
And
 
so
 
this
 
is
 
going
 
to
 
be
 
a
 
huge,
 
huge
 
beneﬁt
 
in
 
driving
 
down
 
the
 
cost
 
while
 
improving
 
the
 
quality
 
of
 
response
 
with
 
excellent
 
quality
 
of
 
service
 
at
 
the
 
same
 
time.
 
So
 
that's
 
the
 
fundamental
 
reason.
 
That
 
was
 
the
 
core
 
driving
 
reason
 
for
 
Grace
 
Blackwell
 
NVLink
 
72.
 
Of
 
course,
 
in
 
order
 
to
 
do
 
that,
 
we
 
had
 
to
 
reinvent,
 
literally
 
redesign,
 
the
 
entire
 
--
 
a
 
way
 
that
 
these
 
super comput ers
 
are
 
built.
 
But
 
now
 
we're
 
in
 
full
 
production.
 
It's
 
going
 
to
 
be
 
exciting.
 
It's
 
going
 
to
 
be
 
incredibly
 
exciting.
 
Oper ator
 
The
 
next
 
question
 
comes
 
from
 
Vivek
 
Arya
 
with
 
Bank
 
of
 
America
 
Securities.
 
Vivek
 
Arya
 
Just
 
a
 
clariﬁcation
 
for
 
Colett e
 
ﬁrst.
 
So
 
on
 
the
 
China
 
impact,
 
I
 
think
 
previously,
 
it
 
was
 
mentioned
 
at
 
about
 
$15
 
billion,
 
so
 
you
 
had
 
the
 
$8
 
billion
 
in
 
Q2.
 
So
 
is
 
there
 
still
 
some
 
left
 
as
 
a
 
headwind
 
for
 
the
 
remaining
 
quarters
 
just
 
Colett e,
 
how
 
to
 
model
 
that?
 
And
 
then
 
a
 
question,
 
Jensen,
 
for
 
you.
 
Back
 
at
 
GTC,
 
you
 
had
 
outlined
 
a
 
path
 
towar ds
 
almost
 
$1
 
trillion
 
of
 
AI
 
spending
 
over
 
the
 
next
 
few
 
years.
 
Wher e
 
are
 
we
 
in
 
that
 
build-out?
 
And
 
do
 
you
 
think
 
it's
 
going
 
to
 
be
 
uniform
 
that
 
you
 
will
 
see
 
every
 
spender ,
 
whether
 
it's
 
ESP,
 
sover eigns,
 
enterprises
 
or
 
build-out,
 
should
 
we
 
expect
 
some
 
periods
 
of
 
digestion
 
in
 
between?
 
Just
 
what
 
are
 
your
 
customer
 
discussions
 
telling
 
you
 
about
 
how
 
to
 
model
 
growth
 
for
 
next
 
year?
 
Colett e
 
Kress
 
Yes,
 
Vivek.
 
Thanks
 
so
 
much
 
for
 
the
 
question
 
regarding
 
H20.
 
Yes,
 
we
 
recogniz ed
 
$4.6
 
billion
 
H20
 
in
 
Q1.
 
We
 
were
 
unable
 
to
 
ship
 
$2.5
 
billion
 
so
 
the
 
total
 
for
 
Q1
 
should
 
have
 
been
 
$7
 
billion.
 
When
 
we
 
look
 
at
 
our
 
Q2,
 
our
 
Q2
 
is
 
going
 
to
 
be
 
meaningfully
 
down
 
in
 
terms
 
of
 
China
 
data
 
center
 
revenue.
 
And
 
we
 
had
 
highlight ed
 
in
 
terms
 
of
 
the
 
amount
 
of
 
orders
 
that
 
we
 
had
 
planned
 
for
 
H20
 
in
 
Q2,
 
and
 
that
 
was
 
$8
 
billion.
 
Now
 
going
 
forward,
 
we
 
did
 
have
 
other
 
orders
 
going
 
forward
 
that
 
we
 
will
 
not
 
be
 
able
 
to
 
fulﬁll.
 
That
 
is
 
what
 
was
 
incorpor ated,
 
therefore,
 
in
 
the
 
amount
 
that
 
we
 
wrote
 
down
 
of
 
the
 
$4.5
 
billion.
 
That
 
write-down
 
was
 
about
 
invent ory
 
and
 
purchase
 
commitments,
 
and
 
our
 
purchase
 
commitments
 
were
 
about
 
what
 
we
 
expect ed
 
regarding
 
the
 
orders
 
that
 
we
 
had
 
received.
 
Going
 
forward,
 
though,
 
it's
 
a
 
bigger
 
issue
 
regarding
 
the
 
amount
 
of
 
the
 
mark et
 
that
 
we
 
will
 
not
 
be
 
able
 
to
 
serve.
 
We
 
assess
 
that
 
TAM
 
to
 
be
 
close
 
to
 
about
 
$50
 
billion
 
in
 
the
 
future
 
as
 
we
 
don't
 
have
 
a
 
product
 
to
 
enable
 
for
 
China.
 
Jensen
 
Huang
 
In
 
fact,
 
the
 
--
 
probably
 
the
 
best
 
way
 
to
 
think
 
through
 
it
 
is
 
that
 
AI
 
is
 
sever al
 
things.
 
Of
 
course,
 
we
 
know
 
that
 
AI
 
is
 
this
 
incredible
 
technology
 
that's
 
going
 
to
 
transform
 
every
 
industr y
 
from,
 
of
 
course,
 
the
 
way
 
we
 
do
 
softwar e
 
to
 
health
 
care
 
and
 
ﬁnancial
 
services
 
to
 
retail
 
to,
 
I
 
guess,
 
every
 
industr y,
 
transpor tation,
 
manufacturing.
 
And
 
we're
 
at
 
the
 
beginning
 
of
 
that.
 
But
 
maybe
 
another
 
way
 
to
 
think
 
about
 
that
 
is
 
wher e
 
do
 
we
 
need
 
intelligence,
 
wher e
 
do
 
we
 
need
 
digital
 
intelligence?
 
And
 
it's
 
in
 
every
 
countr y,
 
it's
 
in
 
every
 
industr y.
 
And
 
we
 
know
 
because
 
of
 
that,
 
we
 
recogniz e
 
that
 
AI
 
is
 
also
 
an
 
infrastructur e.
 
It's
 
a
 
way
 
of
 
developing
 
a
 
technology
 
--
 
delivering
 
a
 
technology
 
that
 
requir es
 
factories
 
and
 
these
 
factories
 
produce
 
tokens.
 
And
 
they,
 
as
 
I
 
mentioned,
 
are
 
impor tant
 
to
 
every
 
single
 
industr y
 
and
 
every
 
single
 
countr y.
 
And
 
so
 
on
 
that
 
basis,
 
we're
 
really
 
at
 
the
 
very
 
beginning
 
of
 
it
 
because
 
the
 
adoption
 
of
 
this
 
technology
 
is
 
really
 
kind
 
of
 
in
 
its
 
early,
 
early
 
stages.
 
Now
 
we've
 
reached
 
an
 
extraordinar y
 
milest one
 
with
 
AIs
 
that
 
are
 
reasoning
 
or
 
thinking,
 
what
 
people
 
call
 
inference
 
time
 
scaling.
 
Of
 
course,
 
it
 
created
 
a
 
whole
 
new
 
--
 
we've
 
entered
 
an
 
era
 
wher e
 
inference
 
is
 
going
 
to
 
be
 
a
 
signiﬁcant
 
part
 
of
 
the
 
comput e
 
workload.
 
But
 
anyhow ,
 
it's
 
going
 
to
 
be
 
a
 
new
 
infrastructur e,
 
and
 
we're
 
building
 
it
 
out
 
in
 
the
 
cloud.
 
The
 
United
 
States
 
is
 
really
 
the
 
early
 
starter
 
and
 
available
 
in
 
U.S.
 
clouds.
 
And
 
this
 
is
 
our
 
largest
 
mark et,
 
our
 
largest
 
installed
 
base
 
and
 
we
 
continue
 
to
 
see
 
that
 
happening.
 
But
 
beyond
 
that,
 
we're
 
going
 
to
 
have
 
to
 
--
 
we're
 
going
 
to
 
see
 
AI
 
go
 
into
 
enterprise,
 
which
 
is
 
on-pr em
 
because
 
so
 
much
 
of
 
the
 
data
 
is
 
still
 
on-pr em.
 
Access
 
contr ol
 
is
 
really
 
impor tant.
 
It's
 
really
 
hard
 
to
 
move
 
all
 
of
 
every
 
company's
 
data
 
into
 
the
 
cloud.
 
And
 
so
 
we're
 
going
 
to
 
move
 
AI
 
into
 
the
 
enterprise.
 
And
 
you
 
saw
 
that
 
we
 
announced
 
a
 
couple
 
of
 
really
 
exciting
 
new
 
products,
 
our
 
RTX
 
Pro
 
Enterprise
 
AI
 
server
 
that
 
runs
 
everything
 
enterprise
 
and
 
AI,
 
our
 
DGX
 
Spark
 
and
 
DGX
 
Station,
 
which
 
is
 
designed
 
for
 
developers
 
who
 
want
 
to
 
work
 
on-pr em.
 
And
 
so
 
enterprise
 
AI
 
is
 
just
 
taking
 
oﬀ.
 
Telcos.
 
Today,
 
a
 
lot
 
of
 
the
 
telco
 
infrastructur e
 
will
 
be,
 
in
 
the
 
future,
 
softwar e
 
deﬁned
 
and
 
built
 
on
 
AI,
 
and
 
so
 
6G
 
is
 
going
 
to
 
be
 
built
 
on
 
AI
 
and
 
that
 
infrastructur e
 
needs
 
to
 
be
 
built
 
out.
 
And
 
I
 
said,
 
it's
 
very,
 
very
 
early
 
stages.
 
And
 
then,
 
of
 
course,
 
every
 
factory
 
today
 
that
 
makes
 
things
 
will
 
have
 
an
 
AI
 
factory
 
that
 
sits
 
with
 
it.
 
And
 
the
 
AI
 
factory
 
is
 
going
 
to
 
be
 
--
 
drive
 
creating
 
AI
 
and
 
operating
 
AI
 
for
 
the
 
factory
 
itself
 
but
 
also
 
to
 
power
 
the
 
products
 
and
 
the
 
things
 
that
 
are
 
made
 
by
 
the
 
factory.
 
So
 
it's
 
very
 
clear
 
that
 
every
 
company
 
will
 
have
 
AI
 
factories.
 
And
 
very
 
soon,
 
there'll
 
be
 
robotics
 
companies,
 
robot
 
companies
 
and
 
those
 
companies
 
will
 
be
 
also
 
building
 
AIs
 
to
 
drive
 
the
 
robots.
 
And
 
so
 
we're
 
at
 
the
 
beginning
 
of
 
all
 
of
 
this
 
build-out.
 
Oper ator
 
The
 
next
 
question
 
comes
 
from
 
C.J.
 
Muse
 
with
 
Cantor
 
Fitzger ald.
 
Christ opher
 
Muse
 
There
 
have
 
been
 
many
 
large
 
GPU
 
clust er
 
investment
 
announcements
 
in
 
the
 
last
 
month,
 
and
 
you
 
alluded
 
to
 
a
 
few
 
of
 
them
 
with
 
Saudi
 
Arabia,
 
the
 
UAE.
 
And
 
then
 
also
 
we
 
heard
 
from
 
Oracle
 
and
 
xAI,
 
just
 
to
 
name
 
a
 
few.
 
So
 
my
 
question,
 
are
 
there
 
other
 
that
 
have
 
yet
 
to
 
be
 
announced
 
of
 
the
 
same
 
kind
 
of
 
scale
 
and
 
magnitude?
 
And
 
perhaps
 
more
 
impor tantly,
 
how
 
are
 
these
 
orders
 
impacting
 
your
 
lead
 
times
 
for
 
Blackwell
 
and
 
your
 
current
 
visibility
 
sitting
 
here
 
today
 
almost
 
halfway
 
through
 
2025?
 
Jensen
 
Huang
 
Well,
 
we
 
have
 
more
 
orders
 
today
 
than
 
we
 
did
 
at
 
the
 
last
 
time
 
I
 
spok e
 
about
 
orders
 
at
 
GTC.
 
However ,
 
we're
 
also
 
increasing
 
our
 
supply
 
chain
 
and
 
building
 
out
 
our
 
supply
 
chain.
 
They'r e
 
doing
 
a
 
fantastic
 
job.
 
We're
 
building
 
it
 
here
 
onshor e
 
in
 
the
 
United
 
States.
 
But
 
we're
 
going
 
to
 
keep
 
our
 
supply
 
chain
 
quite
 
busy
 
for
 
sever al
 
--
 
many
 
more
 
years
 
coming.
 
And
 
with
 
respect
 
to
 
further
 
announcements,
 
I'm
 
going
 
to
 
be
 
on
 
the
 
road
 
next
 
week
 
through
 
Europe.
 
And
 
it's
 
--
 
just
 
about
 
every
 
countr y
 
needs
 
to
 
build
 
out
 
AI
 
infrastructur e
 
and
 
their
 
[
 
umpt eenth
 
]
 
AI
 
factories
 
being
 
planned.
 
We're
 
--
 
I
 
think
 
in
 
the
 
remarks,
 
Colett e
 
mentioned
 
there's
 
some
 
100
 
AI
 
factories
 
being
 
built.
 
There's
 
a
 
whole
 
bunch
 
that
 
haven't
 
been
 
announced.
 
And
 
I
 
think
 
the
 
impor tant
 
concept
 
here
 
which
 
makes
 
it
 
easier
 
to
 
understand
 
is
 
that
 
like
 
other
 
technologies
 
that
 
impact
 
literally
 
every
 
single
 
industr y,
 
of
 
course,
 
electricity
 
was
 
one
 
and
 
it
 
became
 
infrastructur e.
 
Of
 
course,
 
the
 
information
 
infrastructur e,
 
which
 
we
 
now
 
know
 
as
 
the
 
Internet
 
aﬀects
 
every
 
single
 
industr y,
 
every
 
countr y,
 
every
 
society .
 
Intelligence
 
is
 
surely
 
one
 
of
 
those
 
things.
 
I
 
don't
 
know
 
any
 
company,
 
industr y,
 
countr y
 
who
 
thinks
 
that
 
intelligence
 
is
 
optional.
 
It's
 
essential
 
infrastructur e.
 
And
 
so
 
we've
 
now
 
digitaliz ed
 
intelligence.
 
And
 
so
 
I
 
think
 
we're
 
clearly
 
in
 
the
 
beginning
 
of
 
the
 
build-out
 
of
 
this
 
infrastructur e.
 
And
 
every
 
countr y
 
will
 
have
 
it,
 
I'm
 
certain
 
of
 
that.
 
Every
 
industr y
 
will
 
use
 
it,
 
that
 
I'm
 
certain
 
of.
 
And
 
what's
 
unique
 
about
 
this
 
infrastructur e
 
is
 
that
 
it
 
needs
 
factories.
 
It's
 
a
 
little
 
bit
 
like
 
the
 
energy
 
infrastructur e,
 
electricity .
 
It
 
needs
 
factories.
 
We
 
need
 
factories
 
to
 
produce
 
this
 
intelligence,
 
and
 
the
 
intelligence
 
is
 
getting
 
more
 
sophisticat ed.
 
We
 
were
 
talking
 
about
 
earlier
 
that
 
we
 
had
 
a
 
huge
 
breakthr ough
 
in
 
the
 
last
 
couple
 
of
 
years
 
with
 
reasoning
 
AI.
 
And
 
now
 
there
 
are
 
agents
 
that
 
reason
 
and
 
there
 
are
 
super -agents
 
that
 
use
 
a
 
whole
 
bunch
 
of
 
tools
 
and
 
then
 
there's
 
clust ers
 
of
 
super
 
agents
 
wher e
 
agents
 
are
 
working
 
with
 
agents,
 
solving
 
problems.
 
And
 
so
 
you
 
could
 
just
 
imagine,
 
compar ed
 
to
 
one-shot
 
chatbots
 
and
 
the
 
agents
 
that
 
are
 
now
 
using
 
AI
 
built
 
on
 
these
 
large
 
language
 
models,
 
how
 
much
 
more
 
comput e-intensive
 
they
 
really
 
need
 
to
 
be
 
and
 
are.
 
So
 
I
 
think
 
we're
 
in
 
the
 
beginning
 
of
 
the
 
build-out,
 
and
 
there
 
should
 
be
 
many,
 
many
 
more
 
announcements
 
in
 
the
 
future.
 
Oper ator
 
Your
 
next
 
question
 
comes
 
from
 
Ben
 
Reitz es
 
with
 
Melius.
 
Benjamin
 
Reitz es
 
I
 
want ed
 
to
 
ask,
 
ﬁrst
 
to
 
Colett e,
 
just
 
a
 
little
 
clariﬁcation
 
around
 
the
 
guidance
 
and
 
maybe
 
putting
 
it
 
in
 
a
 
diﬀer ent
 
way.
 
The
 
$8
 
billion
 
for
 
H20
 
just
 
seems
 
like
 
it's
 
roughly
 
$3
 
billion
 
more
 
than
 
most
 
people
 
thought
 
with
 
regard
 
to
 
what
 
you'd
 
be
 
foregoing
 
in
 
the
 
second
 
quarter.
 
So
 
that
 
would
 
mean
 
that
 
with
 
regard
 
to
 
your
 
guidance,
 
the
 
rest
 
of
 
the
 
business
 
in
 
order
 
to
 
hit
 
[
 
45
 
]
 
is
 
doing
 
$2
 
billion
 
to
 
$3
 
billion
 
or
 
so
 
better.
 
So
 
I
 
was
 
wondering
 
if
 
that
 
math
 
made
 
sense
 
to
 
you.
 
And
 
then
 
in
 
terms
 
of
 
the
 
guidance,
 
that
 
would
 
imply
 
the
 
non-China
 
business
 
is
 
doing
 
a
 
bit
 
better
 
than
 
the
 
Street
 
expect ed.
 
So
 
wondering
 
what
 
the
 
primar y
 
driver
 
was
 
there
 
in
 
your
 
view.
 
And
 
then
 
this
 
second
 
part
 
of
 
my
 
question,
 
Jensen,
 
I
 
know
 
you
 
guide
 
1
 
quarter
 
at
 
a
 
time,
 
but
 
with
 
regard
 
to
 
the
 
AI
 
diﬀusion
 
rule
 
being
 
lifted
 
and
 
this
 
momentum
 
with
 
sover eign,
 
there's
 
been
 
times
 
in
 
your
 
history
 
wher e
 
you
 
guys
 
have
 
said
 
on
 
calls
 
like
 
this,
 
wher e
 
you
 
have
 
more
 
conviction
 
and
 
sequential
 
growth
 
throughout
 
the
 
year,
 
et
 
cetera.
 
And
 
given
 
the
 
unleashing
 
of
 
demand
 
with
 
AI
 
diﬀusion
 
being
 
revok ed
 
and
 
the
 
supply
 
chain
 
increasing,
 
does
 
the
 
envir onment
 
give
 
you
 
more
 
conviction
 
and
 
sequential
 
growth
 
as
 
we
 
go
 
throughout
 
the
 
year?
 
So
 
ﬁrst
 
1
 
for
 
Colett e
 
and
 
then
 
next
 
1
 
for
 
Jensen.
 
Colett e
 
Kress
 
Thanks,
 
Ben,
 
for
 
the
 
question.
 
When
 
we
 
look
 
at
 
our
 
Q2
 
guidance
 
and
 
our
 
commentar y
 
that
 
we
 
provided,
 
that
 
had
 
the
 
export
 
contr ols
 
not
 
occurr ed,
 
we
 
would
 
have
 
had
 
orders
 
of
 
about
 
$8
 
billion
 
for
 
H20,
 
that's
 
correct.
 
That
 
was
 
a
 
possibility
 
for
 
what
 
we
 
would
 
have
 
had
 
in
 
our
 
outlook
 
for
 
this
 
quarter
 
in
 
Q2.
 
So
 
what
 
we
 
also
 
have
 
talked
 
about
 
here
 
is
 
the
 
growth
 
that
 
we've
 
seen
 
in
 
Blackwell,
 
Blackwell
 
across
 
many
 
of
 
our
 
customers
 
as
 
well
 
as
 
the
 
growth
 
that
 
we
 
continue
 
to
 
have
 
in
 
terms
 
of
 
supply
 
that
 
we
 
need
 
for
 
our
 
customers.
 
So
 
putting
 
those
 
together ,
 
that's
 
wher e
 
we
 
came
 
through
 
with
 
the
 
guidance
 
that
 
we
 
provided.
 
I'm
 
going
 
to
 
turn
 
the
 
rest
 
over
 
to
 
Jensen
 
to
 
see
 
how
 
he
 
wants
 
to...
 
Jensen
 
Huang
 
Yes.
 
Thanks.
 
Thanks,
 
Ben.
 
I
 
would
 
say
 
compar ed
 
to
 
the
 
beginning
 
of
 
the
 
year,
 
compar ed
 
to
 
GTC
 
time
 
frame,
 
there
 
are
 
4
 
positive
 
surprises.
 
The
 
ﬁrst
 
positive
 
surprise
 
is
 
the
 
step
 
function
 
demand
 
increase
 
of
 
reasoning
 
AI,
 
I
 
think
 
it
 
is
 
fairly
 
clear
 
now
 
that
 
AI
 
is
 
going
 
through
 
an
 
exponential
 
growth,
 
and
 
reasoning
 
AI
 
really
 
busted
 
through.
 
Concerns
 
about
 
hallucination
 
or
 
its
 
ability
 
to
 
really
 
solve
 
problems,
 
and
 
I
 
think
 
a
 
lot
 
of
 
people
 
are
 
crossing
 
that
 
barrier
 
and
 
realizing
 
how
 
incredibly
 
eﬀective
 
agentic
 
AI
 
is
 
and
 
reasoning
 
AI
 
is.
 
So
 
number
 
1
 
is
 
inference
 
reasoning
 
and
 
the
 
exponential
 
growth
 
there,
 
demand
 
growth.
 
The
 
second
 
one,
 
you
 
mentioned
 
AI
 
diﬀusion.
 
It's
 
really
 
terriﬁc
 
to
 
see
 
that
 
the
 
AI
 
diﬀusion
 
rule
 
was
 
rescinded.
 
President
 
Trump
 
wants
 
America
 
to
 
win,
 
and
 
he
 
also
 
realizes
 
that
 
we're
 
not
 
the
 
only
 
countr y
 
in
 
the
 
race.
 
And
 
he
 
wants
 
the
 
United
 
States
 
to
 
win
 
and
 
recogniz es
 
that
 
we
 
have
 
to
 
get
 
the
 
American
 
stack
 
out
 
to
 
the
 
world
 
and
 
have
 
the
 
world
 
build
 
on
 
top
 
of
 
American
 
stacks
 
instead
 
of
 
alternatives.
 
And
 
so
 
AI
 
diﬀusion
 
happened,
 
the
 
rescinding
 
of
 
it
 
happened
 
at
 
almost
 
precisely
 
the
 
time
 
that
 
countries
 
around
 
the
 
world
 
are
 
awak ening
 
to
 
the
 
impor tance
 
of
 
AI
 
as
 
an
 
infrastructur e,
 
not
 
just
 
as
 
a
 
technology
 
of
 
great
 
curiosity
 
and
 
great
 
impor tance,
 
but
 
infrastructur e
 
for
 
their
 
industries
 
and
 
start-ups
 
and
 
society .
 
Just
 
as
 
they
 
had
 
to
 
build
 
out
 
infrastructur e
 
for
 
electricity
 
and
 
Internet,
 
you
 
got
 
to
 
build
 
out
 
an
 
infrastructur e
 
for
 
AI.
 
I
 
think
 
that,
 
that's
 
an
 
awak ening,
 
and
 
that
 
creates
 
a
 
lot
 
of
 
oppor tunity .
 
The
 
third
 
is
 
enterprise
 
AI.
 
Agents
 
work
 
and
 
agents
 
are
 
doing
 
--
 
these
 
agents
 
are
 
really
 
quite
 
successful,
 
much
 
more
 
than
 
gener ative
 
AI.
 
Agentic
 
AI
 
is
 
game-changing.
 
Agents
 
can
 
understand
 
ambiguous
 
and
 
rather
 
implicit
 
instructions
 
and
 
able
 
to
 
problem
 
solve
 
and
 
use
 
tools
 
and
 
have
 
memor y
 
and
 
so
 
on.
 
And
 
so
 
I
 
think
 
this
 
is
 
--
 
enterprise
 
AI
 
is
 
ready
 
to
 
take
 
oﬀ.
 
And
 
it's
 
taken
 
us
 
a
 
few
 
years
 
to
 
build
 
a
 
computing
 
system
 
that
 
is
 
able
 
to
 
integrate
 
and
 
run
 
enterprise
 
AI
 
stacks,
 
run
 
enterprise
 
IT
 
stacks
 
but
 
add
 
AI
 
to
 
it.
 
And
 
this
 
is
 
the
 
RTX
 
Pro
 
Enterprise
 
server
 
that
 
we
 
announced
 
at
 
Comput ex
 
just
 
last
 
week.
 
And
 
just
 
about
 
every
 
major
 
IT
 
company
 
has
 
joined
 
us,
 
super
 
excited
 
about
 
that.
 
And
 
so
 
computing
 
is
 
1
 
stack,
 
1
 
part
 
of
 
it.
 
But
 
remember ,
 
enterprise
 
IT
 
is
 
really
 
3
 
pillars:
 
it's
 
comput e,
 
storage,
 
and
 
networking.
 
And
 
we've
 
now
 
put
 
all
 
3
 
of
 
them
 
together
 
for
 
ﬁnally,
 
and
 
we're
 
going
 
to
 
mark et
 
with
 
that.
 
And
 
then
 
lastly,
 
industrial
 
AI.
 
Remember ,
 
one
 
of
 
the
 
implications
 
of
 
the
 
world
 
reordering,
 
if
 
you
 
will,
 
is
 
a
 
region's
 
onshoring
 
manufacturing
 
and
 
building
 
plants
 
everywher e.
 
In
 
addition
 
to
 
AI
 
factories,
 
of
 
course,
 
there
 
are
 
new
 
electr onics
 
manufacturing,
 
chip
 
manufacturing
 
being
 
built
 
around
 
the
 
world.
 
And
 
all
 
of
 
these
 
new
 
plants
 
in
 
these
 
new
 
factories
 
are
 
creating
 
exactly
 
the
 
right
 
time
 
when
 
Omniverse
 
and
 
AI
 
and
 
all
 
the
 
work
 
that
 
we're
 
doing
 
with
 
robotics
 
is
 
emer ging.
 
And
 
so
 
this
 
fourth
 
pillar
 
is
 
quite
 
impor tant.
 
Every
 
factory
 
will
 
have
 
an
 
AI
 
factory
 
associat ed
 
with
 
it.
 
And
 
in
 
order
 
to
 
create
 
these
 
physical
 
AI
 
systems,
 
you
 
really
 
have
 
to
 
train
 
a
 
vast
 
amount
 
of
 
data.
 
So
 
back
 
to
 
more
 
data,
 
more
 
training,
 
more
 
AIs
 
to
 
be
 
created,
 
more
 
comput ers.
 
And
 
so
 
these
 
4
 
drivers
 
are
 
really
 
kicking
 
into
 
turbochar ge.
 
Oper ator
 
Your
 
next
 
question
 
comes
 
from
 
Timothy
 
Arcuri
 
with
 
UBS.
 
Timothy
 
Arcuri
 
Jensen,
 
I
 
want ed
 
to
 
ask
 
about
 
China.
 
It
 
sounds
 
like
 
the
 
July
 
guidance
 
assumes
 
there's
 
no
 
SKU
 
replacement
 
for
 
the
 
age
 
20.
 
But
 
if
 
the
 
President
 
wants
 
the
 
U.S.
 
to
 
win,
 
it
 
seems
 
like
 
you'r e
 
going
 
to
 
have
 
to
 
be
 
allowed
 
to
 
ship
 
something
 
into
 
China.
 
So
 
I
 
guess
 
I
 
had
 
2
 
points
 
on
 
that.
 
First
 
of
 
all,
 
have
 
you
 
been
 
approved
 
to
 
ship
 
a
 
new
 
modiﬁed
 
version
 
into
 
China?
 
And
 
you'r e
 
currently
 
building
 
it
 
but
 
you
 
just
 
can't
 
ship
 
it
 
in
 
ﬁscal
 
Q2?
 
And
 
then
 
you
 
were
 
sort
 
of
 
run
 
rating
 
$7
 
billion
 
to
 
$8
 
billion
 
a
 
quarter
 
into
 
China.
 
Can
 
we
 
get
 
back
 
to
 
those
 
sorts
 
of
 
quarterly
 
run
 
rates
 
once
 
you
 
get
 
something
 
that
 
you'r e
 
allowed
 
to
 
ship
 
back
 
into
 
China?
 
I
 
think
 
we're
 
all
 
trying
 
to
 
ﬁgure
 
out
 
how
 
much
 
to
 
add
 
back
 
to
 
our
 
models
 
and
 
when.
 
So
 
what ever
 
you
 
can
 
say
 
there
 
would
 
be
 
great.
 
Jensen
 
Huang
 
The
 
President
 
has
 
a
 
plan.
 
He
 
has
 
a
 
vision
 
and
 
I
 
trust
 
him.
 
With
 
respect
 
to
 
our
 
export
 
contr ols,
 
it's
 
a
 
set
 
of
 
limits.
 
And
 
the
 
new
 
set
 
of
 
limits
 
pretty
 
much
 
make
 
it
 
impossible
 
for
 
us
 
to
 
reduce
 
hopper
 
any
 
further
 
for
 
any
 
productive
 
use.
 
And
 
so
 
the
 
new
 
limits,
 
it's
 
kind
 
of
 
the
 
end
 
of
 
the
 
road
 
for
 
Hopper .
 
We
 
have
 
some
 
--
 
we
 
have
 
limited
 
options.
 
And
 
so
 
we
 
just
 
--
 
the
 
key
 
is
 
to
 
understand
 
the
 
limits.
 
The
 
key
 
is
 
to
 
understand
 
the
 
limits
 
and
 
see
 
if
 
we
 
can
 
come
 
up
 
with
 
interesting
 
products
 
that
 
could
 
continue
 
to
 
serve
 
the
 
Chinese
 
mark et.
 
We
 
don't
 
have
 
anything
 
at
 
the
 
moment,
 
but
 
we're
 
considering
 
it.
 
We're
 
thinking
 
about
 
it.
 
Obviously,
 
the
 
limits
 
are
 
quite
 
stringent
 
at
 
the
 
moment.
 
And
 
we
 
have
 
nothing
 
to
 
announce
 
today .
 
And
 
when
 
the
 
time
 
comes,
 
we'll
 
engage
 
the
 
administr ation
 
and
 
discuss
 
that.
 
Oper ator
 
Your
 
ﬁnal
 
question
 
comes
 
from
 
the
 
line
 
of
 
Aaron
 
Rakers
 
with
 
Wells
 
Fargo.
 
Jacob
 
Wilhelm
 
This
 
is
 
Jake
 
on
 
for
 
Aaron.
 
I
 
was
 
wondering
 
if
 
you
 
could
 
give
 
some
 
additional
 
color
 
around
 
the
 
strength
 
you
 
saw
 
within
 
the
 
Networking
 
business,
 
particularly
 
around
 
the
 
adoption
 
of
 
your
 
Ethernet
 
solutions
 
at
 
CSPs
 
as
 
well
 
as
 
any
 
change
 
you'r e
 
seeing
 
in
 
network
 
attach
 
rates.
 
Jensen
 
Huang
 
Yes,
 
thank
 
you
 
for
 
that.
 
We
 
now
 
have
 
3
 
networking
 
platforms,
 
maybe
 
4.
 
The
 
ﬁrst
 
1
 
is
 
the
 
scale-up
 
platform
 
to
 
turn
 
a
 
comput er
 
into
 
a
 
much
 
larger
 
comput er.
 
Scaling
 
up
 
is
 
incredibly
 
hard
 
to
 
do.
 
Scaling
 
out
 
is
 
easier
 
to
 
do
 
but
 
scaling
 
up
 
is
 
hard
 
to
 
do.
 
And
 
that
 
platform
 
is
 
called
 
NVLink.
 
NVLink
 
is
 
--
 
comes
 
with
 
it
 
chips
 
and
 
switches
 
and
 
NVLink
 
spines
 
and
 
it's
 
really
 
complicat ed.
 
But
 
anyway,
 
that's
 
our
 
new
 
platform,
 
scale-up
 
platform.
 
In
 
addition
 
to
 
InﬁniBand,
 
we
 
also
 
have
 
Spectrum-X.
 
We've
 
been
 
fairly
 
consist ent
 
that
 
Ethernet
 
was
 
designed
 
for
 
a
 
lot
 
of
 
traﬃc
 
that
 
are
 
independent.
 
But
 
in
 
the
 
case
 
of
 
AI,
 
you
 
have
 
a
 
lot
 
of
 
comput ers
 
working
 
together .
 
And
 
the
 
traﬃc
 
of
 
AI
 
is
 
insanely
 
bursty .
 
Latency
 
matters
 
a
 
lot
 
because
 
the
 
AI
 
is
 
thinking
 
and
 
it
 
wants
 
to
 
get
 
work
 
on
 
as
 
quickly
 
as
 
possible,
 
and
 
you
 
got
 
a
 
whole
 
bunch
 
of
 
nodes
 
working
 
together .
 
And
 
so
 
we
 
enhanced
 
Ethernet,
 
added
 
capabilities
 
like
 
extremely
 
low
 
latency,
 
congestion
 
contr ol,
 
adaptive
 
routing,
 
the
 
type
 
of
 
technologies
 
that
 
were
 
available
 
only
 
in
 
InﬁniBand
 
to
 
Ethernet.
 
And
 
as
 
a
 
result,
 
we
 
improved
 
the
 
utilization
 
of
 
Ethernet
 
in
 
these
 
clust ers.
 
These
 
clust ers
 
are
 
gigantic,
 
from
 
as
 
low
 
as
 
50%
 
to
 
as
 
high
 
as
 
85%,
 
90%.
 
And
 
so
 
the
 
diﬀer ence
 
is
 
if
 
you
 
had
 
a
 
clust er
 
that's
 
$10
 
billion
 
and
 
you
 
improve
 
its
 
eﬀectiveness
 
by
 
40%,
 
that's
 
worth
 
$4
 
billion.
 
It's
 
incredible.
 
And
 
so
 
Spectrum-X
 
has
 
been
 
really,
 
quite
 
frankly,
 
a
 
home
 
run.
 
And
 
this
 
last
 
quarter,
 
as
 
we
 
said
 
in
 
the
 
prepared
 
remarks,
 
we
 
added
 
2
 
very
 
signiﬁcant
 
CSPs
 
to
 
the
 
Spectrum-X
 
adoption.
 
And
 
then
 
the
 
last
 
1
 
is
 
BlueField,
 
which
 
is
 
our
 
contr ol
 
plane.
 
And
 
so
 
in
 
those
 
4
 
--
 
those
 
--
 
the
 
contr ol
 
plane
 
network,
 
which
 
is
 
used
 
for
 
storage.
 
It's
 
used
 
for
 
security
 
and
 
for
 
many
 
of
 
these
 
clust ers
 
that
 
want
 
to
 
achieve
 
isolation
 
among
 
its
 
users,
 
multi-t enant
 
clust ers
 
and
 
still
 
be
 
able
 
to
 
use
 
and
 
have
 
extremely
 
high-per formance
 
bare
 
metal
 
performance,
 
BlueField
 
is
 
ideal
 
for
 
that
 
and
 
is
 
used
 
in
 
a
 
lot
 
of
 
these
 
cases.
 
And
 
so
 
we
 
have
 
these
 
4
 
networking
 
platforms
 
that
 
are
 
all
 
growing
 
and
 
we're
 
doing
 
really
 
well.
 
I'm
 
very
 
proud
 
of
 
the
 
team.
 
Oper ator
 
That
 
is
 
all
 
the
 
time
 
we
 
have
 
for
 
questions.
 
Jensen,
 
I
 
will
 
turn
 
the
 
call
 
back
 
to
 
you.
 
Jensen
 
Huang
 
Thank
 
you.
 
This
 
is
 
the
 
start
 
of
 
a
 
power ful
 
new
 
wave
 
of
 
growth.
 
Grace
 
Blackwell
 
is
 
in
 
full
 
production.
 
We're
 
oﬀ
 
to
 
the
 
races.
 
We
 
now
 
have
 
multiple
 
signiﬁcant
 
growth
 
engines.
 
Inference,
 
once
 
the
 
light
 
workload
 
is
 
surging
 
with
 
revenue-gener ating
 
AI
 
services.
 
AI
 
is
 
growing
 
faster
 
and
 
will
 
be
 
larger
 
than
 
any
 
platform
 
shifts
 
befor e,
 
including
 
the
 
Internet,
 
mobile
 
and
 
cloud.
 
Blackwell
 
is
 
built
 
to
 
power
 
the
 
full
 
AI
 
life
 
cycle
 
from
 
training
 
frontier
 
models
 
to
 
running
 
comple x
 
inference
 
and
 
reasoning
 
agents
 
at
 
scale.
 
Training
 
demand
 
continues
 
to
 
rise
 
with
 
breakthr oughs
 
in
 
post
 
training
 
and
 
like
 
reinfor cement
 
learning
 
and
 
synthetic
 
data
 
gener ation.
 
But
 
inference
 
is
 
exploding.
 
Reasoning
 
AI
 
agents
 
requir e
 
orders
 
of
 
magnitude
 
more
 
comput e.
 
But
 
foundations
 
of
 
our
 
next
 
growth
 
platforms
 
are
 
in
 
place
 
and
 
ready
 
to
 
scale.
 
Sover eign
 
AI,
 
nations
 
are
 
investing
 
in
 
AI
 
infrastructur e.
 
They
 
for
 
electricity
 
and
 
Internet.
 
Enterprise
 
AI,
 
AI
 
must
 
be
 
deployable
 
on
 
prem
 
and
 
integrated
 
with
 
existing
 
IT.
 
Our
 
RTX
 
Pro,
 
DGX
 
Park
 
and
 
DGX
 
Station
 
enterprise
 
AI
 
systems
 
are
 
ready
 
to
 
moderniz e
 
the
 
$500
 
billion
 
IT
 
infrastructur e
 
on-pr em
 
or
 
in
 
the
 
cloud.
 
Every
 
major
 
IT
 
provider
 
is
 
partnering
 
with
 
us.
 
Industrial
 
AI
 
from
 
training
 
to
 
digital
 
twin
 
simulation
 
to
 
deployment,
 
NVIDIA
 
Omniverse
 
and
 
Isaac
 
are
 
powering
 
next-gener ation
 
factories
 
and
 
humanoid
 
robotic
 
systems
 
worldwide.
 
The
 
age
 
of
 
AI
 
is
 
here
 
from
 
AI
 
infrastructur es,
 
inference
 
at
 
scale,
 
sover eign
 
AI,
 
enterprise
 
AI,
 
and
 
industrial
 
AI,
 
NVIDIA
 
is
 
ready .
 
Join
 
us
 
at
 
GTC
 
Paris,
 
our
 
keynot e
 
at
 
VivaTech
 
on
 
June
 
11,
 
talking
 
about
 
quantum
 
GPU
 
computing,
 
robotic
 
factories
 
and
 
robots
 
and
 
celebr ate
 
our
 
partnerships
 
building
 
AI
 
factories
 
across
 
the
 
region.
 
The
 
NVIDIA
 
band
 
will
 
tour
 
France,
 
the
 
U.K.,
 
Germany,
 
and
 
Belgium.
 
Thank
 
you
 
for
 
joining
 
us
 
at
 
the
 
earnings
 
call
 
today .
 
See
 
you
 
in
 
Paris.
 
Oper ator
 
This
 
concludes
 
today's
 
confer ence
 
call.
 
You
 
may
 
now
 
disconnect.
 
 
