NVDA
 
2025
 
Q2
 
Earnings
 
Call
 
Transcript
 
28
 
Aug
 
2024
 
 
Participants
 
Stewar t
 
Stecker
 
executive
 
Colett e
 
Kress
 
executive
 
Vivek
 
Arya
 
analyst
 
Jensen
 
Huang
 
executive
 
Toshiya
 
Hari
 
analyst
 
Joseph
 
Moor e
 
analyst
 
Matthew
 
Ramsay
 
analyst
 
Timothy
 
Arcuri
 
analyst
 
Stacy
 
Rasgon
 
analyst
 
Benjamin
 
Reitz es
 
analyst
 
Christ opher
 
Muse
 
analyst
 
Aaron
 
Rakers
 
analyst
 
Call
 
transcript
 
Oper ator
 
Good
 
afternoon.
 
My
 
name
 
is
 
Abby,
 
and
 
I
 
will
 
be
 
your
 
confer ence
 
operator
 
today .
 
At
 
this
 
time,
 
I
 
would
 
like
 
to
 
welcome
 
everyone
 
to
 
NVIDIA's
 
second
 
quarter
 
earnings
 
call.
 
[Oper ator
 
Instructions]
 
Thank
 
you,
 
and
 
Mr.
 
Stewar t
 
Stecker,
 
you
 
may
 
begin
 
your
 
confer ence.
 
Stewar t
 
Stecker
 
Thank
 
you.
 
Good
 
afternoon,
 
everyone,
 
and
 
welcome
 
to
 
NVIDIA's
 
confer ence
 
call
 
for
 
the
 
second
 
quarter
 
of
 
ﬁscal
 
2025.
 
With
 
me
 
today
 
from
 
NVIDIA
 
are
 
Jensen
 
Huang,
 
President
 
and
 
Chief
 
Executive
 
Oﬃcer;
 
and
 
Colett e
 
Kress,
 
Executive
 
Vice
 
President
 
and
 
Chief
 
Financial
 
Oﬃcer .
 
I
 
would
 
like
 
to
 
remind
 
you
 
that
 
our
 
call
 
is
 
being
 
webcast
 
live
 
on
 
NVIDIA's
 
Invest or
 
Relations
 
websit e.
 
The
 
webcast
 
will
 
be
 
available
 
for
 
replay
 
until
 
the
 
confer ence
 
call
 
to
 
discuss
 
our
 
ﬁnancial
 
results
 
for
 
the
 
third
 
quarter
 
of
 
ﬁscal
 
2025.
 
The
 
content
 
of
 
today's
 
call
 
is
 
NVIDIA's
 
property.
 
It
 
cannot
 
be
 
reproduced
 
or
 
transcribed
 
without
 
prior
 
written
 
consent.
 
During
 
this
 
call,
 
we
 
may
 
make
 
forward-looking
 
statements
 
based
 
on
 
current
 
expectation.
 
These
 
are
 
subject
 
to
 
a
 
number
 
of
 
risks,
 
signiﬁcant
 
risks
 
and
 
uncer tainties,
 
and
 
our
 
actual
 
results
 
may
 
diﬀer
 
materially .
 
For
 
a
 
discussion
 
of
 
factors
 
that
 
could
 
aﬀect
 
our
 
future
 
ﬁnancial
 
results
 
and
 
business,
 
please
 
refer
 
to
 
the
 
disclosur e
 
in
 
today's
 
earnings
 
release,
 
our
 
most
 
recent
 
Forms
 
10-K
 
and
 
10-Q,
 
and
 
the
 
reports
 
that
 
we
 
may
 
ﬁle
 
on
 
Form
 
8-K
 
with
 
the
 
Securities
 
and
 
Exchange
 
Commission.
 
All
 
our
 
statements
 
are
 
made
 
as
 
of
 
today,
 
August
 
28,
 
2024,
 
based
 
on
 
information
 
currently
 
available
 
to
 
us.
 
Except
 
as
 
requir ed
 
by
 
law,
 
we
 
assume
 
no
 
obligation
 
to
 
updat e
 
any
 
such
 
statements.
 
During
 
this
 
call,
 
we
 
will
 
discuss
 
non-GAAP
 
ﬁnancial
 
measur es.
 
You
 
can
 
ﬁnd
 
a
 
reconciliation
 
of
 
these
 
non-GAAP
 
ﬁnancial
 
measur es
 
to
 
GAAP
 
ﬁnancial
 
measur es
 
in
 
our
 
CFO
 
commentar y,
 
which
 
is
 
posted
 
on
 
our
 
websit e.
 
Let
 
me
 
highlight
 
an
 
upcoming
 
event
 
for
 
the
 
ﬁnancial
 
community .
 
We
 
will
 
be
 
attending
 
the
 
Goldman
 
Sachs
 
Communacopia
 
and
 
Technology
 
Confer ence
 
on
 
September
 
11
 
in
 
San
 
Francisco,
 
wher e
 
Jensen
 
will
 
participat e
 
in
 
a
 
keynot e
 
ﬁreside
 
chat.
 
Our
 
earnings
 
call
 
to
 
discuss
 
the
 
results
 
of
 
our
 
third
 
quarter
 
of
 
ﬁscal
 
2025
 
is
 
scheduled
 
for
 
Wednesday,
 
November
 
20,
 
2024.
 
With
 
that,
 
let
 
me
 
turn
 
the
 
call
 
over
 
to
 
Colett e.
 
Colett e
 
Kress
 
Thanks,
 
Stewar t.
 
Q2
 
was
 
another
 
record
 
quarter.
 
Revenue
 
of
 
$30
 
billion
 
was
 
up
 
15%
 
sequentially
 
and
 
up
 
122%
 
year-on-year
 
and
 
well
 
above
 
our
 
outlook
 
of
 
$28
 
billion.
 
Starting
 
with
 
Data
 
Cent er.
 
Data
 
Cent er
 
revenue
 
of
 
$26.3
 
billion
 
was
 
a
 
record,
 
up
 
16%
 
sequentially
 
and
 
up
 
154%
 
year-on-year ,
 
driven
 
by
 
strong
 
demand
 
for
 
NVIDIA
 
Hopper ,
 
GPU
 
computing
 
and
 
our
 
networking
 
platforms.
 
Comput e
 
revenue
 
grew
 
more
 
than
 
2.5x.
 
Networking
 
revenue
 
grew
 
more
 
than
 
2x
 
from
 
the
 
last
 
year.
 
Cloud
 
service
 
providers
 
represent ed
 
roughly
 
45%
 
of
 
our
 
Data
 
Cent er
 
revenue,
 
and
 
more
 
than
 
50%
 
stemmed
 
from
 
the
 
consumer
 
Internet
 
and
 
enterprise
 
companies.
 
Customers
 
continue
 
to
 
acceler ate
 
their
 
Hopper
 
architectur e
 
purchases
 
while
 
gearing
 
up
 
to
 
adopt
 
Blackwell.
 
Key
 
workloads
 
driving
 
our
 
Data
 
Cent er
 
growth
 
include
 
gener ative
 
AI
 
model
 
training
 
and
 
inferencing;
 
video,
 
image,
 
and
 
text
 
data
 
pre
 
and
 
post
 
processing
 
with
 
CUD A
 
and
 
AI
 
workloads;
 
synthetic
 
data
 
gener ation;
 
AI-power ed
 
recommender
 
systems;
 
SQL
 
and
 
Vector
 
database
 
processing
 
as
 
well.
 
Next
 
gener ation
 
models
 
will
 
requir e
 
10
 
to
 
20x
 
more
 
comput e
 
to
 
train
 
with
 
signiﬁcantly
 
more
 
data.
 
The
 
trend
 
is
 
expect ed
 
to
 
continue.
 
Over
 
the
 
trailing
 
4
 
quarters,
 
we
 
estimat e
 
that
 
inference
 
drove
 
more
 
than
 
40%
 
of
 
our
 
Data
 
Cent er
 
revenue.
 
CSPs,
 
consumer
 
Internet
 
companies,
 
and
 
enterprises
 
beneﬁt
 
from
 
the
 
incredible
 
throughput
 
and
 
eﬃciency
 
of
 
NVIDIA's
 
inference
 
platform.
 
Demand
 
for
 
NVIDIA
 
is
 
coming
 
from
 
frontier
 
model
 
makers,
 
consumer
 
Internet
 
services,
 
and
 
tens
 
of
 
thousands
 
of
 
companies
 
and
 
start-ups
 
building
 
gener ative
 
AI
 
applications
 
for
 
consumers,
 
adver tising,
 
education,
 
enterprise
 
and
 
health
 
care,
 
and
 
robotics.
 
Developers
 
desir e
 
NVIDIA's
 
rich
 
ecosyst em
 
and
 
availability
 
in
 
every
 
cloud.
 
CSPs
 
appreciat e
 
the
 
broad
 
adoption
 
of
 
NVIDIA
 
and
 
are
 
growing
 
their
 
NVIDIA
 
capacity,
 
given
 
the
 
high
 
demand.
 
NVIDIA
 
H200
 
platform
 
began
 
ramping
 
in
 
Q2,
 
shipping
 
to
 
large
 
CSPs,
 
consumer
 
Internet
 
and
 
enterprise
 
company .
 
The
 
NVIDIA
 
H200
 
builds
 
upon
 
the
 
strength
 
of
 
our
 
Hopper
 
architectur e
 
and
 
oﬀering
 
over
 
40%
 
more
 
memor y
 
bandwidth
 
compar ed
 
to
 
the
 
H100.
 
Our
 
Data
 
Cent er
 
revenue
 
in
 
China
 
grew
 
sequentially
 
in
 
Q2
 
and
 
a
 
signiﬁcant
 
contribut or
 
to
 
our
 
Data
 
Cent er
 
revenue.
 
As
 
a
 
percentage
 
of
 
total
 
Data
 
Cent er
 
revenue,
 
it
 
remains
 
below
 
levels
 
seen
 
prior
 
to
 
the
 
imposition
 
of
 
export
 
contr ols.
 
We
 
continue
 
to
 
expect
 
the
 
China
 
mark et
 
to
 
be
 
very
 
competitive
 
going
 
forward.
 
The
 
latest
 
round
 
of
 
MLPerf
 
inference
 
benchmarks
 
highlight ed
 
NVIDIA's
 
inference
 
leadership
 
with
 
both
 
NVIDIA
 
Hopper
 
and
 
Blackwell
 
platform
 
combining
 
to
 
win
 
gold
 
medals
 
on
 
all
 
tasks.
 
At
 
Comput ex,
 
NVIDIA,
 
with
 
the
 
top
 
comput er
 
manufactur ers,
 
unveiled
 
an
 
array
 
of
 
Blackwell
 
architectur e-power ed
 
systems
 
and
 
NVIDIA
 
networking
 
for
 
building
 
AI
 
factories
 
and
 
data
 
centers.
 
With
 
the
 
NVIDIA
 
MGX
 
modular
 
reference
 
architectur e,
 
our
 
OEMs
 
and
 
ODM
 
partners
 
are
 
building
 
more
 
than
 
100
 
Blackwell-based
 
systems
 
designed
 
quickly
 
and
 
cost
 
eﬀectively .
 
The
 
NVIDIA
 
Blackwell
 
platform
 
brings
 
together
 
multiple
 
GPU ,
 
CPU,
 
DPU,
 
NVLink
 
and
 
Link
 
Switch
 
and
 
the
 
networking
 
chips,
 
systems
 
and
 
NVIDIA
 
CUD A
 
softwar e
 
to
 
power
 
the
 
next
 
gener ation
 
of
 
AI
 
across
 
the
 
cases,
 
industries,
 
and
 
countries.
 
The
 
NVIDIA
 
GB200
 
NVL72
 
system
 
with
 
the
 
ﬁfth-gener ation
 
NVLink
 
enables
 
all
 
72
 
GPUs
 
to
 
act
 
as
 
a
 
single
 
GPU
 
and
 
deliver
 
up
 
to
 
30x
 
faster
 
inference
 
for
 
LLM's
 
workloads
 
and
 
unlocking
 
the
 
ability
 
to
 
run
 
trillion-par amet er
 
models
 
in
 
real
 
time.
 
Hopper
 
demand
 
is
 
strong
 
and
 
Blackwell
 
is
 
widely
 
sampling.
 
We
 
executed
 
a
 
change
 
to
 
the
 
Blackwell
 
GPU
 
mass
 
to
 
improve
 
production
 
yields.
 
Blackwell
 
production
 
ramp
 
is
 
scheduled
 
to
 
begin
 
in
 
the
 
fourth
 
quarter
 
and
 
continue
 
into
 
ﬁscal
 
year
 
'26.
 
In
 
Q4,
 
we
 
expect
 
to
 
sever al
 
billion
 
dollars
 
in
 
Blackwell
 
revenue.
 
Hopper
 
shipments
 
are
 
expect ed
 
to
 
increase
 
in
 
the
 
second
 
half
 
of
 
ﬁscal
 
2025.
 
Hopper
 
supply
 
and
 
availability
 
have
 
improved.
 
Demand
 
for
 
Blackwell
 
platforms
 
is
 
well
 
above
 
supply,
 
and
 
we
 
expect
 
this
 
to
 
continue
 
into
 
next
 
year.
 
Networking
 
revenue
 
increased
 
16%
 
sequentially .
 
Our
 
Ethernet
 
for
 
AI
 
revenue,
 
which
 
includes
 
our
 
Spectrum-X
 
end-t o-end
 
Ethernet
 
platform
 
doubled
 
sequentially
 
with
 
hundr eds
 
of
 
customers
 
adopting
 
our
 
Ethernet
 
oﬀerings.
 
Spectrum-X
 
has
 
broad
 
mark et
 
suppor t
 
from
 
OEM
 
and
 
ODM
 
partners
 
and
 
is
 
being
 
adopt ed
 
by
 
CSPs,
 
GPU
 
cloud
 
providers
 
and
 
enterprises,
 
including
 
xAI
 
to
 
connect
 
the
 
largest
 
GPU
 
comput e
 
clust er
 
in
 
the
 
world.
 
Spectrum-X
 
super charges
 
Ethernet
 
for
 
AI
 
processing
 
and
 
delivers
 
1.6x
 
the
 
performance
 
of
 
traditional
 
Ethernet.
 
We
 
plan
 
to
 
launch
 
new
 
Spectrum-X
 
products
 
every
 
year
 
to
 
suppor t
 
demand
 
for
 
scaling
 
comput e
 
clust ers
 
from
 
tens
 
of
 
thousands
 
of
 
GPUs
 
today
 
to
 
millions
 
of
 
DPUs
 
in
 
the
 
near
 
future.
 
Spectrum-X
 
is
 
well
 
on
 
track
 
to
 
begin
 
a
 
multibillion-dollar
 
product
 
line
 
within
 
a
 
year.
 
Our
 
sover eign
 
AI
 
oppor tunities
 
continue
 
to
 
expand
 
as
 
countries
 
recogniz e
 
AI
 
expertise
 
and
 
infrastructur e
 
at
 
national
 
imper atives
 
for
 
their
 
society
 
and
 
industries.
 
Japan's
 
National
 
Institut e
 
of
 
Advanced
 
Industrial
 
Science
 
and
 
Technology
 
is
 
building
 
its
 
AI
 
Bridging
 
Cloud
 
Infrastructur e
 
3.0
 
super comput er
 
with
 
NVIDIA.
 
We
 
believe
 
sover eign
 
AI
 
revenue
 
will
 
reach
 
low
 
double-digit
 
billions
 
this
 
year.
 
The
 
enterprise
 
AI
 
wave
 
has
 
started.
 
Enterprises
 
also
 
drove
 
sequential
 
revenue
 
growth
 
in
 
the
 
quarter.
 
We
 
are
 
working
 
with
 
most
 
of
 
the
 
Fortune
 
100
 
companies
 
on
 
AI
 
initiatives
 
across
 
industries
 
and
 
geogr aphies.
 
A
 
range
 
of
 
applications
 
are
 
fueling
 
our
 
growth,
 
including
 
AI-power ed
 
chatbots,
 
gener ative
 
AI
 
copilots
 
and
 
agents
 
to
 
build
 
new
 
monetizable
 
business
 
applications
 
and
 
enhance
 
employee
 
productivity .
 
Amdocs
 
is
 
using
 
NVIDIA
 
gener ative
 
AI
 
for
 
their
 
smar t
 
agent,
 
transforming
 
the
 
customer
 
experience
 
and
 
reducing
 
customer
 
service
 
costs
 
by
 
30%.
 
ServiceNow
 
is
 
using
 
NVIDIA
 
for
 
its
 
Now
 
Assist
 
oﬀering,
 
the
 
fastest-gr owing
 
new
 
product
 
in
 
the
 
company's
 
history.
 
SAP
 
is
 
using
 
NVIDIA
 
to
 
build
 
[
 
dual
 
]
 
copilots.
 
Cohesity
 
is
 
using
 
NVIDIA
 
to
 
build
 
their
 
gener ative
 
AI
 
agent
 
and
 
lower
 
gener ative
 
AI
 
development
 
costs.
 
Snowﬂak e
 
serves
 
over
 
3
 
billion
 
queries
 
a
 
day
 
for
 
over
 
10,000
 
enterprise
 
customers
 
is
 
working
 
with
 
NVIDIA
 
to
 
build
 
copilots.
 
And
 
lastly,
 
is
 
using
 
NVIDIA
 
AI
 
Omniverse
 
to
 
reduce
 
end-t o-end
 
cycle
 
times
 
for
 
their
 
factories
 
by
 
50%.
 
Automotive
 
was
 
a
 
key
 
growth
 
driver
 
for
 
the
 
quarter
 
as
 
every
 
automak er
 
developing
 
autonomous
 
vehicle
 
technology
 
is
 
using
 
NVIDIA
 
in
 
their
 
data
 
centers.
 
Automotive
 
will
 
drive
 
multibillion
 
dollars
 
in
 
revenue
 
across
 
on-pr em
 
and
 
cloud
 
consumption
 
and
 
will
 
grow
 
as
 
next-gener ation
 
AV
 
models
 
requir e
 
signiﬁcantly
 
more
 
comput e.
 
Health
 
care
 
is
 
also
 
on
 
its
 
way
 
to
 
being
 
a
 
multibillion-dollar
 
business
 
as
 
AI
 
revolutioniz es
 
medical
 
imaging,
 
surgical
 
robots,
 
patient
 
care,
 
electr onic
 
health
 
record
 
processing,
 
and
 
drug
 
discover y.
 
During
 
the
 
quarter,
 
we
 
announced
 
a
 
new
 
NVIDIA
 
AI
 
foundr y
 
service
 
to
 
super charge
 
gener ative
 
AI
 
for
 
the
 
world's
 
enterprises
 
with
 
Meta's
 
Llama
 
3.1
 
collection
 
of
 
models.
 
This
 
marks
 
a
 
watershed
 
moment
 
for
 
enterprise
 
AI.
 
Companies
 
for
 
the
 
ﬁrst
 
time
 
can
 
lever age
 
the
 
capabilities
 
of
 
an
 
open
 
source
 
frontier -level
 
model
 
to
 
develop
 
customiz ed
 
AI
 
applications
 
to
 
encode
 
their
 
institutional
 
knowledge
 
into
 
an
 
AI
 
ﬂywheel
 
to
 
automat e
 
and
 
acceler ate
 
their
 
business.
 
Accentur e
 
is
 
the
 
ﬁrst
 
to
 
adopt
 
the
 
new
 
service
 
to
 
build
 
custom
 
Llama
 
3.1
 
models
 
for
 
both
 
its
 
own
 
use
 
and
 
to
 
assist
 
clients
 
seeking
 
to
 
deploy
 
gener ative
 
AI
 
applications.
 
NVIDIA
 
NIMs
 
acceler ate
 
and
 
simplify
 
model
 
deployment.
 
Companies
 
across
 
health
 
care,
 
energy,
 
ﬁnancial
 
services,
 
retail,
 
transpor tation,
 
and
 
telecommunications
 
are
 
adopting
 
NIMs,
 
including
 
Aramco,
 
Lowes,
 
and
 
Uber .
 
AT&T
 
realized
 
70%
 
cost
 
savings
 
and
 
8x
 
latency
 
reduction
 
after
 
moving
 
into
 
NIMs
 
for
 
gener ative
 
AI,
 
call
 
transcription
 
and
 
classiﬁcation.
 
Over
 
150
 
partners
 
are
 
embedding
 
NIMs
 
across
 
every
 
layer
 
of
 
the
 
AI
 
ecosyst em.
 
We
 
announced
 
NIM
 
Agent
 
Blueprint,
 
a
 
catalog
 
of
 
customizable
 
reference
 
applications
 
that
 
include
 
a
 
full
 
suite
 
of
 
softwar e
 
for
 
building
 
and
 
deploying
 
enterprise
 
gener ative
 
AI
 
applications.
 
With
 
NIM
 
Agent
 
Blueprint,
 
enterprises
 
can
 
reﬁne
 
their
 
AI
 
applications
 
over
 
time,
 
creating
 
a
 
data-driven
 
AI
 
ﬂywheel.
 
The
 
ﬁrst
 
NIM
 
Agent
 
Blueprints
 
include
 
workloads
 
for
 
customer
 
service,
 
comput er-aided
 
drug
 
discover y,
 
and
 
enterprise
 
retrieval
 
augment ed
 
gener ation.
 
Our
 
system
 
integrators,
 
technology
 
solution
 
providers,
 
and
 
system
 
builders
 
are
 
bringing
 
NVIDIA
 
NIM
 
Agent
 
Blueprints
 
to
 
enterprises.
 
NVIDIA
 
NIM
 
and
 
NIM
 
Agent
 
Blueprints
 
are
 
available
 
through
 
the
 
NVIDIA
 
AI
 
Enterprise
 
softwar e
 
platform,
 
which
 
has
 
great
 
momentum.
 
We
 
expect
 
our
 
softwar e,
 
SaaS
 
and
 
suppor t
 
revenue
 
to
 
approach
 
a
 
$2
 
billion
 
annual
 
run
 
rate
 
exiting
 
this
 
year,
 
with
 
NVIDIA
 
AI
 
Enterprise
 
notably
 
contributing
 
to
 
growth.
 
Moving
 
to
 
gaming
 
and
 
AI
 
PC.
 
Gaming
 
revenue
 
of
 
$2.88
 
billion
 
increased
 
9%
 
sequentially
 
and
 
16%
 
year-on-year .
 
We
 
saw
 
sequential
 
growth
 
in
 
console,
 
notebook,
 
and
 
deskt op
 
revenue,
 
and
 
demand
 
is
 
strong
 
and
 
growing
 
and
 
channel
 
invent ory
 
remains
 
healthy .
 
Every
 
PC
 
with
 
RTX
 
is
 
an
 
AI
 
PC.
 
RTX
 
PCs
 
can
 
deliver
 
up
 
to
 
1,300
 
AI
 
tops
 
and
 
are
 
now
 
over
 
200
 
RTX
 
AI
 
laptops
 
designed
 
from
 
leading
 
PC
 
manufactur ers.
 
With
 
600
 
AI-power ed
 
applications
 
and
 
games
 
and
 
an
 
installed
 
base
 
of
 
100
 
million
 
devices,
 
RTX
 
is
 
set
 
to
 
revolutioniz e
 
consumer
 
experiences
 
with
 
gener ative
 
AI.
 
NVIDIA
 
ACE,
 
a
 
suite
 
of
 
gener ative
 
AI
 
technologies
 
is
 
available
 
for
 
RTX
 
AI
 
PCs.
 
Megabr eak
 
is
 
the
 
ﬁrst
 
game
 
to
 
use
 
NVIDIA
 
ACE,
 
including
 
our
 
small
 
language
 
model,
 
Nemotr on
 
4B
 
optimiz ed
 
on
 
device
 
inference.
 
The
 
NVIDIA
 
gaming
 
ecosyst em
 
continues
 
to
 
grow.
 
Recently
 
added
 
RTX
 
and
 
DLSS
 
titles
 
include
 
Indiana
 
Jones
 
and
 
The
 
Great
 
Circle,
 
Awakening
 
and
 
Dragon
 
Age:
 
The
 
Vanguar d.
 
The
 
GeForce
 
NOW
 
library
 
continues
 
to
 
expand
 
with
 
total
 
catalog
 
size
 
of
 
over
 
2,000
 
titles,
 
the
 
most
 
content
 
of
 
any
 
cloud
 
gaming
 
service.
 
Moving
 
to
 
pro
 
visualization.
 
Revenue
 
of
 
$454
 
million
 
was
 
up
 
6%
 
sequentially
 
and
 
20%
 
year-on-year .
 
Demand
 
is
 
being
 
driven
 
by
 
AI
 
and
 
graphic
 
use
 
cases,
 
including
 
model
 
ﬁne-tuning
 
and
 
Omniverse-r elated
 
workloads.
 
Automotive
 
and
 
manufacturing
 
were
 
among
 
the
 
key
 
industr y
 
verticals
 
driving
 
growth
 
this
 
quarter.
 
Companies
 
are
 
racing
 
to
 
digitaliz e
 
workﬂows
 
to
 
drive
 
eﬃciency
 
across
 
their
 
operations.
 
The
 
world's
 
largest
 
electr onics
 
manufactur er,
 
Foxconn,
 
is
 
using
 
NVIDIA
 
Omniverse
 
to
 
power
 
digital
 
twins
 
of
 
the
 
physical
 
plants
 
that
 
produce
 
NVIDIA
 
Blackwell
 
systems.
 
And
 
sever al
 
large
 
global
 
enterprises,
 
including
 
Mercedes-Benz
 
signed
 
multiyear
 
contr acts
 
for
 
NVIDIA
 
Omniverse
 
Cloud
 
to
 
build
 
industrial
 
digital
 
twins
 
of
 
factories.
 
We
 
announced
 
new
 
NVIDIA
 
USD
 
NIMs
 
and
 
connect ors
 
to
 
open
 
Omniverse
 
to
 
new
 
industries
 
and
 
enable
 
developers
 
to
 
incorpor ate
 
gener ative
 
AI
 
copilots
 
and
 
agents
 
into
 
USD
 
workloads,
 
acceler ating
 
our
 
ability
 
to
 
build
 
highly
 
accur ate
 
virtual
 
worlds.
 
WPP
 
is
 
implementing
 
the
 
USD
 
NIM
 
microservices
 
in
 
its
 
gener ative
 
AI-enabled
 
content
 
creation
 
pipeline
 
for
 
customers
 
such
 
as
 
The
 
Coca-Cola
 
Company .
 
Moving
 
to
 
automotive
 
and
 
robotics.
 
Revenue
 
was
 
$346
 
million,
 
up
 
5%
 
sequentially
 
and
 
up
 
37%
 
year-on-year .
 
Year-on-year
 
growth
 
was
 
driven
 
by
 
the
 
new
 
customer
 
ramp
 
in
 
self-driving
 
platforms
 
and
 
increased
 
demand
 
for
 
AI
 
cockpit
 
solutions.
 
At
 
the
 
consumer
 
--
 
at
 
the
 
Comput er
 
Vision
 
and
 
Pattern
 
Recognition
 
Confer ence,
 
NVIDIA
 
won
 
the
 
Autonomous
 
Brand
 
Challenge
 
in
 
the
 
end-t o-end
 
driving
 
upscale
 
category,
 
outper forming
 
more
 
than
 
400
 
entries
 
worldwide.
 
Boston
 
Dynamics,
 
BYD
 
Electr onics,
 
Figur e,
 
Intrinsyc,
 
Siemens,
 
and
 
Teradyne
 
Robotics
 
are
 
using
 
the
 
NVIDIA
 
Isaac
 
robotics
 
platform
 
for
 
autonomous
 
robot
 
arms,
 
humanoids
 
and
 
mobile
 
robots.
 
Now
 
moving
 
to
 
the
 
rest
 
of
 
the
 
P&L.
 
GAAP
 
gross
 
margins
 
were
 
75.1%
 
and
 
non-GAAP
 
gross
 
margins
 
were
 
75.7%,
 
down
 
sequentially
 
due
 
to
 
a
 
higher
 
mix
 
of
 
new
 
products
 
within
 
Data
 
Cent er
 
and
 
invent ory
 
provisions
 
for
 
low-yielding
 
Blackwell
 
material.
 
Sequentially,
 
GAAP
 
and
 
non-GAAP
 
operating
 
expenses
 
were
 
up
 
12%,
 
primarily
 
reﬂecting
 
higher
 
compensation-r elated
 
costs.
 
Cash
 
ﬂow
 
from
 
operations
 
was
 
$14.5
 
billion.
 
In
 
Q2,
 
we
 
utilized
 
cash
 
of
 
$7.4
 
billion
 
towar d
 
shareholder
 
returns
 
in
 
the
 
form
 
of
 
share
 
repurchases
 
and
 
cash
 
dividends,
 
reﬂecting
 
the
 
increase
 
in
 
dividend
 
per
 
shareholder .
 
Our
 
Boar d
 
of
 
Directors
 
recently
 
approved
 
a
 
$50
 
billion
 
share
 
repurchase
 
authorization
 
to
 
add
 
to
 
our
 
remaining
 
$7.5
 
billion
 
of
 
authorization
 
at
 
the
 
end
 
of
 
Q2.
 
Let
 
me
 
turn
 
the
 
outlook
 
for
 
the
 
third
 
quarter.
 
Total
 
revenue
 
is
 
expect ed
 
to
 
be
 
$32.5
 
billion,
 
plus
 
or
 
minus
 
2%.
 
Our
 
third
 
quarter
 
revenue
 
outlook
 
incorpor ates
 
continued
 
growth
 
of
 
our
 
Hopper
 
architectur e
 
and
 
sampling
 
of
 
our
 
Blackwell
 
products.
 
We
 
expect
 
Blackwell
 
production
 
ramp
 
in
 
Q4.
 
GAAP
 
and
 
non-GAAP
 
gross
 
margins
 
are
 
expect ed
 
to
 
be
 
74.4%
 
and
 
75%,
 
respectively,
 
plus
 
or
 
minus
 
50
 
basis
 
points.
 
As
 
our
 
Data
 
Cent er
 
mix
 
continues
 
to
 
shift
 
to
 
new
 
products,
 
we
 
expect
 
this
 
trend
 
to
 
continue
 
into
 
the
 
fourth
 
quarter
 
of
 
ﬁscal
 
2025.
 
For
 
the
 
full
 
year,
 
we
 
expect
 
gross
 
margins
 
to
 
be
 
in
 
the
 
mid-70%
 
range.
 
GAAP
 
and
 
non-GAAP
 
operating
 
expenses
 
are
 
expect ed
 
to
 
be
 
approximat ely
 
$4.3
 
billion
 
and
 
$3.0
 
billion,
 
respectively .
 
Full
 
year
 
operating
 
expenses
 
are
 
expect ed
 
to
 
grow
 
in
 
the
 
mid-
 
to
 
upper
 
40%
 
range
 
as
 
we
 
work
 
on
 
developing
 
our
 
next
 
gener ation
 
of
 
products.
 
GAAP
 
and
 
non-GAAP
 
other
 
income
 
and
 
expenses
 
are
 
expect ed
 
to
 
be
 
about
 
$350
 
million,
 
including
 
gains
 
and
 
losses
 
from
 
nonaﬃliat ed
 
investments
 
and
 
publicly
 
held
 
equity
 
securities.
 
GAAP
 
and
 
non-GAAP
 
tax
 
rates
 
are
 
expect ed
 
to
 
be
 
17%,
 
plus
 
or
 
minus
 
1%,
 
excluding
 
any
 
discr ete
 
items.
 
Further
 
ﬁnancial
 
details
 
are
 
included
 
in
 
the
 
CFO
 
commentar y
 
and
 
other
 
information
 
available
 
on
 
our
 
IR
 
websit e.
 
We
 
are
 
now
 
going
 
to
 
open
 
the
 
call
 
for
 
questions.
 
Oper ator,
 
would
 
you
 
please
 
help
 
us
 
poll
 
for
 
questions?
 
Oper ator
 
[Oper ator
 
Instructions]
 
And
 
your
 
ﬁrst
 
question
 
comes
 
from
 
the
 
line
 
of
 
Vivek
 
Arya
 
with
 
Bank
 
of
 
America
 
Securities.
 
Vivek
 
Arya
 
Jensen,
 
you
 
mentioned
 
in
 
the
 
prepared
 
comments
 
that
 
there's
 
a
 
change
 
in
 
the
 
Blackwell
 
GPU
 
mask.
 
I'm
 
curious,
 
are
 
there
 
any
 
other
 
incremental
 
changes
 
in
 
back-end
 
packaging
 
or
 
anything
 
else?
 
And
 
I
 
think
 
related,
 
you
 
suggest ed
 
that
 
you
 
could
 
ship
 
sever al
 
billion
 
dollars
 
of
 
Blackwell
 
in
 
Q4
 
despit e
 
the
 
change
 
in
 
the
 
design.
 
Is
 
it
 
because
 
all
 
these
 
issues
 
will
 
be
 
solved
 
by
 
then?
 
Just
 
help
 
us
 
size
 
what
 
is
 
the
 
overall
 
impact
 
of
 
any
 
changes
 
in
 
Blackwell
 
timing,
 
what
 
that
 
means
 
to
 
your
 
kind
 
of
 
revenue
 
proﬁle
 
and
 
how
 
are
 
customers
 
reacting
 
to
 
it.
 
Jensen
 
Huang
 
Yes.
 
Thanks,
 
Vivek.
 
The
 
change
 
to
 
the
 
mask
 
is
 
complet e.
 
There
 
were
 
no
 
functional
 
changes
 
necessar y.
 
And
 
so
 
we're
 
sampling
 
functional
 
samples
 
of
 
Blackwell,
 
Grace
 
Blackwell,
 
and
 
a
 
variety
 
of
 
system
 
conﬁgur ations
 
as
 
we
 
speak.
 
There
 
are
 
something
 
like
 
100
 
diﬀer ent
 
types
 
of
 
Blackwell-based
 
systems
 
that
 
are
 
built
 
that
 
were
 
shown
 
at
 
Comput ex,
 
and
 
we're
 
enabling
 
our
 
ecosyst em
 
to
 
start
 
sampling
 
those.
 
The
 
functionality
 
of
 
Blackwell
 
is
 
as
 
it
 
is,
 
and
 
we
 
expect
 
to
 
start
 
production
 
in
 
Q4.
 
Oper ator
 
And
 
your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Toshiya
 
Hari
 
with
 
Goldman
 
Sachs.
 
Toshiya
 
Hari
 
Jensen,
 
I
 
had
 
a
 
relatively
 
longer -term
 
question.
 
As
 
you
 
may
 
know ,
 
there's
 
a
 
pretty
 
heated
 
debat e
 
in
 
the
 
mark et
 
on
 
your
 
customers
 
and
 
customers'
 
customers
 
return
 
on
 
investment
 
and
 
what
 
that
 
means
 
for
 
the
 
sustainability
 
of
 
CapEx
 
going
 
forward.
 
Internally
 
at
 
NVIDIA,
 
like
 
what
 
are
 
you
 
guys
 
watching?
 
What's
 
on
 
your
 
dashboar d
 
as
 
you
 
try
 
to
 
gauge
 
customer
 
return
 
and
 
how
 
that
 
impacts
 
CapEx?
 
And
 
then
 
a
 
quick
 
follow-up
 
maybe
 
for
 
Colett e.
 
I
 
think
 
your
 
sover eign
 
AI
 
number
 
for
 
the
 
full
 
year
 
went
 
up
 
maybe
 
a
 
couple
 
of
 
billion.
 
What's
 
driving
 
the
 
improved
 
outlook
 
and
 
how
 
should
 
we
 
think
 
about
 
ﬁscal
 
'26?
 
Jensen
 
Huang
 
Thanks,
 
Toshiya.
 
First
 
of
 
all,
 
when
 
I
 
said
 
ship
 
production
 
in
 
Q4,
 
I
 
mean
 
shipping
 
out,
 
I
 
don't
 
mean
 
starting
 
to
 
ship,
 
but
 
I
 
mean
 
--
 
I
 
don't
 
mean
 
starting
 
production
 
but
 
shipping
 
up.
 
On
 
the
 
longer -term
 
question,
 
let's
 
take
 
a
 
step
 
back.
 
And
 
you've
 
heard
 
me
 
say
 
that
 
we're
 
going
 
through
 
2
 
simultaneous
 
platform
 
transitions
 
at
 
the
 
same
 
time.
 
The
 
ﬁrst
 
1
 
is
 
transitioning
 
from
 
acceler ated
 
computing
 
to
 
--
 
from
 
gener al
 
purpose
 
computing
 
to
 
acceler ated
 
computing.
 
And
 
the
 
reason
 
for
 
that
 
is
 
because
 
CPU
 
scaling
 
has
 
been
 
known
 
to
 
be
 
slowing
 
for
 
some
 
time
 
and
 
it
 
has
 
slowed
 
to
 
a
 
crawl.
 
And
 
yet
 
the
 
amount
 
of
 
computing
 
demand
 
continues
 
to
 
grow
 
quite
 
signiﬁcantly .
 
You
 
could
 
maybe
 
even
 
estimat e
 
it
 
to
 
be
 
doubling
 
every
 
single
 
year.
 
And
 
so
 
if
 
we
 
don't
 
have
 
a
 
new
 
approach,
 
computing
 
inﬂation
 
would
 
be
 
driving
 
up
 
the
 
cost
 
for
 
every
 
company,
 
and
 
it
 
would
 
be
 
driving
 
up
 
the
 
energy
 
consumption
 
of
 
data
 
centers
 
around
 
the
 
world.
 
In
 
fact,
 
you'r e
 
seeing
 
that.
 
And
 
so
 
the
 
answer
 
is
 
acceler ated
 
computing.
 
We
 
know
 
that
 
acceler ated
 
computing,
 
of
 
course,
 
speeds
 
up
 
applications.
 
It
 
also
 
enables
 
you
 
to
 
do
 
computing
 
at
 
a
 
much
 
larger
 
scale,
 
for
 
example,
 
scientiﬁc
 
simulations
 
or
 
database
 
processing,
 
but
 
what
 
that
 
translat es
 
directly
 
to
 
is
 
lower
 
cost
 
and
 
lower
 
energy
 
consumed.
 
And
 
in
 
fact,
 
this
 
week,
 
there's
 
a
 
blog
 
that
 
came
 
out
 
that
 
talked
 
about
 
a
 
whole
 
bunch
 
of
 
new
 
libraries
 
that
 
we
 
oﬀer.
 
And
 
that's
 
really
 
the
 
core
 
of
 
the
 
ﬁrst
 
platform
 
transition,
 
going
 
from
 
gener al
 
purpose
 
computing
 
to
 
acceler ated
 
computing.
 
And
 
it's
 
not
 
unusual
 
to
 
see
 
someone
 
save
 
90%
 
of
 
their
 
computing
 
cost.
 
And
 
the
 
reason
 
for
 
that
 
is,
 
of
 
course,
 
you
 
just
 
sped
 
up
 
an
 
application
 
50x.
 
You
 
would
 
expect
 
the
 
computing
 
cost
 
to
 
decline
 
quite
 
signiﬁcantly .
 
The
 
second
 
was
 
enabled
 
by
 
acceler ated
 
computing
 
because
 
we
 
drove
 
down
 
the
 
cost
 
of
 
training
 
large
 
language
 
models
 
or
 
training
 
deep
 
learning
 
so
 
incredibly
 
that
 
it
 
is
 
now
 
possible
 
to
 
have
 
gigantic
 
scale
 
models,
 
multitrillion-par amet er
 
models
 
and
 
train
 
it
 
on
 
--
 
pretrain
 
it
 
on
 
just
 
about
 
the
 
world's
 
knowledge
 
corpus
 
and
 
let
 
the
 
model
 
go
 
ﬁgure
 
out
 
how
 
to
 
understand
 
human
 
language
 
representation
 
and
 
how
 
to
 
codify
 
knowledge
 
into
 
its
 
neural
 
networks
 
and
 
how
 
to
 
learn
 
reasoning,
 
and
 
so
 
which
 
caused
 
the
 
gener ative
 
AI
 
revolution.
 
Now
 
gener ative
 
AI,
 
taking
 
a
 
step
 
back
 
about
 
why
 
it
 
is
 
that
 
we
 
went
 
so
 
deeply
 
into
 
it
 
is
 
because
 
it's
 
not
 
just
 
a
 
featur e,
 
it's
 
not
 
just
 
the
 
capability .
 
It's
 
a
 
fundamental
 
new
 
way
 
of
 
doing
 
softwar e.
 
Instead
 
of
 
human-engineer ed
 
algorithms,
 
we
 
now
 
have
 
data.
 
We
 
tell
 
the
 
AI,
 
we
 
tell
 
the
 
model,
 
we
 
tell
 
the
 
comput er
 
what
 
are
 
the
 
expect ed
 
answers.
 
What
 
are
 
our
 
previous
 
obser vations?
 
And
 
then
 
for
 
it
 
to
 
ﬁgure
 
out
 
what
 
the
 
algorithm
 
is,
 
what's
 
the
 
function.
 
It
 
learns
 
a
 
universal
 
--
 
AI
 
is
 
a
 
bit
 
of
 
a
 
universal
 
function
 
approximat or
 
and
 
it
 
learns
 
the
 
function.
 
And
 
so
 
you
 
could
 
learn
 
the
 
function
 
of
 
almost
 
anything.
 
And
 
anything
 
that
 
you
 
have
 
that's
 
predictable,
 
anything
 
that
 
has
 
structur e,
 
anything
 
that
 
you
 
have
 
previous
 
examples
 
of.
 
And
 
so
 
now
 
here
 
we
 
are
 
with
 
gener ative
 
AI.
 
It's
 
a
 
fundamental
 
new
 
form
 
of
 
comput er
 
science.
 
It's
 
aﬀecting
 
how
 
every
 
layer
 
of
 
computing
 
is
 
done
 
from
 
CPU
 
to
 
GPU ,
 
from
 
human-engineer ed
 
algorithms
 
to
 
machine-learned
 
algorithms,
 
and
 
the
 
type
 
of
 
applications
 
you
 
could
 
now
 
develop
 
and
 
produce
 
is
 
fundamentally
 
remarkable.
 
And
 
there
 
are
 
sever al
 
things
 
that
 
are
 
happening
 
in
 
gener ative
 
AI.
 
So
 
the
 
ﬁrst
 
thing
 
that's
 
happening
 
is
 
the
 
frontier
 
models
 
are
 
growing
 
in
 
quite
 
substantial
 
scale.
 
And
 
they'r e
 
still
 
seeing
 
--
 
we're
 
still
 
all
 
seeing
 
the
 
beneﬁts
 
of
 
scaling.
 
And
 
whenever
 
you
 
double
 
the
 
size
 
of
 
a
 
model,
 
you
 
also
 
have
 
to
 
more
 
than
 
double
 
the
 
size
 
of
 
the
 
data
 
set
 
to
 
go
 
train
 
it.
 
And
 
so
 
the
 
amount
 
of
 
ﬂops
 
necessar y
 
in
 
order
 
to
 
create
 
that
 
model
 
goes
 
up
 
quadr atically .
 
And
 
so
 
it's
 
not
 
unexpect ed
 
to
 
see
 
that
 
the
 
next-gener ation
 
models
 
could
 
take
 
10x,
 
20x,
 
40x
 
more
 
comput e
 
than
 
last
 
gener ation.
 
So
 
we
 
have
 
to
 
continue
 
to
 
drive
 
the
 
gener ational
 
performance
 
up
 
quite
 
signiﬁcantly
 
so
 
we
 
can
 
drive
 
down
 
the
 
energy
 
consumed
 
and
 
drive
 
down
 
the
 
cost
 
necessar y
 
to
 
do
 
it.
 
And
 
so
 
the
 
ﬁrst
 
1
 
is
 
there
 
are
 
larger
 
frontier
 
models
 
trained
 
on
 
more
 
modalities.
 
And
 
surprisingly,
 
there
 
are
 
more
 
frontier
 
model
 
makers
 
than
 
last
 
year.
 
And
 
so
 
you
 
have
 
more
 
on
 
more
 
on
 
more.
 
That's
 
one
 
of
 
the
 
dynamics
 
going
 
on
 
in
 
gener ative
 
AI.
 
The
 
second
 
is
 
although
 
it's
 
below
 
the
 
tip
 
of
 
the
 
iceber g,
 
what
 
we
 
see
 
are
 
ChatGPT
 
image
 
gener ators.
 
We
 
see
 
coding.
 
We
 
use
 
gener ative
 
AI
 
for
 
coding
 
quite
 
extensively
 
here
 
at
 
NVIDIA
 
now.
 
We,
 
of
 
course,
 
have
 
a
 
lot
 
of
 
digital
 
designers
 
and
 
things
 
like
 
that.
 
But
 
those
 
are
 
kind
 
of
 
the
 
tip
 
of
 
the
 
iceber g.
 
What's
 
below
 
the
 
iceber g
 
are
 
the
 
largest
 
systems,
 
largest
 
computing
 
systems
 
in
 
the
 
world
 
today,
 
which
 
are
 
--
 
and
 
you've
 
heard
 
me
 
talk
 
about
 
this
 
in
 
the
 
past,
 
which
 
are
 
recommender
 
systems
 
moving
 
from
 
CPUs.
 
It's
 
now
 
moving
 
from
 
CPUs
 
to
 
gener ative
 
AI.
 
So
 
recommender
 
systems,
 
ad
 
gener ation,
 
custom
 
ad
 
gener ation
 
targeting
 
ads
 
at
 
very
 
large
 
scale
 
and
 
quite
 
hyper -targeting,
 
search
 
and
 
user-gener ated
 
content,
 
these
 
are
 
all
 
very
 
large-scale
 
applications
 
have
 
now
 
evolved
 
to
 
gener ative
 
AI.
 
Of
 
course,
 
the
 
number
 
of
 
gener ative
 
AI
 
start-ups
 
is
 
gener ating
 
tens
 
of
 
billions
 
of
 
dollars
 
of
 
cloud
 
renting
 
oppor tunities
 
for
 
our
 
cloud
 
partners.
 
And
 
sover eign
 
AI,
 
countries
 
that
 
are
 
now
 
realizing
 
that
 
their
 
data
 
is
 
their
 
natur al
 
and
 
national
 
resour ce
 
and
 
they
 
have
 
to
 
use
 
AI,
 
build
 
their
 
own
 
AI
 
infrastructur e
 
so
 
that
 
they
 
could
 
have
 
their
 
own
 
digital
 
intelligence.
 
Enterprise
 
AI,
 
as
 
Colett e
 
mentioned
 
earlier ,
 
is
 
starting,
 
and
 
you
 
might
 
have
 
seen
 
our
 
announcement
 
that
 
the
 
world's
 
leading
 
IT
 
companies
 
are
 
joining
 
us
 
to
 
take
 
the
 
NVIDIA
 
AI
 
Enterprise
 
platform
 
to
 
the
 
world's
 
enterprises.
 
The
 
companies
 
that
 
we're
 
talking
 
to,
 
so
 
many
 
of
 
them
 
are
 
just
 
so
 
incredibly
 
excited
 
to
 
drive
 
more
 
productivity
 
out
 
of
 
the
 
company .
 
And
 
then
 
gener al
 
robotics.
 
The
 
big
 
transformation
 
last
 
year
 
as
 
we
 
are
 
able
 
to
 
now
 
learn
 
physical
 
AI
 
from
 
watching
 
video
 
and
 
human
 
demonstr ation
 
and
 
synthetic
 
data
 
gener ation
 
from
 
reinfor cement
 
learning
 
from
 
systems
 
like
 
Omniverse,
 
we
 
are
 
now
 
able
 
to
 
work
 
with
 
just
 
about
 
every
 
robotics
 
companies
 
now
 
to
 
start
 
thinking
 
about,
 
start
 
building
 
gener al
 
robotics.
 
And
 
so
 
you
 
can
 
see
 
that
 
there
 
are
 
just
 
so
 
many
 
diﬀer ent
 
directions
 
that
 
gener ative
 
AI
 
is
 
going.
 
And
 
so
 
we're
 
actually
 
seeing
 
the
 
momentum
 
of
 
gener ative
 
AI
 
acceler ating.
 
Colett e
 
Kress
 
And
 
Toshiya,
 
to
 
answer
 
your
 
question
 
regarding
 
sover eign
 
AI
 
and
 
our
 
goals
 
in
 
terms
 
of
 
growth,
 
in
 
terms
 
of
 
revenue,
 
it
 
certainly
 
is
 
a
 
unique
 
and
 
growing
 
oppor tunity,
 
something
 
that
 
surfaced
 
with
 
gener ative
 
AI
 
and
 
the
 
desir es
 
of
 
countries
 
around
 
the
 
world
 
to
 
have
 
their
 
own
 
gener ative
 
AI
 
that
 
would
 
be
 
able
 
to
 
incorpor ate
 
their
 
own
 
language,
 
incorpor ate
 
their
 
own
 
cultur e,
 
incorpor ate
 
their
 
own
 
data
 
in
 
that
 
countr y.
 
So
 
more
 
and
 
more
 
excitement
 
around
 
these
 
models
 
and
 
what
 
they
 
can
 
be
 
speciﬁc
 
for
 
those
 
countries.
 
So
 
yes,
 
we
 
are
 
seeing
 
some
 
growth
 
oppor tunity
 
in
 
front
 
of
 
us.
 
Oper ator
 
And
 
your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Joe
 
Moor e
 
with
 
Morgan
 
Stanley .
 
Joseph
 
Moor e
 
Jensen,
 
in
 
the
 
press
 
release,
 
you
 
talked
 
about
 
Blackwell
 
anticipation
 
being
 
incredible.
 
But
 
it
 
seems
 
like
 
Hopper
 
demand
 
is
 
also
 
really
 
strong.
 
I
 
mean,
 
you'r e
 
guiding
 
for
 
a
 
very
 
strong
 
quarter
 
without
 
Blackwell
 
in
 
October.
 
So
 
how
 
long
 
do
 
you
 
see
 
sort
 
of
 
coexisting
 
strong
 
demand
 
for
 
both?
 
And
 
can
 
you
 
talk
 
about
 
the
 
transition
 
to
 
Blackwell?
 
Do
 
you
 
see
 
people
 
intermixing
 
clust ers?
 
Do
 
you
 
think
 
most
 
of
 
the
 
Blackwell
 
activities,
 
new
 
clust ers?
 
Just
 
some
 
sense
 
of
 
what
 
that
 
transition
 
looks
 
like.
 
Jensen
 
Huang
 
Yes.
 
Thanks,
 
Joe.
 
The
 
demand
 
for
 
Hopper
 
is
 
really
 
strong.
 
And
 
it's
 
true,
 
the
 
demand
 
for
 
Blackwell
 
is
 
incredible.
 
There's
 
a
 
couple
 
of
 
reasons
 
for
 
that.
 
The
 
ﬁrst
 
reason
 
is
 
if
 
you
 
just
 
look
 
at
 
the
 
world's
 
cloud
 
service
 
providers,
 
the
 
amount
 
of
 
GPU
 
capacity
 
they
 
have
 
available,
 
it's
 
basically
 
none.
 
And
 
the
 
reason
 
for
 
that
 
is
 
because
 
they'r e
 
either
 
being
 
deployed
 
internally
 
for
 
acceler ating
 
their
 
own
 
workloads,
 
data
 
processing,
 
for
 
example.
 
Data
 
processing,
 
we
 
hardly
 
ever
 
talk
 
about
 
it
 
because
 
it's
 
mundane.
 
It's
 
not
 
very
 
cool
 
because
 
it
 
doesn't
 
gener ate
 
a
 
pictur e
 
or
 
gener ate
 
words.
 
But
 
almost
 
every
 
single
 
company
 
in
 
the
 
world
 
processes
 
data
 
in
 
the
 
backgr ound.
 
And
 
NVIDIA's
 
GPUs
 
are
 
the
 
only
 
acceler ators
 
on
 
the
 
planet
 
that
 
process
 
and
 
acceler ate
 
data.
 
SQL
 
data,
 
Panda's
 
data,
 
data
 
science
 
toolkits
 
like
 
Panda's
 
and
 
the
 
new
 
one,
 
Polar's.
 
These
 
are
 
the
 
ones
 
--
 
the
 
most
 
popular
 
data
 
processing
 
platforms
 
in
 
the
 
world.
 
And
 
aside
 
from
 
CPUs,
 
which
 
as
 
I've
 
mentioned
 
befor e,
 
really
 
running
 
out
 
of
 
steam,
 
NVIDIA's
 
acceler ated
 
computing
 
is
 
really
 
the
 
only
 
way
 
to
 
get
 
boosting
 
performance
 
out
 
of
 
that.
 
And
 
so
 
that's
 
number
 
1
 
is
 
the
 
primar y
 
--
 
the
 
#1
 
use
 
case
 
long
 
befor e
 
gener ative
 
AI
 
along
 
is
 
that
 
the
 
migration
 
of
 
applications
 
one
 
after
 
another
 
to
 
acceler ated
 
computing.
 
The
 
second
 
is,
 
of
 
course,
 
the
 
rentals.
 
They'r e
 
renting
 
capacity
 
to
 
model
 
makers,
 
they'r e
 
renting
 
it
 
to
 
start-up
 
companies.
 
And
 
a
 
gener ative
 
AI
 
company
 
spends
 
the
 
vast
 
majority
 
of
 
their
 
invest ed
 
capital
 
into
 
infrastructur e
 
so
 
that
 
they
 
could
 
use
 
an
 
AI
 
to
 
help
 
them
 
create
 
products.
 
And
 
so
 
these
 
companies
 
need
 
it
 
now.
 
They
 
just
 
simply
 
can't
 
aﬀord
 
--
 
you
 
just
 
raise
 
money,
 
they
 
want
 
you
 
to
 
put
 
it
 
to
 
use
 
now.
 
You
 
have
 
processing
 
that
 
you
 
have
 
to
 
do.
 
You
 
can't
 
do
 
it
 
next
 
year,
 
you
 
got
 
to
 
do
 
it
 
today .
 
And
 
so
 
there's
 
a
 
fair
 
--
 
that's
 
1
 
reason.
 
The
 
second
 
reason
 
for
 
Hopper
 
demand
 
right
 
now
 
is
 
because
 
of
 
the
 
race
 
to
 
the
 
next
 
plateau.
 
The
 
ﬁrst
 
person
 
to
 
the
 
next
 
plateau
 
gets
 
to
 
be
 
--
 
get
 
to
 
introduce
 
a
 
revolutionar y
 
level
 
of
 
AI.
 
The
 
second
 
person
 
who
 
gets
 
there
 
is
 
incrementally
 
better
 
or
 
about
 
the
 
same.
 
And
 
so
 
the
 
ability
 
to
 
systematically
 
and
 
consist ently
 
race
 
to
 
the
 
next
 
plateau
 
and
 
be
 
the
 
ﬁrst
 
one
 
there
 
is
 
how
 
you
 
establish
 
leadership.
 
NVIDIA
 
is
 
constantly
 
doing
 
that,
 
and
 
we
 
show
 
that
 
to
 
the
 
world
 
and
 
the
 
GPUs
 
we
 
make
 
and
 
the
 
AI
 
factories
 
that
 
we
 
make,
 
the
 
networking
 
systems
 
that
 
we
 
make,
 
the
 
SoCs
 
we
 
create.
 
I
 
mean,
 
we
 
want
 
to
 
set
 
the
 
pace.
 
We
 
want
 
to
 
be
 
consist ently
 
the
 
world's
 
best.
 
And
 
that's
 
the
 
reason
 
why
 
we
 
drive
 
ourselves
 
so
 
hard.
 
Of
 
course,
 
we
 
also
 
want
 
to
 
see
 
our
 
dreams
 
come
 
true
 
and
 
all
 
of
 
the
 
capabilities
 
that
 
we
 
imagine
 
in
 
the
 
future
 
and
 
the
 
beneﬁts
 
that
 
we
 
can
 
bring
 
to
 
society,
 
we
 
want
 
to
 
see
 
all
 
that
 
come
 
true.
 
And
 
so
 
these
 
model
 
makers
 
are
 
the
 
same.
 
Of
 
course,
 
they
 
want
 
to
 
be
 
the
 
world's
 
best.
 
They
 
want
 
to
 
be
 
the
 
world's
 
ﬁrst.
 
And
 
although
 
Blackwell
 
will
 
start
 
shipping
 
out
 
in
 
billions
 
of
 
dollars
 
at
 
the
 
end
 
of
 
this
 
year,
 
the
 
standing
 
up
 
of
 
the
 
capacity
 
is
 
still
 
probably
 
weeks
 
and
 
a
 
month
 
or
 
so
 
away .
 
And
 
so
 
between
 
now
 
and
 
then
 
is
 
a
 
lot
 
of
 
gener ative
 
AI
 
mark et
 
dynamic.
 
And
 
so
 
everybody
 
is
 
just
 
really
 
in
 
a
 
hurry.
 
It's
 
either
 
operational
 
reasons
 
that
 
they
 
need
 
it.
 
They
 
need
 
acceler ated
 
computing.
 
They
 
don't
 
want
 
to
 
build
 
any
 
more
 
gener al
 
purpose
 
computing
 
infrastructur e
 
and
 
even
 
Hopper .
 
Of
 
course,
 
H200
 
is
 
state
 
of
 
the
 
art.
 
Hopper ,
 
if
 
you
 
have
 
a
 
choice
 
between
 
building
 
CPU
 
infrastructur e
 
right
 
now
 
for
 
business
 
or
 
Hopper
 
infrastructur e
 
for
 
business
 
right
 
now,
 
that
 
decision
 
is
 
relatively
 
clear .
 
And
 
so
 
I
 
think
 
people
 
are
 
just
 
clamoring
 
to
 
transition
 
the
 
$1
 
trillion
 
of
 
established
 
installed
 
infrastructur e
 
to
 
a
 
modern
 
infrastructur e
 
and
 
Hopper's
 
state
 
of
 
the
 
art.
 
Oper ator
 
And
 
your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Matt
 
Ramsay
 
with
 
TD
 
Cowen.
 
Matthew
 
Ramsay
 
Jensen,
 
want ed
 
to
 
kind
 
of
 
circle
 
back
 
to
 
an
 
earlier
 
question
 
about
 
the
 
debat e
 
that
 
invest ors
 
are
 
having
 
about
 
the
 
ROI
 
on
 
all
 
of
 
this
 
CapEx.
 
And
 
hopefully,
 
this
 
question
 
and
 
the
 
distinction
 
will
 
make
 
some
 
sense.
 
But
 
what
 
I'm
 
having
 
discussions
 
about
 
is
 
with
 
like
 
the
 
percentage
 
of
 
folks
 
that
 
you
 
see
 
that
 
are
 
spending
 
all
 
of
 
this
 
money
 
and
 
looking
 
to
 
sort
 
of
 
push
 
the
 
frontier
 
towar ds
 
AGI
 
conver gence
 
and,
 
as
 
you
 
just
 
said,
 
a
 
new
 
plateau
 
in
 
capability,
 
and
 
they'r e
 
going
 
to
 
spend
 
regardless
 
to
 
get
 
to
 
that
 
level
 
of
 
capability
 
because
 
it
 
opens
 
up
 
so
 
many
 
doors
 
for
 
the
 
industr y
 
and
 
for
 
their
 
company
 
versus
 
customers
 
that
 
are
 
really,
 
really
 
focused
 
today
 
on
 
CapEx
 
versus
 
ROI.
 
I
 
don't
 
know
 
if
 
that
 
distinction
 
makes
 
sense.
 
I'm
 
just
 
trying
 
to
 
get
 
a
 
sense
 
of
 
how
 
you'r e
 
seeing
 
the
 
priorities
 
of
 
people
 
that
 
are
 
putting
 
the
 
dollars
 
in
 
the
 
ground
 
on
 
this
 
new
 
technology
 
and
 
what
 
their
 
priorities
 
are
 
and
 
their
 
time
 
frames
 
are
 
for
 
that
 
investment.
 
Jensen
 
Huang
 
Thanks,
 
Matt.
 
The
 
people
 
who
 
are
 
investing
 
in
 
NVIDIA
 
infrastructur e
 
are
 
getting
 
returns
 
on
 
it
 
right
 
away .
 
It's
 
the
 
best
 
ROI
 
infrastructur e,
 
computing
 
infrastructur e
 
investment
 
you
 
can
 
make
 
today .
 
And
 
so
 
1
 
way
 
to
 
think
 
through
 
it,
 
probably
 
the
 
most
 
--
 
the
 
easiest
 
way
 
to
 
think
 
through
 
it
 
is
 
just
 
go
 
back
 
to
 
ﬁrst
 
principles.
 
You
 
have
 
$1
 
trillion
 
worth
 
of
 
gener al
 
purpose
 
computing
 
infrastructur e.
 
And
 
the
 
question
 
is,
 
do
 
you
 
want
 
to
 
build
 
more
 
of
 
that
 
or
 
not?
 
And
 
for
 
every
 
$1
 
billion
 
worth
 
of
 
Juniper
 
CPU-based
 
infrastructur e
 
that
 
you
 
stand
 
up,
 
you
 
probably
 
rent
 
it
 
for
 
less
 
than
 
$1
 
billion.
 
And
 
so
 
because
 
it's
 
commoditiz ed,
 
there's
 
already
 
$1
 
trillion
 
on
 
the
 
ground.
 
What's
 
the
 
point
 
of
 
getting
 
more?
 
And
 
so
 
the
 
people
 
who
 
are
 
clamoring
 
to
 
get
 
this
 
infrastructur e,
 
one,
 
when
 
they
 
build
 
out
 
Hopper -based
 
infrastructur e
 
and
 
soon,
 
Blackwell-based
 
infrastructur e,
 
they
 
start
 
saving
 
money .
 
That's
 
tremendous
 
return
 
on
 
investment.
 
And
 
the
 
reason
 
why
 
they
 
start
 
saving
 
money
 
is
 
because
 
data
 
processing
 
saves
 
money,
 
and
 
data
 
processing
 
is
 
probably
 
just
 
a
 
giant
 
part
 
of
 
it
 
already .
 
And
 
so
 
recommender
 
systems
 
save
 
money,
 
so
 
on
 
and
 
so
 
forth,
 
okay?
 
And
 
so
 
you
 
start
 
saving
 
money .
 
The
 
second
 
thing
 
is
 
everything
 
you
 
stand
 
up
 
are
 
going
 
to
 
get
 
rented
 
because
 
so
 
many
 
companies
 
are
 
being
 
founded
 
to
 
create
 
gener ative
 
AI.
 
And
 
so
 
your
 
capacity
 
gets
 
rented
 
right
 
away
 
and
 
the
 
return
 
on
 
investment
 
of
 
that
 
is
 
really
 
good.
 
And
 
then
 
the
 
third
 
reason
 
is
 
your
 
own
 
business.
 
Do
 
you
 
want
 
to
 
either
 
create
 
the
 
next
 
frontier
 
yourself
 
or
 
your
 
own
 
Internet
 
services
 
beneﬁt
 
from
 
a
 
next-gener ation
 
ad
 
system
 
or
 
a
 
next-gener ation
 
recommender
 
system
 
or
 
a
 
next-gener ation
 
search
 
system?
 
So
 
for
 
your
 
own
 
services,
 
for
 
your
 
own
 
stores,
 
for
 
your
 
own
 
user-gener ated
 
content,
 
social
 
media
 
platforms,
 
for
 
your
 
own
 
services,
 
gener ative
 
AI
 
is
 
also
 
a
 
fast
 
ROI.
 
And
 
so
 
there's
 
a
 
lot
 
of
 
ways
 
you
 
could
 
think
 
through
 
it.
 
But
 
at
 
the
 
core,
 
it's
 
because
 
it
 
is
 
the
 
best
 
computing
 
infrastructur e
 
you
 
could
 
put
 
in
 
the
 
ground
 
today .
 
The
 
world
 
of
 
gener al
 
purpose
 
computing
 
is
 
shifting
 
to
 
acceler ated
 
computing.
 
The
 
world
 
of
 
human-engineer ed
 
softwar e
 
is
 
moving
 
to
 
gener ative
 
AI
 
softwar e.
 
If
 
you
 
were
 
to
 
build
 
infrastructur e
 
to
 
moderniz e
 
your
 
cloud
 
and
 
your
 
data
 
centers,
 
build
 
it
 
with
 
acceler ated
 
computing
 
NVIDIA.
 
That's
 
the
 
best
 
way
 
to
 
do
 
it.
 
Oper ator
 
And
 
your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Timothy
 
Arcuri
 
with
 
UBS.
 
Timothy
 
Arcuri
 
I
 
had
 
a
 
question
 
on
 
the
 
shape
 
of
 
the
 
revenue
 
growth,
 
both
 
near
 
and
 
longer
 
term.
 
I
 
know
 
Colett e,
 
you
 
did
 
increase
 
OpEx
 
for
 
the
 
year.
 
And
 
if
 
I
 
look
 
at
 
the
 
increase
 
in
 
your
 
purchase
 
commitments
 
and
 
your
 
supply
 
obligations,
 
that's
 
also
 
quite
 
bullish.
 
On
 
the
 
other
 
hand,
 
there's
 
some
 
school
 
of
 
thought
 
that
 
not
 
that
 
many
 
customers
 
really
 
seem
 
ready
 
for
 
liquid
 
cooling,
 
and
 
I
 
do
 
recogniz e
 
that
 
some
 
of
 
these
 
racks
 
can
 
be
 
air
 
cooled.
 
But
 
Jensen,
 
is
 
that
 
something
 
to
 
consider
 
sort
 
of
 
on
 
the
 
shape
 
of
 
how
 
Blackwell
 
is
 
going
 
to
 
ramp?
 
And
 
then
 
I
 
guess
 
when
 
you
 
look
 
beyond
 
next
 
year,
 
which
 
is
 
obviously
 
going
 
to
 
be
 
a
 
great
 
year
 
and
 
you
 
look
 
into
 
'26,
 
do
 
you
 
worry
 
about
 
any
 
other
 
gating
 
factors
 
like,
 
say,
 
the
 
power
 
supply
 
chain
 
or
 
at
 
some
 
point,
 
models
 
start
 
to
 
get
 
smaller?
 
I'm
 
just
 
wondering
 
if
 
you
 
can
 
speak
 
to
 
that.
 
Jensen
 
Huang
 
I'm
 
going
 
to
 
work
 
backwar ds.
 
I
 
really
 
appreciat e
 
the
 
question,
 
Tim.
 
So
 
remember ,
 
the
 
world
 
is
 
moving
 
from
 
gener al
 
purpose
 
computing
 
to
 
acceler ated
 
computing.
 
And
 
the
 
world
 
builds
 
about
 
$1
 
trillion
 
worth
 
of
 
data
 
centers.
 
$1
 
trillion
 
worth
 
of
 
data
 
centers
 
in
 
a
 
few
 
years
 
will
 
be
 
all
 
acceler ated
 
computing.
 
In
 
the
 
past,
 
no
 
GPUs
 
are
 
in
 
data
 
centers,
 
just
 
CPUs.
 
In
 
the
 
future,
 
every
 
single
 
data
 
center
 
will
 
have
 
GPUs.
 
And
 
the
 
reason
 
for
 
that
 
is
 
very
 
clear:
 
because
 
we
 
need
 
to
 
acceler ate
 
workloads
 
so
 
that
 
we
 
can
 
continue
 
to
 
be
 
sustainable,
 
continue
 
to
 
drive
 
down
 
the
 
cost
 
of
 
computing
 
so
 
that
 
when
 
we
 
do
 
more
 
computing,
 
we
 
don't
 
experience
 
computing
 
inﬂation.
 
Second,
 
we
 
need
 
GPUs
 
for
 
a
 
new
 
computing
 
model
 
called
 
gener ative
 
AI
 
that
 
we
 
could
 
all
 
acknowledge
 
is
 
going
 
to
 
be
 
quite
 
transformative
 
to
 
the
 
future
 
of
 
computing.
 
And
 
so
 
I
 
think
 
working
 
backwar ds,
 
the
 
way
 
to
 
think
 
about
 
that
 
is
 
the
 
next
 
$1
 
trillion
 
of
 
the
 
world's
 
infrastructur e
 
will
 
clearly
 
be
 
diﬀer ent
 
than
 
the
 
last
 
$1
 
trillion,
 
and
 
it
 
will
 
be
 
vastly
 
acceler ated.
 
With
 
respect
 
to
 
the
 
shape
 
of
 
our
 
ramp,
 
we
 
oﬀer
 
multiple
 
conﬁgur ations
 
of
 
Blackwell.
 
Blackwell
 
comes
 
in
 
either
 
a
 
Blackwell
 
classic,
 
if
 
you
 
will,
 
that
 
uses
 
the
 
HGX
 
form
 
factor
 
that
 
we
 
pioneer ed
 
with
 
Volta.
 
And
 
I
 
think
 
it
 
was
 
Volta.
 
And
 
so
 
we've
 
been
 
shipping
 
the
 
HGX
 
form
 
factor
 
for
 
some
 
time.
 
It
 
is
 
air
 
cooled.
 
The
 
Grace
 
Blackwell
 
is
 
liquid
 
cooled.
 
However ,
 
the
 
number
 
of
 
data
 
centers
 
that
 
want
 
to
 
go
 
to
 
liquid
 
cooled
 
is
 
quite
 
signiﬁcant.
 
And
 
the
 
reason
 
for
 
that
 
is
 
because
 
we
 
can,
 
in
 
a
 
liquid
 
cooled
 
data
 
center,
 
in
 
any
 
data
 
center
 
--
 
power -limit ed
 
data
 
center,
 
what ever
 
size
 
data
 
center
 
you
 
choose,
 
you
 
could
 
install
 
and
 
deploy
 
anywher e
 
from
 
3
 
to
 
5x
 
the
 
AI
 
throughput
 
compar ed
 
to
 
the
 
past.
 
And
 
so
 
liquid
 
cooling
 
is
 
cheaper .
 
Liquid
 
cooling,
 
our
 
TCO
 
is
 
better,
 
and
 
liquid
 
cooling
 
allows
 
you
 
to
 
have
 
the
 
beneﬁt
 
of
 
this
 
capability
 
we
 
call
 
NVLink,
 
which
 
allows
 
us
 
to
 
expand
 
it
 
to
 
72
 
Grace
 
Blackwell
 
packages,
 
which
 
has
 
essentially
 
144
 
GPUs.
 
And
 
so
 
imagine
 
144
 
GPUs
 
connect ed
 
in
 
NVLink.
 
And
 
that,
 
we're
 
increasingly
 
showing
 
you
 
the
 
beneﬁts
 
of
 
that.
 
And
 
the
 
next
 
click
 
is
 
obviously
 
very
 
low
 
latency,
 
very
 
high
 
throughput
 
large
 
language
 
model
 
inference,
 
and
 
the
 
large
 
NVLink
 
domain
 
is
 
going
 
to
 
be
 
a
 
game
 
changer
 
for
 
that.
 
And
 
so
 
I
 
think
 
people
 
are
 
very
 
comfor table
 
deploying
 
both.
 
And
 
so
 
almost
 
every
 
CSP
 
we're
 
working
 
with
 
are
 
deploying
 
some
 
of
 
both.
 
And
 
so
 
I'm
 
pretty
 
conﬁdent
 
that
 
we'll
 
ramp
 
it
 
up
 
just
 
ﬁne.
 
Your
 
second
 
question
 
out
 
of
 
the
 
third
 
is
 
that
 
looking
 
forward,
 
yes,
 
next
 
year
 
is
 
going
 
to
 
be
 
a
 
great
 
year.
 
We
 
expect
 
to
 
grow
 
our
 
Data
 
Cent er
 
business
 
quite
 
signiﬁcantly
 
next
 
year.
 
Blackwell
 
is
 
going
 
to
 
be
 
a
 
complet e
 
game
 
changer
 
for
 
the
 
industr y.
 
And
 
Blackwell
 
is
 
going
 
to
 
carry
 
into
 
the
 
following
 
year.
 
And
 
as
 
I
 
mentioned
 
earlier ,
 
working
 
backwar ds
 
from
 
ﬁrst
 
principles,
 
remember
 
that
 
computing
 
is
 
going
 
through
 
2
 
platform
 
transitions
 
at
 
the
 
same
 
time.
 
And
 
that's
 
just
 
really,
 
really
 
impor tant
 
to
 
keep
 
your
 
head
 
on
 
--
 
your
 
mind
 
focused
 
on,
 
which
 
is
 
gener al
 
purpose
 
computing
 
is
 
shifting
 
to
 
acceler ated
 
computing,
 
and
 
human-engineer ed
 
softwar e
 
is
 
going
 
to
 
transition
 
to
 
gener ative
 
AI
 
or
 
artiﬁcial
 
intelligence-learned
 
softwar e.
 
Okay .
 
Oper ator
 
And
 
your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Stacy
 
Rasgon
 
with
 
Bernst ein
 
Resear ch.
 
Stacy
 
Rasgon
 
I
 
have
 
2
 
short
 
questions
 
for
 
Colett e.
 
The
 
ﬁrst
 
sever al
 
billion
 
dollars
 
of
 
Blackwell
 
revenue
 
in
 
Q4,
 
is
 
that
 
additive?
 
You
 
said
 
you
 
expect ed
 
Hopper
 
demand
 
to
 
strengthen
 
in
 
the
 
second
 
half.
 
Does
 
that
 
mean
 
Hopper
 
strengthens
 
Q3
 
to
 
Q4
 
as
 
well
 
on
 
top
 
of
 
Blackwell
 
adding
 
sever al
 
billion
 
dollars?
 
And
 
the
 
second
 
question
 
on
 
gross
 
margins.
 
If
 
I
 
have
 
mid-70s
 
for
 
the
 
year,
 
let's
 
say,
 
wher e
 
I
 
want
 
to
 
draw
 
that,
 
if
 
I
 
have
 
[
 
75
 
]
 
for
 
the
 
year,
 
I'd
 
be
 
something
 
like
 
71
 
to
 
72
 
for
 
Q4,
 
somewher e
 
in
 
that
 
range.
 
Is
 
that
 
the
 
kind
 
of
 
exit
 
rate
 
for
 
gross
 
margins
 
that
 
you'r e
 
expecting?
 
And
 
how
 
should
 
we
 
think
 
about
 
the
 
drivers
 
of
 
gross
 
margin
 
evolution
 
into
 
next
 
year
 
as
 
Blackwell
 
ramps?
 
And
 
I
 
mean,
 
hopefully,
 
I
 
guess
 
the
 
yields
 
and
 
the
 
invent ory
 
reserves
 
and
 
everything
 
come
 
up.
 
Colett e
 
Kress
 
Yes.
 
So
 
Stacy,
 
let's
 
ﬁrst
 
take
 
your
 
question
 
that
 
you
 
had
 
about
 
Hopper
 
and
 
Blackwell.
 
So
 
we
 
believe
 
our
 
Hopper
 
will
 
continue
 
to
 
grow
 
into
 
the
 
second
 
half.
 
We
 
have
 
many
 
new
 
products
 
for
 
Hopper ,
 
our
 
existing
 
products
 
for
 
Hopper
 
that
 
we
 
believe
 
will
 
start
 
continuing
 
to
 
ramp
 
in
 
the
 
next
 
quarters,
 
including
 
our
 
Q3
 
and
 
those
 
new
 
products
 
moving
 
to
 
Q4.
 
So
 
let's
 
say,
 
Hopper
 
there
 
for
 
versus
 
H1
 
is
 
a
 
growth
 
oppor tunity
 
for
 
that.
 
Additionally,
 
we
 
have
 
the
 
Blackwell
 
on
 
top
 
of
 
that,
 
and
 
the
 
Blackwell
 
starting
 
ramping
 
in
 
Q4.
 
So
 
I
 
hope
 
that
 
helps
 
you
 
on
 
those
 
2
 
pieces.
 
Your
 
second
 
piece
 
is
 
in
 
terms
 
of
 
our
 
gross
 
margin.
 
We
 
provided
 
gross
 
margin
 
for
 
our
 
Q3.
 
We
 
provided
 
our
 
gross
 
margin
 
on
 
a
 
non-GAAP
 
at
 
about
 
75.
 
We'll
 
work
 
with
 
all
 
the
 
diﬀer ent
 
transitions
 
that
 
we're
 
going
 
through,
 
but
 
we
 
do
 
believe
 
we
 
can
 
do
 
that
 
75
 
in
 
Q3.
 
We
 
provided
 
that
 
we're
 
still
 
on
 
track
 
for
 
the
 
full
 
year
 
also
 
in
 
the
 
mid-70s
 
or
 
approximat ely
 
the
 
75.
 
So
 
we're
 
going
 
to
 
see
 
some
 
slight
 
diﬀer ence
 
possibly
 
in
 
Q4,
 
again
 
with
 
our
 
transitions
 
and
 
the
 
diﬀer ent
 
cost
 
structur es
 
that
 
we
 
have
 
on
 
our
 
new
 
product
 
introductions.
 
However ,
 
I'm
 
not
 
in
 
the
 
same
 
number
 
that
 
you
 
are
 
there.
 
We
 
don't
 
have
 
exactly
 
guidance,
 
but
 
I
 
do
 
believe
 
you'r e
 
lower
 
than
 
wher e
 
we
 
are.
 
Oper ator
 
And
 
your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Ben
 
Reitz es
 
with
 
Melius.
 
Benjamin
 
Reitz es
 
I
 
want ed
 
to
 
ask
 
about
 
the
 
geogr aphies.
 
There
 
was
 
the
 
10-Q
 
that
 
came
 
out,
 
and
 
the
 
United
 
States
 
was
 
down
 
sequentially
 
while
 
sever al
 
Asian
 
geogr aphies
 
were
 
up
 
a
 
lot
 
sequentially .
 
Just
 
wondering
 
what
 
the
 
dynamics
 
are
 
there.
 
And
 
obviously,
 
China
 
did
 
very
 
well.
 
You
 
mentioned
 
it
 
in
 
your
 
remarks.
 
What
 
are
 
the
 
puts
 
and
 
takes?
 
And
 
then
 
I
 
just
 
want ed
 
to
 
clarify
 
from
 
Stacy's
 
question
 
if
 
that
 
means
 
the
 
sequential
 
overall
 
revenue
 
growth
 
rates
 
for
 
the
 
company
 
acceler ate
 
in
 
the
 
fourth
 
quarter,
 
given
 
all
 
those
 
favor able
 
revenue
 
dynamics.
 
Colett e
 
Kress
 
Let
 
me
 
talk
 
about
 
a
 
bit
 
in
 
terms
 
of
 
our
 
disclosur e
 
in
 
terms
 
of
 
the
 
10-Q,
 
a
 
requir ed
 
disclosur e
 
in
 
a
 
choice
 
of
 
geogr aphies.
 
Very
 
challenging
 
sometimes
 
to
 
create
 
that
 
right
 
disclosur e
 
as
 
we
 
have
 
to
 
come
 
up
 
with
 
1
 
key
 
piece.
 
The
 
pieces
 
in
 
terms
 
of
 
we
 
have
 
in
 
terms
 
of
 
who
 
we
 
sell
 
to
 
and/or
 
speciﬁcally
 
who
 
we
 
invoice
 
to,
 
and
 
so
 
what
 
you'r e
 
seeing
 
in
 
terms
 
of
 
there
 
is
 
who
 
we
 
invoice.
 
That's
 
not
 
necessarily
 
wher e
 
the
 
product
 
will
 
eventually
 
be
 
and
 
wher e
 
it
 
may
 
even
 
travel
 
to
 
the
 
end
 
customer .
 
These
 
are
 
just
 
moving
 
to
 
our
 
OEMs
 
or
 
ODMs
 
and
 
our
 
system
 
integrators
 
for
 
the
 
most
 
part
 
across
 
our
 
product
 
portfolio.
 
So
 
what
 
you'r e
 
seeing
 
there
 
is
 
sometimes
 
just
 
a
 
swift
 
shift
 
in
 
terms
 
of
 
who
 
they
 
are
 
using
 
to
 
complet e
 
their
 
full
 
conﬁgur ation
 
befor e
 
those
 
things
 
are
 
going
 
into
 
the
 
data
 
center,
 
going
 
into
 
notebooks
 
and
 
those
 
pieces
 
of
 
it.
 
And
 
that
 
shift
 
happens
 
from
 
time
 
to
 
time.
 
But
 
yes,
 
our
 
China
 
number
 
there
 
are
 
invoicing
 
to
 
China.
 
Keep
 
in
 
mind
 
that
 
is
 
incorpor ating
 
both
 
gaming,
 
also
 
Data
 
Cent er,
 
also
 
automotive
 
in
 
those
 
numbers
 
that
 
we
 
have.
 
Going
 
back
 
to
 
your
 
statement
 
and
 
regarding
 
gross
 
margin
 
and
 
also
 
what
 
we're
 
seeing
 
in
 
terms
 
of
 
what
 
we're
 
looking
 
at
 
for
 
Hopper
 
and
 
Blackwell
 
in
 
terms
 
of
 
revenue.
 
Hopper
 
will
 
continue
 
to
 
grow
 
in
 
the
 
second
 
half.
 
We'll
 
continue
 
to
 
grow
 
from
 
what
 
we
 
are
 
currently
 
seeing.
 
Determining
 
that
 
exact
 
mix
 
in
 
each
 
Q3
 
and
 
Q4,
 
we
 
don't
 
have
 
here.
 
We
 
are
 
not
 
here
 
to
 
guide
 
yet
 
in
 
terms
 
of
 
Q4.
 
But
 
we
 
do
 
see
 
right
 
now
 
the
 
demand
 
expectations.
 
We
 
do
 
see
 
the
 
visibility
 
that,
 
that
 
will
 
be
 
a
 
growth
 
oppor tunity
 
in
 
Q4.
 
On
 
top
 
of
 
that,
 
we
 
will
 
have
 
our
 
Blackwell
 
architectur e.
 
Oper ator
 
And
 
your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
C.J.
 
Muse
 
with
 
Cantor
 
Fitzger ald.
 
Christ opher
 
Muse
 
You've
 
embark ed
 
on
 
a
 
remarkable
 
annual
 
product
 
cadence
 
with
 
challenges
 
only
 
likely
 
becoming
 
more
 
and
 
more,
 
given
 
rising
 
comple xity
 
in
 
a
 
rather
 
limit
 
advanced
 
package
 
world.
 
So
 
curious,
 
if
 
you
 
take
 
a
 
step
 
back,
 
how
 
does
 
this
 
backdr op
 
alter
 
your
 
thinking
 
around
 
potentially
 
greater
 
vertical
 
integration,
 
supply
 
chain
 
partnerships,
 
and
 
then
 
taking
 
through
 
a
 
consequential
 
impact
 
to
 
your
 
margin
 
proﬁle?
 
Jensen
 
Huang
 
Yes,
 
thanks.
 
Let's
 
see.
 
I
 
think
 
the
 
ﬁrst
 
answer
 
to
 
your
 
--
 
the
 
answer
 
to
 
your
 
ﬁrst
 
question
 
is
 
that
 
the
 
reason
 
why
 
our
 
velocity
 
is
 
so
 
high
 
is
 
simultaneously
 
because
 
the
 
comple xity
 
of
 
the
 
model
 
is
 
growing
 
and
 
we
 
want
 
to
 
continue
 
to
 
drive
 
its
 
cost
 
down.
 
It's
 
growing
 
so
 
we
 
want
 
to
 
continue
 
to
 
increase
 
its
 
scale.
 
And
 
we
 
believe
 
that
 
by
 
continuing
 
to
 
scale
 
the
 
AI
 
models,
 
that
 
we'll
 
reach
 
a
 
level
 
of
 
extraordinar y
 
usefulness
 
and
 
that
 
it
 
would
 
open
 
up,
 
realize
 
the
 
next
 
industrial
 
revolution.
 
We
 
believe
 
it.
 
And
 
so
 
we're
 
going
 
to
 
drive
 
ourselves
 
really
 
hard
 
to
 
continue
 
to
 
go
 
up
 
that
 
scale.
 
We
 
have
 
the
 
ability,
 
fairly
 
uniquely,
 
to
 
integrate,
 
to
 
design
 
an
 
AI
 
factory
 
because
 
we
 
have
 
all
 
the
 
parts.
 
It's
 
not
 
possible
 
to
 
come
 
up
 
with
 
a
 
new
 
AI
 
factory
 
every
 
year
 
unless
 
you
 
have
 
all
 
the
 
parts.
 
And
 
so
 
we
 
have
 
--
 
next
 
year,
 
we're
 
going
 
to
 
ship
 
a
 
lot
 
more
 
CPUs
 
than
 
we've
 
ever
 
had
 
in
 
the
 
history
 
of
 
our
 
company,
 
more
 
GPUs,
 
of
 
course,
 
but
 
also
 
NVLink
 
switches,
 
CX
 
DPUs,
 
ConnectX
 
for
 
East
 
and
 
West,
 
BlueField
 
DPUs
 
for
 
North
 
and
 
South,
 
and
 
data
 
and
 
storage
 
processing
 
to
 
InﬁniBand
 
for
 
super computing
 
centers,
 
to
 
Ethernet,
 
which
 
is
 
a
 
brand-new
 
product
 
for
 
us,
 
which
 
is
 
well
 
on
 
its
 
way
 
to
 
becoming
 
a
 
multibillion-dollar
 
business
 
to
 
bring
 
AI
 
to
 
Ethernet.
 
And
 
so
 
the
 
fact
 
that
 
we
 
could
 
build
 
--
 
we
 
have
 
access
 
to
 
all
 
of
 
this,
 
we
 
have
 
1
 
architectur al
 
stack,
 
as
 
you
 
know ,
 
it
 
allows
 
us
 
to
 
introduce
 
new
 
capabilities
 
to
 
the
 
mark et
 
as
 
we
 
complet e
 
it.
 
Other wise,
 
what
 
happens,
 
you
 
ship
 
these
 
parts,
 
you
 
go
 
ﬁnd
 
customers
 
to
 
sell
 
it
 
to,
 
and
 
then
 
you've
 
got
 
to
 
build
 
--
 
somebody's
 
got
 
to
 
build
 
up
 
an
 
AI
 
factory,
 
and
 
the
 
AI
 
factory
 
has
 
got
 
a
 
mountain
 
of
 
softwar e.
 
And
 
so
 
it's
 
not
 
about
 
who
 
integrates
 
it.
 
We
 
love
 
the
 
fact
 
that
 
our
 
supply
 
chain
 
is
 
disint egrated
 
in
 
the
 
sense
 
that
 
we
 
could
 
service
 
Quanta,
 
Foxconn,
 
HP,
 
Dell,
 
Lenovo,
 
Super
 
Micro.
 
We
 
used
 
to
 
be
 
able
 
to
 
serve
 
ZTE.
 
They
 
were
 
recently
 
purchased
 
and
 
so
 
on
 
and
 
so
 
forth.
 
And
 
so
 
the
 
number
 
of
 
ecosyst em
 
partners
 
that
 
we
 
have,
 
Gigabyt e,
 
the
 
number
 
of
 
ecosyst em
 
partners
 
that
 
we
 
have
 
that
 
allows
 
them
 
to
 
take
 
our
 
architectur e,
 
which
 
all
 
works,
 
but
 
integrated
 
in
 
a
 
bespok e
 
way
 
into
 
all
 
of
 
the
 
world's
 
cloud
 
service
 
providers,
 
enterprise
 
data
 
centers,
 
the
 
scale
 
and
 
reach
 
necessar y
 
from
 
our
 
ODMs
 
and
 
our
 
integrators,
 
integrated
 
supply
 
chain,
 
is
 
vast
 
and
 
gigantic
 
because
 
the
 
world
 
is
 
huge.
 
And
 
so
 
that
 
part,
 
we
 
don't
 
want
 
to
 
do
 
and
 
we're
 
not
 
good
 
at
 
doing.
 
And
 
--
 
but
 
we
 
know
 
how
 
to
 
design
 
the
 
AI
 
infrastructur e,
 
provided
 
the
 
way
 
that
 
customers
 
would
 
like
 
it
 
and
 
lets
 
the
 
ecosyst em
 
integrate
 
it.
 
Well,
 
yes.
 
So
 
anyways,
 
that's
 
the
 
reason
 
why.
 
Oper ator
 
And
 
your
 
ﬁnal
 
question
 
comes
 
from
 
the
 
line
 
of
 
Aaron
 
Rakers
 
with
 
Wells
 
Fargo.
 
Aaron
 
Rakers
 
I
 
want ed
 
to
 
go
 
back
 
into
 
the
 
Blackwell
 
product
 
cycle.
 
One
 
of
 
the
 
questions
 
that
 
we
 
tend
 
to
 
get
 
asked
 
is
 
how
 
you
 
see
 
the
 
rack
 
scale
 
system
 
mix
 
dynamic
 
as
 
you
 
think
 
about
 
lever aging
 
NVLink,
 
you
 
think
 
about
 
GB
 
NVL72
 
and
 
how
 
that
 
go-to-mark et
 
dynamic
 
looks
 
as
 
far
 
as
 
the
 
Blackwell
 
product
 
cycle.
 
I
 
guess
 
to
 
put
 
it
 
simply,
 
how
 
do
 
you
 
see
 
that
 
mix
 
of
 
rack
 
scale
 
systems
 
as
 
we
 
start
 
to
 
think
 
about
 
the
 
Blackwell
 
cycle
 
playing
 
out?
 
Jensen
 
Huang
 
Yes,
 
Aaron,
 
thanks.
 
The
 
Blackwell
 
rack
 
system,
 
it's
 
designed
 
and
 
architected
 
as
 
a
 
rack
 
but
 
it's
 
sold
 
in
 
disaggr egated
 
system
 
components.
 
We
 
don't
 
sell
 
the
 
whole
 
rack.
 
And
 
the
 
reason
 
for
 
that
 
is
 
because
 
everybody's
 
rack's
 
a
 
little
 
diﬀer ent
 
surprisingly .
 
Some
 
of
 
them
 
are
 
OCP
 
standar ds,
 
some
 
of
 
them
 
are
 
not.
 
Some
 
of
 
them
 
are
 
enterprise.
 
And
 
the
 
power
 
limits
 
for
 
everybody
 
could
 
be
 
a
 
little
 
diﬀer ent.
 
Choice
 
of
 
CDUs,
 
the
 
choice
 
of
 
power
 
bus
 
bars,
 
the
 
conﬁgur ation
 
and
 
integration
 
into
 
people's
 
data
 
centers,
 
all
 
diﬀer ent.
 
And
 
so
 
the
 
way
 
we
 
designed
 
it,
 
we
 
architected
 
the
 
whole
 
rack.
 
The
 
softwar e
 
is
 
going
 
to
 
work
 
perfectly
 
across
 
the
 
whole
 
rack.
 
And
 
then
 
we
 
provide
 
the
 
system
 
components.
 
Like
 
for
 
example,
 
the
 
CPU
 
and
 
GPU
 
comput e
 
board
 
is
 
then
 
integrated
 
into
 
an
 
MGX.
 
It's
 
a
 
modular
 
system
 
architectur e.
 
MGX
 
is
 
complet ely
 
ingenious.
 
And
 
we
 
have
 
MGX
 
ODMs
 
and
 
integrators
 
and
 
OEMs
 
all
 
over
 
the
 
plant.
 
And
 
so
 
just
 
about
 
any
 
conﬁgur ation
 
you
 
would
 
like,
 
wher e
 
you
 
would
 
like
 
that
 
3,000-pound
 
rack
 
to
 
be
 
deliver ed,
 
it's
 
got
 
to
 
be
 
close
 
to.
 
It
 
has
 
to
 
be
 
integrated
 
and
 
assembled
 
close
 
to
 
the
 
data
 
center
 
because
 
it's
 
fairly
 
heav y.
 
And
 
so
 
everything
 
from
 
the
 
supply
 
chain
 
from
 
the
 
moment
 
that
 
we
 
ship
 
the
 
GPU ,
 
CPUs,
 
the
 
switches,
 
the
 
NICs,
 
from
 
that
 
point
 
forward,
 
the
 
integration
 
is
 
done
 
quite
 
close
 
to
 
the
 
location
 
of
 
the
 
CSPs
 
and
 
the
 
locations
 
of
 
the
 
data
 
centers.
 
And
 
so
 
you
 
can
 
imagine
 
how
 
many
 
data
 
centers
 
in
 
the
 
world
 
there
 
are
 
and
 
how
 
many
 
logistics
 
hubs
 
we've
 
scaled
 
out
 
to
 
with
 
our
 
ODM
 
partners.
 
And
 
so
 
I
 
think
 
because
 
we
 
show
 
it
 
as
 
1
 
rack
 
and
 
because
 
it's
 
always
 
render ed
 
that
 
way
 
and
 
shown
 
that
 
way,
 
we
 
might
 
have
 
left
 
the
 
impression
 
that
 
we're
 
doing
 
the
 
integration.
 
Our
 
customers
 
hate
 
that
 
we
 
do
 
integration.
 
The
 
supply
 
chain
 
hates
 
us
 
doing
 
integration.
 
They
 
want
 
to
 
do
 
the
 
integration.
 
That's
 
their
 
value-add.
 
There's
 
a
 
ﬁnal
 
design-in,
 
if
 
you
 
will.
 
It's
 
not
 
quite
 
as
 
simple
 
as
 
shimmy
 
into
 
a
 
data
 
center
 
but
 
the
 
design
 
ﬁt
 
in
 
is
 
really
 
complicat ed.
 
And
 
so
 
the
 
design
 
ﬁt-in,
 
the
 
installation,
 
the
 
bring-up,
 
the
 
repair
 
and
 
replace,
 
that
 
entire
 
cycle
 
is
 
done
 
all
 
over
 
the
 
world.
 
And
 
we
 
have
 
a
 
sprawling
 
network
 
of
 
ODM
 
and
 
OEM
 
partners
 
that
 
does
 
this
 
incredibly
 
well.
 
So
 
integration
 
is
 
not
 
the
 
reason
 
why
 
we're
 
doing
 
racks.
 
It's
 
the
 
anti-r eason
 
of
 
doing
 
it.
 
The
 
way
 
we
 
don't
 
want
 
to
 
be
 
an
 
integrator,
 
we
 
want
 
to
 
be
 
a
 
technology
 
provider .
 
Oper ator
 
And
 
I
 
will
 
now
 
turn
 
the
 
call
 
back
 
over
 
to
 
Jensen
 
Huang
 
for
 
closing
 
remarks.
 
Jensen
 
Huang
 
Thank
 
you.
 
Let
 
me
 
make
 
a
 
couple
 
more
 
--
 
make
 
a
 
couple
 
of
 
comments
 
that
 
I
 
made
 
earlier
 
again.
 
The
 
data
 
center
 
worldwide
 
are
 
in
 
full
 
steam
 
to
 
moderniz e
 
the
 
entire
 
computing
 
stack
 
with
 
acceler ated
 
computing
 
and
 
gener ative
 
AI.
 
Hopper
 
demand
 
remains
 
strong
 
and
 
the
 
anticipation
 
for
 
Blackwell
 
is
 
incredible.
 
Let
 
me
 
highlight
 
the
 
top
 
5
 
things,
 
the
 
top
 
5
 
things
 
of
 
our
 
company .
 
Acceler ated
 
computing
 
has
 
reached
 
the
 
tipping
 
point.
 
CPU
 
scaling
 
slows.
 
Developers
 
must
 
acceler ate
 
everything
 
possible.
 
Acceler ated
 
computing
 
starts
 
with
 
CUD A-X
 
libraries.
 
New
 
libraries
 
open
 
new
 
mark ets
 
for
 
NVIDIA.
 
We
 
released
 
many
 
new
 
libraries,
 
including
 
CUD A-X
 
Acceler ated
 
Polars,
 
Pandas
 
and
 
Spark,
 
the
 
leading
 
data
 
science
 
and
 
data
 
processing
 
libraries,
 
CUVI-S
 
for
 
vector
 
databases.
 
This
 
is
 
incredibly
 
hot
 
right
 
now.
 
Ariel
 
and
 
for
 
5G
 
wireless
 
base
 
station,
 
a
 
whole
 
suite
 
of
 
a
 
whole
 
world
 
of
 
data
 
centers
 
that
 
we
 
can
 
go
 
into
 
now.
 
Parabricks
 
for
 
gene
 
sequencing
 
and
 
AlphaF old2
 
for
 
protein
 
structur e
 
prediction
 
is
 
now
 
CUD A
 
acceler ated.
 
We
 
are
 
at
 
the
 
beginning
 
of
 
our
 
journey
 
to
 
moderniz e
 
$1
 
trillion
 
worth
 
of
 
data
 
centers
 
from
 
gener al
 
purpose
 
computing
 
to
 
acceler ated
 
computing.
 
That's
 
number
 
one.
 
Number
 
two,
 
Blackwall
 
is
 
a
 
step-function
 
leap
 
over
 
Hopper .
 
Blackwell
 
is
 
an
 
AI
 
infrastructur e
 
platform,
 
not
 
just
 
the
 
GPU .
 
Also
 
happens
 
to
 
be
 
the
 
name
 
of
 
our
 
GPU
 
but
 
it's
 
an
 
AI
 
infrastructur e
 
platform.
 
As
 
we
 
reveal
 
more
 
of
 
Blackwell
 
and
 
sample
 
systems
 
to
 
our
 
partners
 
and
 
customers,
 
the
 
extent
 
of
 
Blackwell's
 
lead
 
becomes
 
clear .
 
The
 
Blackwell
 
vision
 
took
 
nearly
 
5
 
years
 
and
 
7
 
one-of-a-kind
 
chips
 
to
 
realize,
 
the
 
Gray
 
CPU,
 
the
 
Blackwell
 
dual
 
GPU
 
and
 
a
 
colos
 
package,
 
ConnectX
 
DPU
 
for
 
East-W est
 
traﬃc,
 
BlueField
 
DPU
 
for
 
North-South
 
and
 
storage
 
traﬃc,
 
NVLink
 
switch
 
for
 
all-to-all
 
GPU
 
communications,
 
and
 
Quantum
 
and
 
Spectrum-X
 
for
 
both
 
InﬁniBand
 
and
 
Ethernet
 
can
 
suppor t
 
the
 
massive
 
traﬃc
 
of
 
AI.
 
Blackwell
 
AI
 
factories
 
are
 
building
 
size
 
comput ers.
 
NVIDIA
 
designed
 
and
 
optimiz ed,
 
the
 
Blackwell
 
platform
 
full
 
stack
 
end-t o-end
 
from
 
chips,
 
systems,
 
networking,
 
even
 
structur ed
 
cables,
 
power
 
and
 
cooling
 
and
 
mounds
 
of
 
softwar e
 
to
 
make
 
it
 
fast
 
for
 
customers
 
to
 
build
 
AI
 
factories.
 
These
 
are
 
very
 
capital-int ensive
 
infrastructur es.
 
Customers
 
want
 
to
 
deploy
 
it
 
as
 
soon
 
as
 
they
 
get
 
their
 
hands
 
on
 
the
 
equipment
 
and
 
deliver
 
the
 
best
 
performance
 
and
 
TCO.
 
Blackwell
 
provides
 
3
 
to
 
5x
 
more
 
AI
 
throughput
 
in
 
a
 
power -limit ed
 
data
 
center
 
than
 
Hopper .
 
The
 
third
 
is
 
NVLink.
 
This
 
is
 
a
 
very
 
big
 
deal
 
with
 
its
 
all-to-all
 
GPU
 
switch
 
is
 
game
 
changing.
 
The
 
Blackwell
 
system
 
lets
 
us
 
connect
 
144
 
GPUs
 
in
 
72
 
GB200
 
packages
 
into
 
1
 
NVLink
 
domain,
 
with
 
an
 
aggregate
 
NVLink
 
bandwidth
 
of
 
259
 
terabytes
 
per
 
second
 
in
 
1
 
rack.
 
Just
 
to
 
put
 
that
 
in
 
perspective,
 
that's
 
about
 
10x
 
higher
 
than
 
Hopper .
 
259
 
terabytes
 
per
 
second
 
kind
 
of
 
makes
 
sense
 
because
 
you
 
need
 
to
 
boost
 
the
 
training
 
of
 
multitrillion-par amet er
 
models
 
on
 
trillions
 
of
 
tokens.
 
And
 
so
 
that
 
natur al
 
amount
 
of
 
data
 
needs
 
to
 
be
 
moved
 
around
 
from
 
GPU
 
to
 
GPU .
 
For
 
inference,
 
NVLink
 
is
 
vital
 
for
 
low-lat ency,
 
high-thr oughput
 
large
 
language
 
model
 
token
 
gener ation.
 
We
 
now
 
have
 
3
 
networking
 
platforms,
 
NVLink
 
for
 
GPU
 
scale-up,
 
Quantum
 
InﬁniBand
 
for
 
super computing
 
and
 
dedicat ed
 
AI
 
factories,
 
and
 
Spectrum-X
 
for
 
AI
 
on
 
Ethernet.
 
NVIDIA's
 
networking
 
footprint
 
is
 
much
 
bigger
 
than
 
befor e.
 
Gener ative
 
AI
 
momentum
 
is
 
acceler ating.
 
Gener ative
 
AI
 
frontier
 
model
 
makers
 
are
 
racing
 
to
 
scale
 
to
 
the
 
next
 
AI
 
plateau
 
to
 
increase
 
model
 
safety
 
and
 
IQ.
 
We're
 
also
 
scaling
 
to
 
understand
 
more
 
modalities
 
from
 
text,
 
images,
 
and
 
video
 
to
 
3D
 
physics,
 
chemistr y,
 
and
 
biology .
 
Chatbots,
 
coding
 
AIs,
 
and
 
image
 
gener ators
 
are
 
growing
 
fast
 
but
 
it's
 
just
 
the
 
tip
 
of
 
the
 
iceber g.
 
Internet
 
services
 
are
 
deploying
 
gener ative
 
AI
 
for
 
large-scale
 
recommenders,
 
ad
 
targeting
 
and
 
search
 
systems.
 
AI
 
start-ups
 
are
 
consuming
 
tens
 
of
 
billions
 
of
 
dollars
 
yearly
 
of
 
CSP's
 
cloud
 
capacity,
 
and
 
countries
 
are
 
recognizing
 
the
 
impor tance
 
of
 
AI
 
and
 
investing
 
in
 
sover eign
 
AI
 
infrastructur e.
 
And
 
NVIDIA
 
AI,
 
NVIDIA
 
Omniverse
 
is
 
opening
 
up
 
the
 
next
 
era
 
of
 
AI,
 
gener al
 
robotics.
 
And
 
now
 
the
 
enterprise
 
AI
 
wave
 
has
 
started
 
and
 
we're
 
poised
 
to
 
help
 
companies
 
transform
 
their
 
businesses.
 
The
 
NVIDIA
 
AI
 
Enterprise
 
platform
 
consists
 
of
 
Nemo,
 
NIMs,
 
NIM
 
Agent
 
Blueprints
 
and
 
AI
 
Foundr y.
 
That
 
our
 
ecosyst em
 
partners
 
the
 
world-leading
 
IT
 
companies
 
used
 
to
 
help
 
companies
 
customiz e
 
AI
 
models
 
and
 
build
 
bespok e
 
AI
 
applications.
 
Enterprises
 
can
 
then
 
deploy
 
on
 
NVIDIA
 
AI
 
Enterprise
 
run
 
time,
 
and
 
at
 
$4,500
 
per
 
GPU
 
per
 
year,
 
NVIDIA
 
AI
 
Enterprise
 
is
 
an
 
exceptional
 
value
 
for
 
deploying
 
AI
 
anywher e.
 
And
 
for
 
NVIDIA's
 
softwar e
 
TAM
 
can
 
be
 
signiﬁcant
 
as
 
the
 
CUD A-compatible
 
GPU
 
installed
 
base
 
grows
 
from
 
millions
 
to
 
tens
 
of
 
millions.
 
And
 
as
 
Colett e
 
mentioned,
 
NVIDIA
 
softwar e
 
will
 
exit
 
the
 
year
 
at
 
a
 
$2
 
billion
 
run
 
rate.
 
Thank
 
you
 
all
 
for
 
joining
 
us
 
today .
 
Oper ator
 
And
 
ladies
 
and
 
gentlemen,
 
this
 
concludes
 
today's
 
call
 
and
 
we
 
thank
 
you
 
for
 
your
 
participation.
 
You
 
may
 
now
 
disconnect.
 
 
