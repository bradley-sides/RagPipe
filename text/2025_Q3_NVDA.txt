NVDA
 
2025
 
Q3
 
Earnings
 
Call
 
Transcript
 
20
 
Nov
 
2024
 
Participants
 
Stewar t
 
Stecker
 
executive
 
Colett e
 
Kress
 
executive
 
Christ opher
 
Muse
 
analyst
 
Jensen
 
Huang
 
executive
 
Toshiya
 
Hari
 
analyst
 
Timothy
 
Arcuri
 
analyst
 
Vivek
 
Arya
 
analyst
 
Stacy
 
Rasgon
 
analyst
 
Joseph
 
Moor e
 
analyst
 
Aaron
 
Rakers
 
analyst
 
Atif
 
Malik
 
analyst
 
Benjamin
 
Reitz es
 
analyst
 
Pierre
 
Ferragu
 
analyst
 
Call
 
transcript
 
Oper ator
 
Good
 
afternoon.
 
My
 
name
 
is
 
Jay,
 
and
 
I'll
 
be
 
your
 
confer ence
 
operator
 
today .
 
At
 
this
 
time,
 
I
 
would
 
like
 
to
 
welcome
 
everyone
 
to
 
NVIDIA's
 
third
 
quarter
 
earnings
 
call.
 
[Oper ator
 
Instructions]
 
Thank
 
you.
 
Stewar t
 
Stecker,
 
you
 
may
 
begin
 
your
 
confer ence.
 
Stewar t
 
Stecker
 
Thank
 
you.
 
Good
 
afternoon,
 
everyone,
 
and
 
welcome
 
to
 
NVIDIA's
 
confer ence
 
call
 
for
 
the
 
third
 
quarter
 
of
 
ﬁscal
 
2025.
 
With
 
me
 
today
 
from
 
NVIDIA
 
are
 
Jensen
 
Huang,
 
President
 
and
 
Chief
 
Executive
 
Oﬃcer;
 
and
 
Colett e
 
Kress,
 
Executive
 
Vice
 
President
 
and
 
Chief
 
Financial
 
Oﬃcer .
 
I'd
 
like
 
to
 
remind
 
you
 
that
 
our
 
call
 
is
 
being
 
webcast
 
live
 
on
 
NVIDIA's
 
Invest or
 
Relations
 
websit e.
 
The
 
webcast
 
will
 
be
 
available
 
for
 
replay
 
until
 
the
 
confer ence
 
call
 
to
 
discuss
 
our
 
ﬁnancial
 
results
 
for
 
the
 
fourth
 
quarter
 
of
 
ﬁscal
 
2025.
 
The
 
content
 
of
 
today's
 
call
 
is
 
NVIDIA's
 
property.
 
It
 
can't
 
be
 
reproduced
 
or
 
transcribed
 
without
 
our
 
prior
 
written
 
consent.
 
During
 
this
 
call,
 
we
 
may
 
make
 
forward-looking
 
statements
 
based
 
on
 
current
 
expectations.
 
These
 
are
 
subject
 
to
 
a
 
number
 
of
 
signiﬁcant
 
risks
 
and
 
uncer tainties,
 
and
 
our
 
actual
 
results
 
may
 
diﬀer
 
materially .
 
For
 
a
 
discussion
 
of
 
factors
 
that
 
could
 
aﬀect
 
our
 
future
 
ﬁnancial
 
results
 
and
 
business,
 
please
 
refer
 
to
 
the
 
disclosur e
 
in
 
today's
 
earnings
 
release,
 
our
 
most
 
recent
 
Forms
 
10-K
 
and
 
10-Q,
 
and
 
the
 
reports
 
that
 
we
 
may
 
ﬁle
 
on
 
Form
 
8-K
 
with
 
the
 
Securities
 
and
 
Exchange
 
Commission.
 
All
 
our
 
statements
 
are
 
made
 
as
 
of
 
today,
 
November
 
20,
 
2024,
 
based
 
on
 
information
 
currently
 
available
 
to
 
us.
 
Except
 
as
 
requir ed
 
by
 
law,
 
we
 
assume
 
no
 
obligation
 
to
 
updat e
 
any
 
such
 
statements.
 
During
 
this
 
call,
 
we
 
will
 
discuss
 
non-GAAP
 
ﬁnancial
 
measur es.
 
You
 
can
 
ﬁnd
 
a
 
reconciliation
 
of
 
these
 
non-GAAP
 
ﬁnancial
 
measur es
 
to
 
GAAP
 
ﬁnancial
 
measur es
 
in
 
our
 
CFO
 
commentar y,
 
which
 
is
 
posted
 
on
 
our
 
websit e.
 
With
 
that,
 
let
 
me
 
turn
 
the
 
call
 
over
 
to
 
Colett e.
 
Colett e
 
Kress
 
Thank
 
you,
 
Stewar t.
 
Q3
 
was
 
another
 
record
 
quarter.
 
We
 
continue
 
to
 
deliver
 
incredible
 
growth.
 
Revenue
 
of
 
$35.1
 
billion
 
was
 
up
 
17%
 
sequentially
 
and
 
up
 
94%
 
year-on-year
 
and
 
well
 
above
 
our
 
outlook
 
of
 
$32.5
 
billion.
 
All
 
mark et
 
platforms
 
posted
 
strong
 
sequential
 
and
 
year-over -year
 
growth,
 
fueled
 
by
 
the
 
adoption
 
of
 
NVIDIA
 
acceler ated
 
computing
 
and
 
AI.
 
Starting
 
with
 
Data
 
Cent er.
 
Another
 
record
 
was
 
achieved
 
in
 
Data
 
Cent er.
 
Revenue
 
of
 
$30.8
 
billion,
 
up
 
17%
 
sequential
 
and
 
up
 
112%
 
year-on-year .
 
NVIDIA
 
Hopper
 
demand
 
is
 
exceptional,
 
and
 
sequentially,
 
NVIDIA
 
H200
 
sales
 
increased
 
signiﬁcantly
 
to
 
double-digit
 
billions,
 
the
 
fastest
 
prod
 
ramp
 
in
 
our
 
company's
 
history.
 
The
 
H200
 
delivers
 
up
 
to
 
2x
 
faster
 
inference
 
performance
 
and
 
up
 
to
 
50%
 
improved
 
TCO.
 
Cloud
 
service
 
providers
 
were
 
approximat ely
 
half
 
of
 
our
 
Data
 
Cent er
 
sales
 
with
 
revenue
 
increasing
 
more
 
than
 
2x
 
year-on-year .
 
CSPs
 
deployed
 
NVIDIA
 
H200
 
infrastructur e
 
and
 
high-speed
 
networking
 
with
 
installations
 
scaling
 
to
 
tens
 
of
 
thousands
 
of
 
DPUs
 
to
 
grow
 
their
 
business
 
and
 
serve
 
rapidly
 
rising
 
demand
 
for
 
AI
 
training
 
and
 
inference
 
workloads.
 
NVIDIA
 
H200-power ed
 
cloud
 
instances
 
are
 
now
 
available
 
from
 
AWS,
 
CoreWeave,
 
and
 
Microsoft
 
Azure
 
with
 
Google
 
Cloud
 
and
 
OCI
 
coming
 
soon.
 
Alongside
 
signiﬁcant
 
growth
 
from
 
our
 
large
 
CSPs,
 
NVIDIA
 
GPU
 
regional
 
cloud
 
revenue
 
jumped
 
year-on-year
 
as
 
North
 
America,
 
India,
 
and
 
Asia
 
Paciﬁc
 
regions
 
ramped
 
NVIDIA
 
Cloud
 
instances
 
and
 
sover eign
 
cloud
 
build-outs.
 
Consumer
 
Internet
 
revenue
 
more
 
than
 
doubled
 
year-on-year
 
as
 
companies
 
scaled
 
their
 
NVIDIA
 
Hopper
 
infrastructur e
 
to
 
suppor t
 
next-gener ation
 
AI
 
models
 
training,
 
multimodal,
 
and
 
agentic
 
AI,
 
deep
 
learning
 
recommender
 
engines,
 
and
 
gener ative
 
AI
 
inference
 
and
 
content
 
creation
 
workloads.
 
NVIDIA
 
Amper e
 
and
 
Hopper
 
infrastructur es
 
are
 
fueling
 
inference
 
revenue
 
growth
 
for
 
customers.
 
NVIDIA
 
is
 
the
 
largest
 
inference
 
platform
 
in
 
the
 
world.
 
Our
 
large
 
installed
 
base
 
and
 
rich
 
softwar e
 
ecosyst em
 
encour age
 
developers
 
to
 
optimiz e
 
for
 
NVIDIA
 
and
 
deliver
 
continued
 
performance
 
and
 
TCO
 
improvements.
 
Rapid
 
advancements
 
in
 
NVIDIA
 
softwar e
 
algorithms
 
boost ed
 
Hopper
 
inference
 
throughput
 
by
 
an
 
incredible
 
5x
 
in
 
1
 
year
 
and
 
cut
 
time
 
to
 
ﬁrst
 
token
 
by
 
5x.
 
Our
 
upcoming
 
release
 
of
 
NVIDIA
 
NIM
 
will
 
boost
 
Hopper
 
inference
 
performance
 
by
 
an
 
additional
 
2.4x.
 
Continuous
 
performance
 
optimizations
 
are
 
a
 
hallmark
 
of
 
NVIDIA
 
and
 
drive
 
increasingly
 
economic
 
returns
 
for
 
the
 
entire
 
NVIDIA
 
installed
 
base.
 
Blackwell
 
is
 
in
 
full
 
production
 
after
 
a
 
successfully
 
executed
 
change.
 
We
 
shipped
 
13,000
 
GPU
 
samples
 
to
 
customers
 
in
 
the
 
third
 
quarter,
 
including
 
one
 
of
 
the
 
ﬁrst
 
Blackwell
 
DGX
 
engineering
 
samples
 
to
 
OpenAI.
 
Blackwell
 
is
 
a
 
full
 
stack,
 
full
 
infrastructur e,
 
AI
 
data
 
center
 
scale
 
system
 
with
 
customizable
 
conﬁgur ations
 
needed
 
to
 
address
 
a
 
diverse
 
and
 
growing
 
AI
 
mark et
 
from
 
x86
 
to
 
ARM,
 
training
 
to
 
inferencing
 
GPUs,
 
InﬁniBand
 
to
 
Ethernet
 
switches,
 
and
 
NVLink
 
and
 
from
 
liquid
 
cooled
 
to
 
air
 
cooled,
 
every
 
customer
 
is
 
racing
 
to
 
be
 
the
 
ﬁrst
 
to
 
mark et.
 
Blackwell
 
is
 
now
 
in
 
the
 
hands
 
of
 
all
 
of
 
our
 
major
 
partners,
 
and
 
they
 
are
 
working
 
to
 
bring
 
up
 
their
 
data
 
centers.
 
We
 
are
 
integrating
 
Blackwell
 
systems
 
into
 
the
 
diverse
 
data
 
center
 
conﬁgur ations
 
of
 
our
 
customers.
 
Blackwell
 
demand
 
is
 
staggering,
 
and
 
we
 
are
 
racing
 
to
 
scale
 
supply
 
to
 
meet
 
the
 
incredible
 
demand
 
customers
 
are
 
placing
 
on
 
us.
 
Customers
 
are
 
gearing
 
up
 
to
 
deploy
 
Blackwell
 
at
 
scale.
 
Oracle
 
announced
 
the
 
world's
 
ﬁrst
 
Zettascale
 
AI
 
cloud
 
computing
 
clust ers
 
that
 
can
 
scale
 
to
 
over
 
131,000
 
Blackwell
 
GPUs
 
to
 
help
 
enterprises
 
train
 
and
 
deploy
 
some
 
of
 
the
 
most
 
demanding
 
next-gener ation
 
AI
 
models.
 
Yesterday,
 
Microsoft
 
announced
 
they
 
will
 
be
 
the
 
ﬁrst
 
CSP
 
to
 
oﬀer,
 
in
 
privat e
 
preview ,
 
Blackwell-based
 
cloud
 
instances
 
power ed
 
by
 
NVIDIA
 
GB200
 
and
 
Quantum
 
InﬁniBand.
 
Last
 
week,
 
Blackwell
 
made
 
its
 
debut
 
on
 
the
 
most
 
recent
 
round
 
of
 
MLPerf
 
training
 
results,
 
sweeping
 
the
 
per
 
GPU
 
benchmarks
 
and
 
delivering
 
a
 
2.2x
 
leap
 
in
 
performance
 
over
 
Hopper .
 
The
 
results
 
also
 
demonstr ate
 
our
 
relentless
 
pursuit
 
to
 
drive
 
down
 
the
 
cost
 
of
 
comput e.
 
The
 
64
 
Blackwell
 
GPUs
 
are
 
requir ed
 
to
 
run
 
the
 
GPT-3
 
benchmark
 
compar ed
 
to
 
256
 
H100s
 
or
 
a
 
4x
 
reduction
 
in
 
cost.
 
NVIDIA
 
Blackwell
 
architectur e
 
with
 
NVLink
 
Switch
 
enables
 
up
 
to
 
30x
 
faster
 
inference
 
performance
 
and
 
a
 
new
 
level
 
of
 
inference
 
scaling,
 
throughput
 
and
 
response
 
time
 
that
 
is
 
excellent
 
for
 
running
 
new
 
reasoning
 
inference
 
applications
 
like
 
OpenAI's
 
o1
 
model.
 
With
 
every
 
new
 
platform
 
shift,
 
a
 
wave
 
of
 
start-ups
 
is
 
created.
 
Hundr eds
 
of
 
AI-native
 
companies
 
are
 
already
 
delivering
 
AI
 
services
 
with
 
great
 
success.
 
Through
 
Google,
 
Meta,
 
Microsoft,
 
and
 
OpenAI
 
are
 
the
 
headliners.
 
Anthr opic,
 
Perple xity,
 
Mistr al,
 
Adobe
 
Fireﬂy,
 
Runway,
 
Midjourney,
 
Light
 
Tricks,
 
Harvey,
 
Podium,
 
Purser ,
 
and
 
the
 
Bridge
 
are
 
seeing
 
great
 
success
 
while
 
thousands
 
of
 
AI-native
 
start-ups
 
are
 
building
 
new
 
services.
 
The
 
next
 
wave
 
of
 
AI
 
are
 
Enterprise
 
AI
 
and
 
industrial
 
AI.
 
Enterprise
 
AI
 
is
 
in
 
full
 
throttle.
 
NVIDIA
 
AI
 
Enterprise,
 
which
 
includes
 
NVIDIA
 
NeMo
 
and
 
NIMs
 
micro
 
services
 
is
 
an
 
operating
 
platform
 
of
 
agentic
 
AI.
 
Industr y
 
leaders
 
are
 
using
 
NVIDIA
 
AI
 
to
 
build
 
Copilots
 
and
 
agents.
 
Working
 
with
 
NVIDIA,
 
Cadence,
 
Clouder a,
 
Cohesity,
 
NetApp,
 
Salesfor ce,
 
SAP,
 
and
 
ServiceNow
 
are
 
racing
 
to
 
acceler ate
 
development
 
of
 
these
 
applications
 
with
 
the
 
potential
 
for
 
billions
 
of
 
agents
 
to
 
be
 
deployed
 
in
 
the
 
coming
 
years.
 
Consulting
 
leaders
 
like
 
Accentur e
 
and
 
Deloitt e
 
are
 
taking
 
NVIDIA
 
AI
 
to
 
the
 
world's
 
enterprises.
 
Accentur e
 
launched
 
a
 
new
 
business
 
group
 
with
 
30,000
 
professionals
 
trained
 
on
 
NVIDIA
 
AI
 
technology
 
to
 
help
 
facilitat e
 
this
 
global
 
build-out.
 
Additionally,
 
Accentur e
 
with
 
over
 
707,000
 
employees,
 
is
 
lever aging
 
NVIDIA-power ed
 
agentic
 
AI
 
applications
 
internally,
 
including
 
1
 
case
 
that
 
cuts
 
manual
 
steps
 
in
 
mark eting
 
campaigns
 
for
 
25%
 
to
 
35%.
 
Nearly
 
1,000
 
companies
 
are
 
using
 
NVIDIA
 
NIM,
 
and
 
the
 
speed
 
of
 
its
 
uptak e
 
is
 
evident
 
in
 
NVIDIA
 
AI
 
Enterprise
 
monetization.
 
We
 
expect
 
NVIDIA
 
AI
 
Enterprise
 
full
 
year
 
revenue
 
to
 
increase
 
over
 
2x
 
from
 
last
 
year
 
and
 
our
 
pipeline
 
continues
 
to
 
build.
 
Over all,
 
our
 
softwar e,
 
service
 
and
 
suppor t
 
revenue
 
is
 
annualizing
 
at
 
$1.5
 
billion,
 
and
 
we
 
expect
 
to
 
exit
 
this
 
year
 
annualizing
 
at
 
over
 
$2
 
billion.
 
Industrial
 
AI
 
and
 
robotics
 
are
 
acceler ating.
 
This
 
is
 
trigger ed
 
by
 
breakthr oughs
 
in
 
physical
 
AI,
 
foundation
 
models
 
that
 
understand
 
the
 
physical
 
world,
 
like
 
NVIDIA
 
NeMo
 
for
 
enterprise
 
AI
 
agents.
 
We
 
built
 
NVIDIA
 
Omniverse
 
for
 
developers
 
to
 
build,
 
train,
 
and
 
operate
 
industrial
 
AI
 
and
 
robotics.
 
Some
 
of
 
the
 
largest
 
industrial
 
manufactur ers
 
in
 
the
 
world
 
are
 
adopting
 
NVIDIA
 
Omniverse
 
to
 
acceler ate
 
their
 
businesses,
 
automat e
 
their
 
workﬂows,
 
and
 
to
 
achieve
 
new
 
levels
 
of
 
operating
 
eﬃciency .
 
Foxconn,
 
the
 
world's
 
largest
 
electr onics
 
manufactur er,
 
is
 
using
 
digital
 
twin
 
and
 
industrial
 
AI
 
built
 
on
 
NVIDIA
 
Omniverse
 
to
 
speed
 
the
 
bring-up
 
of
 
its
 
Blackwell
 
factories
 
and
 
drive
 
new
 
levels
 
of
 
eﬃciency .
 
In
 
its
 
Mexico
 
facility
 
alone,
 
Foxconn
 
expects
 
to
 
reduce
 
--
 
a
 
reduction
 
of
 
over
 
30%
 
in
 
annual
 
hour
 
usage.
 
From
 
a
 
geogr aphic
 
perspective,
 
our
 
data
 
center
 
revenue
 
in
 
China
 
grew
 
sequentially
 
due
 
to
 
shipments
 
of
 
export-compliant
 
Hopper
 
products
 
to
 
industries.
 
As
 
a
 
percentage
 
of
 
total
 
data
 
center
 
revenue,
 
it
 
remained
 
well
 
below
 
levels
 
prior
 
to
 
the
 
onset
 
of
 
export
 
contr ols.
 
We
 
expect
 
the
 
mark et
 
in
 
China
 
to
 
remain
 
very
 
competitive
 
going
 
forward.
 
We
 
will
 
continue
 
to
 
comply
 
with
 
export
 
contr ols
 
while
 
serving
 
our
 
customers.
 
Our
 
AI
 
initiatives
 
continue
 
to
 
gather
 
momentum
 
as
 
countries
 
embr ace
 
NVIDIA
 
acceler ated
 
computing
 
for
 
a
 
new
 
industrial
 
revolution
 
power ed
 
by
 
AI.
 
India's
 
leading
 
CSPs
 
include
 
product
 
communications
 
and
 
data
 
services
 
are
 
building
 
AI
 
factories
 
for
 
tens
 
of
 
thousands
 
of
 
NVIDIA
 
GPUs.
 
By
 
year-end,
 
they
 
will
 
have
 
boost ed
 
NVIDIA
 
GPU
 
deployments
 
in
 
the
 
countr y
 
by
 
nearly
 
10x.
 
Infosys,
 
TFC,
 
Wipro
 
are
 
adopting
 
NVIDIA
 
AI
 
Enterprise
 
and
 
upskilling
 
nearly
 
0.5
 
million
 
developers
 
and
 
consultants
 
to
 
help
 
clients
 
build
 
and
 
run
 
AI
 
agents
 
on
 
our
 
platform.
 
In
 
Japan,
 
SoftBank
 
is
 
building
 
the
 
nation's
 
most
 
power ful
 
AI
 
super comput er
 
with
 
NVIDIA
 
DGX
 
Blackwell
 
and
 
Quantum
 
InﬁniBand.
 
SoftBank
 
is
 
also
 
partnering
 
with
 
NVIDIA
 
to
 
transform
 
the
 
telecommunications
 
network
 
into
 
a
 
distribut ed
 
AI
 
network
 
with
 
NVIDIA
 
AI
 
Aerial
 
and
 
ARN
 
platform
 
that
 
can
 
process
 
both
 
5G
 
RAN
 
on
 
AI
 
on
 
CUD A.
 
We
 
are
 
launching
 
the
 
same
 
in
 
the
 
U.S.
 
with
 
T-Mobile.
 
Leaders
 
across
 
Japan,
 
including
 
Fujitsu,
 
NEC,
 
and
 
NTT
 
are
 
adopting
 
NVIDIA
 
AI
 
Enterprise
 
and
 
major
 
consulting
 
companies,
 
including
 
EY
 
Strategy
 
and
 
Consulting
 
will
 
help
 
bring
 
NVIDIA
 
AI
 
technology
 
to
 
Japan's
 
industries.
 
Networking
 
revenue
 
increased
 
20%
 
year-on-year .
 
Areas
 
of
 
sequential
 
revenue
 
growth
 
include
 
InﬁniBand
 
and
 
Ethernet
 
switches,
 
Smar tNICs
 
and
 
BlueField
 
DPUs.
 
Though
 
networking
 
revenue
 
was
 
sequentially
 
down,
 
networking
 
demand
 
is
 
strong
 
and
 
growing,
 
and
 
we
 
anticipat e
 
sequential
 
growth
 
in
 
Q4.
 
CSPs
 
and
 
super computing
 
centers
 
are
 
using
 
and
 
adopting
 
the
 
NVIDIA
 
InﬁniBand
 
platform
 
to
 
power
 
new
 
H200
 
clust ers.
 
NVIDIA
 
Spectrum-X
 
Ethernet
 
for
 
AI
 
revenue
 
increased
 
over
 
3x
 
year-on-year .
 
And
 
our
 
pipeline
 
continues
 
to
 
build
 
with
 
multiple
 
CSPs
 
and
 
consumer
 
Internet
 
companies
 
planning
 
large
 
clust er
 
deployments.
 
Traditional
 
Ethernet
 
was
 
not
 
designed
 
for
 
AI.
 
NVIDIA
 
Spectrum-X
 
uniquely
 
lever ages
 
technology
 
previously
 
exclusive
 
to
 
InﬁniBand
 
to
 
enable
 
customers
 
to
 
achieve
 
massive
 
scale
 
of
 
their
 
comput e.
 
Utilizing
 
Spectrum-X,
 
xAI's
 
Colossus
 
100,000-Hopper
 
super comput er
 
experienced
 
0
 
application
 
latency
 
degradation
 
and
 
maintained
 
95%
 
of
 
data
 
throughput
 
versus
 
60%
 
for
 
traditional
 
Ethernet.
 
Now
 
moving
 
to
 
Gaming
 
and
 
AI
 
PCs.
 
Gaming
 
revenue
 
of
 
$3.3
 
billion
 
increased
 
14%
 
sequentially
 
and
 
15%
 
year-on-year .
 
Q3
 
was
 
a
 
great
 
quarter
 
for
 
Gaming
 
with
 
notebook,
 
console,
 
and
 
deskt op
 
revenue
 
all
 
growing
 
sequentially
 
and
 
year-on-year .
 
RTX
 
end
 
demand
 
was
 
fueled
 
by
 
strong
 
back-t o-school
 
sales
 
as
 
consumers
 
continue
 
to
 
choose
 
GeForce
 
RTX
 
GPUs
 
and
 
devices
 
to
 
power
 
gaming,
 
creative,
 
and
 
AI
 
applications.
 
Channel
 
invent ory
 
remains
 
healthy
 
and
 
we
 
are
 
gearing
 
up
 
for
 
the
 
holiday
 
season.
 
We
 
began
 
shipping
 
new
 
GeForce
 
RTX
 
AI
 
PC
 
with
 
up
 
to
 
321
 
AI
 
TOPS
 
from
 
ASUS
 
and
 
MSI
 
with
 
Microsoft's
 
Copilot+
 
capabilities
 
anticipat ed
 
in
 
Q4.
 
These
 
machines
 
harness
 
the
 
power
 
of
 
RTX
 
ray
 
tracing
 
and
 
AI
 
technologies
 
to
 
super charge
 
gaming,
 
photo,
 
and
 
video
 
editing,
 
image
 
gener ation
 
and
 
coding.
 
This
 
past
 
quarter,
 
we
 
celebr ated
 
the
 
25th
 
anniversar y
 
of
 
the
 
GeForce
 
256,
 
the
 
world's
 
ﬁrst
 
GPU .
 
The
 
transforming
 
executing
 
graphics
 
to
 
igniting
 
the
 
AI
 
revolution.
 
NVIDIA's
 
GPUs
 
have
 
been
 
the
 
driving
 
force
 
behind
 
some
 
of
 
the
 
most
 
consequential
 
technologies
 
of
 
our
 
time.
 
Moving
 
to
 
ProViz.
 
Revenue
 
of
 
$486
 
million
 
was
 
up
 
7%
 
sequentially
 
and
 
17%
 
year-on-year .
 
NVIDIA
 
RTX
 
workstations
 
continue
 
to
 
be
 
the
 
preferred
 
choice
 
to
 
power
 
professional
 
graphics,
 
design,
 
and
 
engineering-r elated
 
workloads.
 
Additionally,
 
AI
 
is
 
emer ging
 
as
 
a
 
power ful
 
demand
 
driver ,
 
including
 
autonomous
 
vehicle
 
simulation,
 
[indiscernible]
 
model
 
prototyping
 
for
 
productivity-r elated
 
use
 
cases
 
and
 
gener ative
 
AI
 
content
 
creation
 
in
 
media
 
and
 
entertainment.
 
Moving
 
to
 
Automotive.
 
Revenue
 
was
 
a
 
record
 
$449
 
million,
 
up
 
30%
 
sequentially
 
and
 
up
 
72%
 
year-on-year .
 
Strong
 
growth
 
was
 
driven
 
by
 
self-driving
 
brands
 
of
 
NVIDIA
 
Orin
 
and
 
robust
 
end
 
mark et
 
demand
 
for
 
NAVs.
 
Global
 
Cars
 
is
 
rolling
 
out
 
its
 
fully
 
electric
 
SUV
 
built
 
on
 
NVIDIA
 
Orin
 
and
 
DriveOS.
 
Okay,
 
moving
 
to
 
the
 
rest
 
of
 
the
 
P&L.
 
GAAP
 
gross
 
margin
 
was
 
74.6%
 
and
 
non-GAAP
 
gross
 
margin
 
was
 
75%,
 
down
 
sequentially,
 
primarily
 
driven
 
by
 
a
 
mix
 
shift
 
of
 
the
 
H100
 
systems
 
to
 
more
 
comple x
 
and
 
higher -cost
 
systems
 
within
 
Data
 
Cent er.
 
Sequentially,
 
GAAP
 
operating
 
expenses
 
and
 
non-GAAP
 
operating
 
expenses
 
were
 
up
 
9%
 
due
 
to
 
higher
 
comput e,
 
infrastructur e
 
and
 
engineering
 
development
 
costs
 
for
 
new
 
product
 
introductions.
 
In
 
Q3,
 
we
 
returned
 
$11.2
 
billion
 
to
 
shareholders
 
in
 
the
 
form
 
of
 
share
 
repurchases
 
and
 
cash
 
dividends.
 
Well,
 
let
 
me
 
turn
 
to
 
the
 
outlook
 
for
 
the
 
fourth
 
quarter.
 
Total
 
revenue
 
is
 
expect ed
 
to
 
be
 
$37.5
 
billion,
 
plus
 
or
 
minus
 
2%,
 
which
 
incorpor ates
 
continued
 
demand
 
for
 
Hopper
 
architectur e
 
and
 
the
 
initial
 
ramp
 
of
 
our
 
Blackwell
 
products.
 
While
 
demand
 
greatly
 
exceed
 
supply,
 
we
 
are
 
on
 
track
 
to
 
exceed
 
our
 
previous
 
Blackwell
 
revenue
 
estimat e
 
of
 
sever al
 
billion
 
dollars
 
as
 
our
 
visibility
 
into
 
supply
 
continues
 
to
 
increase.
 
On
 
Gaming,
 
although
 
sell-thr ough
 
was
 
strong
 
in
 
Q3,
 
we
 
expect
 
fourth
 
quarter
 
revenue
 
to
 
decline
 
sequentially
 
due
 
to
 
supply
 
constr aints.
 
GAAP
 
and
 
non-GAAP
 
gross
 
margins
 
are
 
expect ed
 
to
 
be
 
73%
 
and
 
73.5%,
 
respectively,
 
plus
 
or
 
minus
 
50
 
basis
 
points.
 
Blackwell
 
is
 
a
 
customizable
 
AI
 
infrastructur e
 
with
 
7
 
diﬀer ent
 
types
 
of
 
NVIDIA-built
 
chips,
 
multiple
 
networking
 
options
 
and
 
for
 
air
 
and
 
liquid
 
cooled
 
data
 
centers.
 
Our
 
current
 
focus
 
is
 
on
 
ramping
 
to
 
strong
 
demand,
 
increasing
 
system
 
availability,
 
and
 
providing
 
the
 
optimal
 
mix
 
of
 
conﬁgur ations
 
to
 
our
 
customer .
 
As
 
Blackwell
 
ramps,
 
we
 
expect
 
gross
 
margins
 
to
 
moder ate
 
to
 
the
 
low
 
70s.
 
When
 
fully
 
ramped,
 
we
 
expect
 
Blackwell
 
margins
 
to
 
be
 
in
 
the
 
mid-70s.
 
GAAP
 
and
 
non-GAAP
 
operating
 
expenses
 
are
 
expect ed
 
to
 
be
 
approximat ely
 
$4.8
 
billion
 
and
 
$3.4
 
billion,
 
respectively .
 
We
 
are
 
a
 
data
 
center-scale
 
AI
 
infrastructur e
 
company .
 
Our
 
investments
 
include
 
building
 
data
 
centers
 
for
 
development
 
of
 
our
 
hardwar e
 
and
 
softwar e
 
stacks
 
and
 
to
 
suppor t
 
new
 
introductions.
 
GAAP
 
and
 
non-GAAP
 
other
 
income
 
and
 
expenses
 
are
 
expect ed
 
to
 
be
 
an
 
income
 
of
 
approximat ely
 
$400
 
million,
 
excluding
 
gains
 
and
 
losses
 
from
 
nonaﬃliat ed
 
investments.
 
GAAP
 
and
 
non-GAAP
 
tax
 
rates
 
are
 
expect ed
 
to
 
be
 
16.5%,
 
plus
 
or
 
minus
 
1%,
 
excluding
 
any
 
discr ete
 
items.
 
Further
 
ﬁnancial
 
details
 
are
 
included
 
in
 
the
 
CFO
 
commentar y
 
and
 
other
 
information
 
available
 
on
 
our
 
IR
 
websit e.
 
In
 
closing,
 
let
 
me
 
highlight
 
upcoming
 
events
 
for
 
the
 
ﬁnancial
 
community .
 
We
 
will
 
be
 
attending
 
the
 
UBS
 
Global
 
Technology
 
and
 
AI
 
Confer ence
 
on
 
December
 
3
 
in
 
Scottsdale.
 
Please
 
join
 
us
 
at
 
CES
 
in
 
Las
 
Vegas,
 
wher e
 
Jensen
 
will
 
deliver
 
a
 
keynot e
 
on
 
Januar y
 
6.
 
And
 
we
 
will
 
host
 
a
 
Q&A
 
session
 
for
 
ﬁnancial
 
analysts
 
the
 
next
 
day
 
on
 
Januar y
 
7.
 
Our
 
earnings
 
call
 
to
 
discuss
 
results
 
for
 
the
 
fourth
 
quarter
 
of
 
ﬁscal
 
2025
 
is
 
scheduled
 
for
 
Februar y
 
26,
 
2025.
 
We
 
will
 
now
 
open
 
the
 
call
 
for
 
questions.
 
Oper ator,
 
can
 
you
 
poll
 
for
 
questions,
 
please?
 
Oper ator
 
[Oper ator
 
Instructions]
 
Your
 
ﬁrst
 
question
 
comes
 
from
 
the
 
line
 
of
 
C.J.
 
Muse
 
of
 
Cantor
 
Fitzger ald.
 
Christ opher
 
Muse
 
I
 
guess
 
just
 
a
 
question
 
for
 
you
 
on
 
the
 
debat e
 
around
 
whether
 
scaling
 
for
 
large
 
language
 
models
 
have
 
stalled.
 
Obviously,
 
we're
 
very
 
early
 
here,
 
but
 
would
 
love
 
to
 
hear
 
your
 
thoughts
 
on
 
this
 
front.
 
How
 
are
 
you
 
helping
 
your
 
customers
 
as
 
they
 
work
 
through
 
these
 
issues?
 
And
 
then
 
obviously,
 
part
 
of
 
the
 
context
 
here
 
is
 
we're
 
discussing
 
clust ers
 
that
 
have
 
yet
 
to
 
beneﬁt
 
from
 
Blackwell.
 
So
 
is
 
this
 
driving
 
even
 
greater
 
demand
 
for
 
Blackwell?
 
Jensen
 
Huang
 
Foundation
 
model
 
pretraining
 
scaling
 
is
 
intact
 
and
 
it's
 
continuing.
 
As
 
you
 
know ,
 
this
 
is
 
an
 
empirical
 
law,
 
not
 
a
 
fundamental
 
physical
 
law.
 
But
 
the
 
evidence
 
is
 
that
 
it
 
continues
 
to
 
scale.
 
What
 
we're
 
learning,
 
however ,
 
is
 
that
 
it's
 
not
 
enough,
 
that
 
we've
 
now
 
discover ed
 
2
 
other
 
ways
 
to
 
scale.
 
One
 
is
 
post-tr aining
 
scaling.
 
Of
 
course,
 
the
 
ﬁrst
 
gener ation
 
of
 
post-tr aining
 
was
 
reinfor cement
 
learning
 
human
 
feedback,
 
but
 
now
 
we
 
have
 
reinfor cement
 
learning
 
AI
 
feedback
 
and
 
all
 
forms
 
of
 
synthetic
 
data
 
gener ated
 
data
 
that
 
assists
 
in
 
post-tr aining
 
scaling.
 
And
 
one
 
of
 
the
 
biggest
 
events
 
and
 
one
 
of
 
the
 
most
 
exciting
 
developments
 
is
 
Strawberr y,
 
ChatGPT
 
o1,
 
OpenAI's
 
o1,
 
which
 
does
 
inference
 
time
 
scaling,
 
what
 
is
 
called
 
test
 
time
 
scaling.
 
The
 
longer
 
it
 
thinks,
 
the
 
better
 
and
 
higher -quality
 
answer
 
it
 
produces.
 
And
 
it
 
considers
 
approaches
 
like
 
chain
 
of
 
thought
 
and
 
multi-path
 
planning
 
and
 
all
 
kinds
 
of
 
techniques
 
necessar y
 
to
 
reﬂect
 
and
 
so
 
on
 
and
 
so
 
forth.
 
And
 
it's
 
--
 
intuitively,
 
it's
 
a
 
little
 
bit
 
like
 
us
 
doing
 
thinking
 
in
 
our
 
head
 
befor e
 
we
 
answer
 
your
 
question.
 
And
 
so
 
we
 
now
 
have
 
3
 
ways
 
of
 
scaling
 
and
 
we're
 
seeing
 
all
 
3
 
ways
 
of
 
scaling.
 
And
 
as
 
a
 
result
 
of
 
that,
 
the
 
demand
 
for
 
our
 
infrastructur e
 
is
 
really
 
great.
 
You
 
see
 
now
 
that
 
at
 
the
 
tail
 
end
 
of
 
the
 
last
 
gener ation
 
of
 
foundation
 
models
 
were
 
at
 
about
 
100,000
 
Hoppers.
 
The
 
next
 
gener ation
 
starts
 
at
 
100,000
 
Blackwells.
 
And
 
so
 
that
 
kind
 
of
 
gives
 
you
 
a
 
sense
 
of
 
wher e
 
the
 
industr y
 
is
 
moving
 
with
 
respect
 
to
 
pretraining
 
scaling,
 
post-tr aining
 
scaling,
 
and
 
then
 
now
 
very
 
impor tantly,
 
inference
 
time
 
scaling.
 
And
 
so
 
the
 
demand
 
is
 
really
 
great
 
for
 
all
 
of
 
those
 
reasons.
 
But
 
remember ,
 
simultaneously,
 
we're
 
seeing
 
inference
 
really
 
starting
 
to
 
scale
 
up
 
for
 
our
 
company .
 
We
 
are
 
the
 
largest
 
inference
 
platform
 
in
 
the
 
world
 
today
 
because
 
our
 
installed
 
base
 
is
 
so
 
large.
 
And
 
everything
 
that
 
was
 
trained
 
on
 
Amper es
 
and
 
Hoppers
 
inference
 
incredibly
 
on
 
Amper es
 
and
 
Hoppers.
 
And
 
as
 
we
 
move
 
to
 
Blackwells
 
for
 
training
 
foundation
 
models,
 
it
 
leads
 
behind
 
it
 
a
 
large
 
installed
 
base
 
of
 
extraordinar y
 
infrastructur e
 
for
 
inference.
 
And
 
so
 
we're
 
seeing
 
inference
 
demand
 
go
 
up.
 
We're
 
seeing
 
inference
 
time
 
scaling
 
go
 
up.
 
We
 
see
 
the
 
number
 
of
 
AI
 
native
 
companies
 
continue
 
to
 
grow.
 
And
 
of
 
course,
 
we're
 
starting
 
to
 
see
 
enterprise
 
adoption
 
of
 
agentic
 
AI
 
really
 
is
 
the
 
latest
 
rage.
 
And
 
so
 
we're
 
seeing
 
a
 
lot
 
of
 
demand
 
coming
 
from
 
a
 
lot
 
of
 
diﬀer ent
 
places.
 
Oper ator
 
Your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Toshiya
 
Hari
 
of
 
Goldman
 
Sachs.
 
Toshiya
 
Hari
 
Jensen,
 
you
 
executed
 
the
 
mass
 
change
 
earlier
 
this
 
year.
 
There
 
were
 
some
 
reports
 
over
 
the
 
week end
 
about
 
some
 
heating
 
issues.
 
On
 
the
 
back
 
of
 
this,
 
we've
 
had
 
invest ors
 
ask
 
about
 
your
 
ability
 
to
 
execute
 
to
 
the
 
road
 
map
 
you
 
present ed
 
at
 
GTC
 
this
 
year
 
with
 
Ultra
 
coming
 
out
 
next
 
year
 
and
 
the
 
transition
 
to
 
Ruben
 
in
 
'26.
 
Can
 
you
 
sort
 
of
 
speak
 
to
 
that?
 
And
 
some
 
invest ors
 
are
 
questioning
 
that,
 
so
 
if
 
you
 
can
 
sort
 
of
 
speak
 
to
 
your
 
ability
 
to
 
execute
 
on
 
time,
 
that
 
would
 
be
 
super
 
helpful.
 
And
 
then
 
a
 
quick
 
part
 
B.
 
On
 
supply
 
constr aints,
 
is
 
it
 
a
 
multitude
 
of
 
componentr y
 
that's
 
causing
 
this
 
or
 
is
 
it
 
speciﬁcally
 
HBN?
 
Is
 
it
 
supply
 
constr aints?
 
Are
 
the
 
supply
 
constr aints
 
getting
 
better?
 
Are
 
they
 
worsening?
 
Any
 
sort
 
of
 
color
 
on
 
that
 
would
 
be
 
super
 
helpful
 
as
 
well.
 
Jensen
 
Huang
 
So
 
let's
 
see.
 
Back
 
to
 
the
 
ﬁrst
 
question.
 
Blackwell
 
production
 
is
 
in
 
full
 
steam.
 
In
 
fact,
 
as
 
Colett e
 
mentioned
 
earlier ,
 
we
 
will
 
deliver
 
this
 
quarter
 
more
 
Blackwells
 
than
 
we
 
had
 
previously
 
estimat ed.
 
And
 
so
 
the
 
supply
 
chain
 
team
 
is
 
doing
 
an
 
incredible
 
job
 
working
 
with
 
our
 
supply
 
partners
 
to
 
increase
 
Blackwell,
 
and
 
we're
 
going
 
to
 
continue
 
to
 
work
 
hard
 
to
 
increase
 
Blackwell
 
through
 
next
 
year.
 
It
 
is
 
the
 
case
 
that
 
demand
 
exceeds
 
our
 
supply .
 
And
 
that's
 
expect ed
 
as
 
we're
 
in
 
the
 
beginnings
 
of
 
this
 
gener ative
 
AI
 
revolution
 
as
 
we
 
all
 
know .
 
And
 
we're
 
at
 
the
 
beginning
 
of
 
a
 
new
 
gener ation
 
of
 
foundation
 
models
 
that
 
are
 
able
 
to
 
do
 
reasoning
 
and
 
able
 
to
 
do
 
long
 
thinking.
 
And
 
of
 
course,
 
one
 
of
 
the
 
really
 
exciting
 
areas
 
is
 
physical
 
AI,
 
AI
 
that
 
now
 
understands
 
the
 
structur e
 
of
 
the
 
physical
 
world.
 
And
 
so
 
Blackwell
 
demand
 
is
 
very
 
strong.
 
Our
 
execution
 
is
 
going
 
well.
 
And
 
there's
 
obviously
 
a
 
lot
 
of
 
engineering
 
that
 
we're
 
doing
 
across
 
the
 
world.
 
You
 
see
 
now
 
systems
 
that
 
are
 
being
 
stood
 
up
 
by
 
Dell
 
and
 
CoreWeave.
 
I
 
think
 
you
 
saw
 
systems
 
from
 
Oracle
 
stood
 
up.
 
You
 
have
 
systems
 
from
 
Microsoft,
 
and
 
they'r e
 
about
 
to
 
preview
 
their
 
Grace
 
Blackwell
 
systems.
 
You
 
have
 
systems
 
that
 
are
 
at
 
Google.
 
And
 
so
 
all
 
of
 
these
 
CSPs
 
are
 
racing
 
to
 
be
 
ﬁrst.
 
The
 
engineering
 
that
 
we
 
do
 
with
 
them
 
is,
 
as
 
you
 
know ,
 
rather
 
complicat ed.
 
And
 
the
 
reason
 
for
 
that
 
is
 
because
 
although
 
we
 
build
 
full
 
stack
 
and
 
full
 
infrastructur e,
 
we
 
disaggr egate
 
all
 
of
 
this
 
AI
 
super comput er
 
and
 
we
 
integrate
 
it
 
into
 
all
 
of
 
the
 
custom
 
data
 
centers
 
and
 
architectur es
 
around
 
the
 
world.
 
That
 
integration
 
process,
 
it's
 
something
 
we've
 
done
 
sever al
 
gener ations
 
now.
 
We're
 
very
 
good
 
at
 
it
 
but
 
still
 
there's
 
still
 
a
 
lot
 
of
 
engineering
 
that
 
happens
 
at
 
this
 
point.
 
But
 
as
 
you
 
see
 
from
 
all
 
of
 
the
 
systems
 
that
 
are
 
being
 
stood
 
up,
 
Blackwell
 
is
 
in
 
great
 
shape.
 
And
 
as
 
we
 
mentioned
 
earlier ,
 
the
 
supply
 
and
 
what
 
we're
 
planning
 
to
 
ship
 
this
 
quarter
 
is
 
greater
 
than
 
our
 
previous
 
estimat es.
 
With
 
respect
 
to
 
the
 
supply
 
chain,
 
there
 
are
 
7
 
diﬀer ent
 
chips,
 
7
 
custom
 
chips
 
that
 
we
 
built
 
in
 
order
 
for
 
us
 
to
 
deliver
 
the
 
Blackwell
 
systems.
 
The
 
Blackwell
 
systems
 
go
 
in
 
air
 
cooled
 
or
 
liquid
 
cooled,
 
NVLink
 
8
 
or
 
NVLink
 
72
 
or
 
NVLink
 
8,
 
NVLink
 
36,
 
NVLink
 
72.
 
We
 
have
 
x86
 
or
 
Grace.
 
And
 
the
 
integration
 
of
 
all
 
of
 
those
 
systems
 
into
 
the
 
world's
 
data
 
centers
 
is
 
nothing
 
short
 
of
 
a
 
miracle.
 
And
 
so
 
the
 
component
 
supply
 
chain
 
necessar y
 
to
 
ramp
 
at
 
this
 
scale,
 
you
 
have
 
to
 
go
 
back
 
and
 
take
 
a
 
look
 
at
 
how
 
much
 
Blackwell
 
we
 
shipped
 
last
 
quarter,
 
which
 
was
 
0.
 
And
 
in
 
terms
 
of
 
how
 
much
 
Blackwell
 
total
 
systems
 
will
 
ship
 
this
 
quarter,
 
which
 
is
 
measur ed
 
in
 
billions,
 
the
 
ramp
 
is
 
incredible.
 
And
 
so
 
almost
 
every
 
company
 
in
 
the
 
world
 
seems
 
to
 
be
 
involved
 
in
 
our
 
supply
 
chain.
 
And
 
we've
 
got
 
great
 
partners,
 
everybody
 
from,
 
of
 
course,
 
TSMC
 
and
 
Amphenol,
 
the
 
connect or
 
company,
 
incredible
 
company .
 
Vertiv
 
and
 
SK
 
Hynix
 
and
 
Micron,
 
Spill
 
Amcor ,
 
KYEC,
 
and
 
there's
 
Foxconn
 
and
 
the
 
factories
 
that
 
they've
 
built
 
and
 
Quanta
 
and
 
we
 
win.
 
And
 
gosh,
 
Dell
 
and
 
HP,
 
and
 
Super
 
Micro,
 
Lenovo
 
and
 
the
 
number
 
of
 
companies
 
is
 
just
 
really
 
quite
 
incredible,
 
Quanta.
 
And
 
I'm
 
sure
 
I've
 
missed
 
partners
 
that
 
are
 
involved
 
in
 
the
 
ramping
 
of
 
Blackwell,
 
which
 
I
 
really
 
appreciat e.
 
And
 
so
 
anyways,
 
I
 
think
 
we're
 
in
 
great
 
shape
 
with
 
respect
 
to
 
the
 
Blackwell
 
ramp
 
at
 
this
 
point.
 
And
 
then
 
lastly,
 
your
 
question
 
about
 
our
 
execution
 
of
 
our
 
road
 
map.
 
We're
 
on
 
an
 
annual
 
road
 
map
 
and
 
we're
 
expecting
 
to
 
continue
 
to
 
execute
 
on
 
our
 
annual
 
road
 
map.
 
And
 
by
 
doing
 
so,
 
we
 
increased
 
the
 
performance,
 
of
 
course,
 
of
 
our
 
platform.
 
But
 
it's
 
also
 
really
 
impor tant
 
to
 
realize
 
that
 
when
 
we're
 
able
 
to
 
increase
 
performance
 
and
 
do
 
so
 
at
 
factors
 
at
 
a
 
time,
 
we're
 
reducing
 
the
 
cost
 
of
 
training.
 
We're
 
reducing
 
the
 
cost
 
of
 
inferencing.
 
We're
 
reducing
 
the
 
cost
 
of
 
AI
 
so
 
that
 
it
 
could
 
be
 
much
 
more
 
accessible.
 
But
 
the
 
other
 
factor
 
that's
 
very
 
impor tant
 
to
 
note
 
is
 
that
 
when
 
there's
 
a
 
data
 
center
 
of
 
some
 
ﬁxed
 
size
 
and
 
the
 
data
 
center
 
always
 
is
 
of
 
some
 
ﬁxed
 
size.
 
It
 
could
 
be,
 
of
 
course,
 
tens
 
of
 
megawatts
 
in
 
the
 
past,
 
and
 
now
 
it's
 
--
 
most
 
data
 
centers
 
are
 
now
 
100
 
megawatts
 
to
 
sever al
 
hundr ed
 
megawatts,
 
and
 
we're
 
planning
 
on
 
gigawatt
 
data
 
centers,
 
it
 
doesn't
 
really
 
matter
 
how
 
large
 
the
 
data
 
centers
 
are.
 
The
 
power
 
is
 
limited.
 
And
 
when
 
you'r e
 
in
 
the
 
power -limit ed
 
data
 
center,
 
the
 
best
 
--
 
the
 
highest
 
performance
 
per
 
watt
 
translat es
 
directly
 
into
 
the
 
highest
 
revenues
 
for
 
our
 
partners.
 
And
 
so
 
on
 
the
 
1
 
hand,
 
our
 
annual
 
road
 
map
 
reduces
 
cost.
 
But
 
on
 
the
 
other
 
hand,
 
because
 
our
 
perf
 
per
 
watt
 
is
 
so
 
good
 
compar ed
 
to
 
anything
 
out
 
there,
 
we
 
gener ate
 
for
 
our
 
customers
 
the
 
greatest
 
possible
 
revenues.
 
And
 
so
 
that
 
annual
 
rhythm
 
is
 
really
 
impor tant
 
to
 
us,
 
and
 
we
 
have
 
every
 
intentions
 
of
 
continuing
 
to
 
do
 
that.
 
And
 
everything
 
is
 
on
 
track
 
as
 
far
 
as
 
I
 
know .
 
Oper ator
 
Your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Timothy
 
Arcuri
 
of
 
UBS.
 
Timothy
 
Arcuri
 
I'm
 
wondering
 
if
 
you
 
can
 
talk
 
about
 
the
 
traject ory
 
of
 
how
 
Blackwell
 
is
 
going
 
to
 
ramp
 
this
 
year.
 
I
 
know
 
Jensen,
 
you
 
did
 
just
 
talk
 
about
 
Blackwell
 
being
 
better
 
than
 
I
 
think
 
you
 
had
 
said
 
sever al
 
billions
 
of
 
dollars
 
in
 
Januar y.
 
It
 
sounds
 
like
 
you'r e
 
going
 
to
 
do
 
more
 
than
 
that.
 
But
 
I
 
think
 
in
 
recent
 
months
 
also,
 
you
 
said
 
that
 
Blackwell
 
crosses
 
over
 
Hopper
 
in
 
the
 
April
 
quarter.
 
So
 
I
 
guess
 
I
 
had
 
2
 
questions.
 
First
 
of
 
all,
 
is
 
that
 
still
 
the
 
right
 
way
 
to
 
think
 
about
 
it,
 
that
 
Blackwell
 
will
 
cross
 
over
 
Hopper
 
in
 
April?
 
And
 
then
 
Colett e,
 
you
 
kind
 
of
 
talked
 
about
 
Blackwell
 
bringing
 
down
 
gross
 
margin
 
to
 
the
 
low
 
70s
 
as
 
it
 
ramps.
 
So
 
I
 
guess
 
if
 
April
 
is
 
the
 
crossover ,
 
is
 
that
 
the
 
worst
 
of
 
the
 
pressur e
 
on
 
gross
 
margin?
 
So
 
you'r e
 
going
 
to
 
be
 
kind
 
of
 
in
 
the
 
low
 
70s
 
as
 
soon
 
as
 
April.
 
I'm
 
just
 
wondering
 
if
 
you
 
can
 
sort
 
of
 
shape
 
that
 
for
 
us.
 
Jensen
 
Huang
 
Colett e,
 
why
 
don't
 
you
 
start?
 
Colett e
 
Kress
 
Sure,
 
let
 
me
 
ﬁrst
 
start
 
with
 
your
 
question,
 
Tim,
 
thank
 
you,
 
regarding
 
our
 
gross
 
margins.
 
And
 
we
 
discussed
 
that
 
our
 
gross
 
margins
 
as
 
we
 
are
 
ramping
 
Blackwell
 
in
 
the
 
very
 
beginning
 
and
 
the
 
many
 
diﬀer ent
 
conﬁgur ations,
 
the
 
many
 
diﬀer ent
 
chips
 
that
 
we
 
are
 
bringing
 
to
 
mark et,
 
we
 
are
 
going
 
to
 
focus
 
on
 
making
 
sure
 
we
 
have
 
the
 
best
 
experience
 
for
 
our
 
customers
 
as
 
they
 
stand
 
that
 
up.
 
We
 
will
 
start
 
growing
 
into
 
our
 
gross
 
margins
 
but
 
we
 
do
 
believe
 
those
 
will
 
be
 
in
 
the
 
low
 
70s
 
in
 
that
 
ﬁrst
 
program.
 
So
 
you'r e
 
correct,
 
as
 
you
 
look
 
at
 
the
 
quarters
 
following
 
after
 
that,
 
we
 
will
 
start
 
increasing
 
our
 
gross
 
margins,
 
and
 
we
 
hope
 
to
 
get
 
to
 
the
 
mid-70s
 
quite
 
quickly
 
as
 
part
 
of
 
that
 
round.
 
Jensen
 
Huang
 
Hopper
 
demand
 
will
 
continue
 
through
 
next
 
year,
 
surely
 
the
 
ﬁrst
 
sever al
 
quarters
 
of
 
the
 
next
 
year.
 
And
 
meanwhile,
 
we
 
will
 
ship
 
more
 
Blackwells
 
next
 
quarter
 
than
 
this,
 
and
 
we'll
 
ship
 
more
 
Blackwells
 
the
 
quarter
 
after
 
that
 
than
 
our
 
ﬁrst
 
quarter.
 
And
 
so
 
that
 
kind
 
of
 
puts
 
it
 
in
 
perspective.
 
We
 
are
 
really
 
at
 
the
 
beginnings
 
of
 
2
 
fundamental
 
shifts
 
in
 
computing
 
that
 
is
 
really
 
quite
 
signiﬁcant.
 
The
 
ﬁrst
 
is
 
moving
 
from
 
coding
 
that
 
runs
 
on
 
CPUs
 
to
 
machine
 
learning
 
that
 
creates
 
neural
 
networks
 
that
 
runs
 
on
 
GPUs.
 
And
 
that
 
fundamental
 
shift
 
from
 
coding
 
to
 
machine
 
learning
 
is
 
widespr ead
 
at
 
this
 
point.
 
There
 
are
 
no
 
companies
 
who
 
are
 
not
 
going
 
to
 
do
 
machine
 
learning.
 
And
 
so
 
machine
 
learning
 
is
 
also
 
what
 
enables
 
gener ative
 
AI.
 
And
 
so
 
on
 
the
 
1
 
hand,
 
the
 
ﬁrst
 
thing
 
that's
 
happening
 
is
 
$1
 
trillion
 
worth
 
of
 
computing
 
systems
 
and
 
data
 
centers
 
around
 
the
 
world
 
is
 
now
 
being
 
moderniz ed
 
for
 
machine
 
learning.
 
On
 
the
 
other
 
hand,
 
secondarily,
 
I
 
guess,
 
is
 
that
 
on
 
top
 
of
 
these
 
systems
 
are
 
going
 
to
 
be
 
--
 
we're
 
going
 
to
 
be
 
creating
 
a
 
new
 
type
 
of
 
capability
 
called
 
AI.
 
And
 
when
 
we
 
say
 
gener ative
 
AI,
 
we're
 
essentially
 
saying
 
that
 
these
 
data
 
centers
 
are
 
really
 
AI
 
factories.
 
They'r e
 
gener ating
 
something.
 
Just
 
like
 
we
 
gener ate
 
electricity,
 
we're
 
now
 
going
 
to
 
be
 
gener ating
 
AI.
 
And
 
if
 
the
 
number
 
of
 
customers
 
is
 
large,
 
just
 
as
 
the
 
number
 
of
 
consumers
 
of
 
electricity
 
is
 
large,
 
these
 
gener ators
 
are
 
going
 
to
 
be
 
running
 
24/7.
 
And
 
today,
 
many
 
AI
 
services
 
are
 
running
 
24/7,
 
just
 
like
 
an
 
AI
 
factory.
 
And
 
so
 
we're
 
going
 
to
 
see
 
this
 
new
 
type
 
of
 
system
 
come
 
online,
 
and
 
I
 
call
 
it
 
an
 
AI
 
factory
 
because
 
that's
 
really
 
as
 
close
 
to
 
what
 
it
 
is.
 
It's
 
unlike
 
a
 
data
 
center
 
of
 
the
 
past.
 
And
 
so
 
these
 
2
 
fundamental
 
trends
 
are
 
really
 
just
 
beginning.
 
And
 
so
 
we
 
expect
 
this
 
to
 
happen,
 
this
 
growth,
 
this
 
modernization
 
and
 
the
 
creation
 
of
 
a
 
new
 
industr y
 
to
 
go
 
on
 
for
 
sever al
 
years.
 
Oper ator
 
Your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Vivek
 
Arya
 
of
 
Bank
 
of
 
America
 
Securities.
 
Vivek
 
Arya
 
Colett e,
 
just
 
to
 
clarify,
 
do
 
you
 
think
 
it's
 
a
 
fair
 
assumption
 
to
 
think
 
NVIDIA
 
could
 
recover
 
to
 
kind
 
of
 
mid-70s
 
gross
 
margin
 
in
 
the
 
back
 
half
 
of
 
calendar
 
'25?
 
Just
 
want ed
 
to
 
clarify
 
that.
 
And
 
then
 
Jensen,
 
my
 
main
 
question,
 
historically,
 
when
 
we
 
have
 
seen
 
hardwar e
 
deployment
 
cycles,
 
they
 
have
 
inevitably
 
included
 
some
 
digestion
 
along
 
the
 
way.
 
When
 
do
 
you
 
think
 
we
 
get
 
to
 
that
 
phase?
 
Or
 
is
 
it
 
just
 
too
 
prematur e
 
to
 
discuss
 
that
 
because
 
you'r e
 
just
 
at
 
the
 
start
 
of
 
Blackwell?
 
So
 
how
 
many
 
quarters
 
of
 
shipments
 
do
 
you
 
think
 
is
 
requir ed
 
to
 
kind
 
of
 
satisfy
 
this
 
ﬁrst
 
wave?
 
Can
 
you
 
continue
 
to
 
grow
 
this
 
into
 
calendar
 
'26?
 
Just
 
how
 
should
 
we
 
be
 
prepared
 
to
 
see
 
what
 
we
 
have
 
seen
 
historically,
 
right,
 
a
 
period
 
of
 
digestion
 
along
 
the
 
way
 
of
 
a
 
long-t erm
 
kind
 
of
 
secular
 
hardwar e
 
deployment?
 
Colett e
 
Kress
 
Okay .
 
Vivek,
 
thank
 
you
 
for
 
the
 
question.
 
Let
 
me
 
clarify
 
your
 
question
 
regarding
 
gross
 
margins.
 
Could
 
we
 
reach
 
the
 
mid-70s
 
in
 
the
 
second
 
half
 
of
 
next
 
year?
 
And
 
yes,
 
I
 
think
 
it
 
is
 
a
 
reasonable
 
assumption
 
or
 
goal
 
for
 
us
 
to
 
do,
 
but
 
we'll
 
just
 
have
 
to
 
see
 
how
 
that
 
mix
 
of
 
ramp
 
goes.
 
But
 
yes,
 
it
 
is
 
deﬁnit ely
 
possible.
 
Jensen
 
Huang
 
The
 
way
 
to
 
think
 
through
 
that,
 
Vivek,
 
is
 
I
 
believe
 
that
 
there
 
will
 
be
 
no
 
digestion
 
until
 
we
 
moderniz e
 
$1
 
trillion
 
with
 
the
 
data
 
centers.
 
Those
 
--
 
if
 
you
 
just
 
look
 
at
 
the
 
world's
 
data
 
centers,
 
the
 
vast
 
majority
 
of
 
it
 
is
 
built
 
for
 
a
 
time
 
when
 
we
 
wrote
 
applications
 
by
 
hand
 
and
 
we
 
ran
 
them
 
on
 
CPUs.
 
It's
 
just
 
not
 
a
 
sensible
 
thing
 
to
 
do
 
anymor e.
 
If
 
you
 
have
 
--
 
if
 
every
 
company's
 
CapEx,
 
if
 
they'r e
 
ready
 
to
 
build
 
data
 
center
 
tomorr ow,
 
they
 
ought
 
to
 
build
 
it
 
for
 
a
 
future
 
of
 
machine
 
learning
 
and
 
gener ative
 
AI
 
because
 
they
 
have
 
plenty
 
of
 
old
 
data
 
centers.
 
And
 
so
 
what's
 
going
 
to
 
happen
 
over
 
the
 
course
 
of
 
the
 
next
 
X
 
number
 
of
 
years,
 
and
 
let's
 
assume
 
that
 
over
 
the
 
course
 
of
 
4
 
years,
 
the
 
world's
 
data
 
centers
 
could
 
be
 
moderniz ed
 
as
 
we
 
grow
 
into
 
IT,
 
as
 
you
 
know ,
 
IT
 
continues
 
to
 
grow
 
about
 
20%,
 
30%
 
a
 
year,
 
let's
 
say.
 
And
 
so
 
--
 
but
 
let's
 
say
 
by
 
2030,
 
the
 
world's
 
data
 
centers
 
for
 
computing
 
is,
 
call
 
it,
 
a
 
couple
 
of
 
trillion
 
dollars.
 
We
 
have
 
to
 
grow
 
into
 
that.
 
We
 
have
 
to
 
moderniz e
 
the
 
data
 
center
 
from
 
coding
 
to
 
machine
 
learning.
 
That's
 
number
 
one.
 
The
 
second
 
part
 
of
 
it
 
is
 
gener ative
 
AI.
 
And
 
we're
 
now
 
producing
 
a
 
new
 
type
 
of
 
capability
 
the
 
world's
 
never
 
known,
 
a
 
new
 
mark et
 
segment
 
that
 
the
 
world's
 
never
 
had.
 
If
 
you
 
look
 
at
 
OpenAI,
 
it
 
didn't
 
replace
 
anything.
 
It's
 
something
 
that's
 
complet ely
 
brand
 
new.
 
It's,
 
in
 
a
 
lot
 
of
 
ways
 
as
 
when
 
the
 
iPhone
 
came,
 
was
 
complet ely
 
brand
 
new.
 
It
 
wasn't
 
really
 
replacing
 
anything.
 
And
 
so
 
we're
 
going
 
to
 
see
 
more
 
and
 
more
 
companies
 
like
 
that.
 
And
 
they'r e
 
going
 
to
 
create
 
and
 
gener ate,
 
out
 
of
 
their
 
services,
 
essentially
 
intelligence.
 
Some
 
of
 
it
 
would
 
be
 
digital
 
intelligence
 
like
 
Runway .
 
Some
 
of
 
it
 
would
 
be
 
basic
 
intelligence
 
like
 
OpenAI.
 
Some
 
of
 
it
 
would
 
be
 
legal
 
intelligence
 
like
 
Harvey.
 
Digital
 
mark eting
 
intelligence
 
like
 
riders,
 
so
 
on
 
and
 
so
 
forth.
 
And
 
the
 
number
 
of
 
these
 
companies,
 
these
 
--
 
what
 
are
 
they
 
called,
 
AI-native
 
companies,
 
are
 
just
 
in
 
hundr eds.
 
And
 
almost
 
every
 
platform
 
shift,
 
there
 
was
 
--
 
there
 
were
 
Internet
 
companies,
 
as
 
you
 
recall.
 
There
 
were
 
cloud-ﬁrst
 
companies.
 
There
 
were
 
mobile-ﬁrst
 
companies.
 
Now
 
they'r e
 
AI
 
natives.
 
And
 
so
 
these
 
companies
 
are
 
being
 
created
 
because
 
people
 
see
 
that
 
there's
 
a
 
platform
 
shift,
 
and
 
there's
 
a
 
brand-new
 
oppor tunity
 
to
 
do
 
something
 
complet ely
 
new.
 
And
 
so
 
my
 
sense
 
is
 
that
 
we're
 
going
 
to
 
continue
 
to
 
build
 
out
 
to
 
moderniz e
 
IT,
 
moderniz e
 
computing,
 
number
 
one;
 
and
 
then
 
number
 
two,
 
create
 
these
 
AI
 
factories
 
that
 
are
 
going
 
to
 
be
 
for
 
a
 
new
 
industr y
 
for
 
the
 
production
 
of
 
artiﬁcial
 
intelligence.
 
Oper ator
 
Your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Stacy
 
Rasgon
 
of
 
Bernst ein
 
Resear ch.
 
Stacy
 
Rasgon
 
Colett e,
 
I
 
had
 
a
 
clariﬁcation
 
and
 
a
 
question
 
for
 
you.
 
The
 
clariﬁcation,
 
just
 
when
 
you
 
say
 
low
 
70s
 
gross
 
margins,
 
does
 
73.5%
 
count
 
as
 
low
 
70s
 
or
 
do
 
you
 
have
 
something
 
else
 
in
 
mind?
 
And
 
for
 
my
 
question,
 
you'r e
 
guiding
 
total
 
revenues,
 
and
 
so
 
I
 
mean,
 
total
 
data
 
center
 
revenues
 
in
 
the
 
next
 
quarter
 
must
 
be
 
up
 
sever al
 
billion
 
dollars.
 
But
 
it
 
sounds
 
like
 
Blackwell
 
now
 
should
 
be
 
up
 
more
 
than
 
that.
 
But
 
you
 
also
 
said
 
Hopper
 
was
 
still
 
strong,
 
so
 
like
 
is
 
Hopper
 
down
 
sequentially
 
next
 
quarter?
 
And
 
if
 
it
 
is,
 
like
 
why?
 
Is
 
it
 
because
 
of
 
the
 
supply
 
constr aints?
 
China
 
has
 
been
 
pretty
 
strong.
 
Is
 
China
 
is
 
kind
 
of
 
rolling
 
oﬀ
 
a
 
bit
 
into
 
Q4?
 
So
 
any
 
color
 
you
 
can
 
give
 
us
 
on
 
sort
 
of
 
the
 
Blackwell
 
ramp
 
and
 
the
 
Blackwell
 
versus
 
Hopper
 
behavior
 
into
 
Q4
 
would
 
be
 
really
 
helpful.
 
Colett e
 
Kress
 
So
 
ﬁrst,
 
starting
 
on
 
your
 
ﬁrst
 
question
 
there,
 
Stacy,
 
regarding
 
our
 
gross
 
margin
 
and
 
deﬁne
 
low.
 
Low,
 
of
 
course,
 
is
 
below
 
the
 
mids
 
and
 
let's
 
say,
 
we
 
might
 
be
 
at
 
71%,
 
maybe
 
about
 
72%,
 
72.5%.
 
We're
 
going
 
to
 
be
 
in
 
that
 
range.
 
We
 
could
 
be
 
higher
 
than
 
that
 
as
 
well.
 
We're
 
just
 
going
 
to
 
have
 
to
 
see
 
how
 
it
 
comes
 
through.
 
We
 
do
 
want
 
to
 
make
 
sure
 
that
 
we
 
are
 
ramping
 
and
 
continuing
 
that
 
improvement,
 
the
 
improvement
 
in
 
terms
 
of
 
our
 
yields,
 
the
 
improvement
 
in
 
terms
 
of
 
the
 
product
 
as
 
we
 
go
 
through
 
the
 
rest
 
of
 
the
 
year
 
So
 
we'll
 
get
 
up
 
to
 
the
 
mid-70s
 
by
 
that
 
point.
 
The
 
second
 
statement
 
was
 
a
 
question
 
regarding
 
our
 
Hopper
 
and
 
what
 
is
 
our
 
Hopper
 
doing?
 
We
 
have
 
seen
 
substantial
 
growth
 
for
 
H200
 
not
 
only
 
in
 
terms
 
of
 
orders
 
but
 
the
 
quickness
 
in
 
terms
 
of
 
those
 
that
 
are
 
standing
 
that
 
out.
 
It
 
is
 
an
 
amazing
 
product
 
and
 
it's
 
the
 
fastest
 
growing
 
and
 
ramping
 
that
 
we've
 
seen.
 
We
 
will
 
continue
 
to
 
be
 
selling
 
Hopper
 
in
 
this
 
quarter,
 
in
 
Q4
 
for
 
sure.
 
That
 
is
 
across
 
the
 
board
 
in
 
terms
 
of
 
all
 
of
 
our
 
diﬀer ent
 
conﬁgur ations,
 
and
 
our
 
conﬁgur ations
 
include
 
what
 
we
 
may
 
do
 
in
 
terms
 
of
 
China.
 
But
 
keep
 
that
 
in
 
mind
 
that
 
folks
 
are
 
also,
 
at
 
the
 
same
 
time,
 
looking
 
to
 
build
 
out
 
their
 
Blackwell.
 
So
 
we've
 
got
 
a
 
little
 
bit
 
of
 
both
 
happening
 
in
 
Q4.
 
But
 
yes,
 
is
 
it
 
possible
 
for
 
Hopper
 
to
 
grow
 
between
 
Q3
 
and
 
Q4?
 
It's
 
possible
 
but
 
we'll
 
just
 
have
 
to
 
see.
 
Oper ator
 
Your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Joseph
 
Moor e
 
of
 
Morgan
 
Stanley .
 
Joseph
 
Moor e
 
Great.
 
I
 
wonder
 
if
 
you
 
could
 
talk
 
a
 
little
 
bit
 
about
 
what
 
you'r e
 
seeing
 
in
 
the
 
inference
 
mark et.
 
You've
 
talked
 
about
 
Strawberr y
 
and
 
some
 
of
 
the
 
ramiﬁcations
 
of
 
longer
 
scaling
 
inﬂuence
 
projects.
 
But
 
you've
 
also
 
talked
 
about
 
the
 
possibility
 
that
 
as
 
some
 
of
 
these
 
Hopper
 
clust ers
 
age,
 
that
 
you
 
could
 
use
 
some
 
of
 
the
 
Hopper
 
chips
 
for
 
inference.
 
So
 
I
 
guess
 
do
 
you
 
expect
 
inference
 
to
 
outgr ow
 
training
 
in
 
the
 
next
 
kind
 
of
 
12
 
months
 
time
 
frame?
 
And
 
just
 
gener ally,
 
your
 
thoughts
 
there.
 
Jensen
 
Huang
 
Our
 
hopes
 
and
 
dreams
 
is
 
that
 
someday,
 
the
 
world
 
does
 
a
 
ton
 
of
 
inference.
 
And
 
that's
 
when
 
AI
 
has
 
really
 
exceeded
 
is
 
when
 
every
 
single
 
company
 
is
 
doing
 
inference
 
inside
 
their
 
companies
 
for
 
the
 
mark eting
 
depar tment
 
and
 
forecasting
 
depar tment
 
and
 
supply
 
chain
 
group
 
and
 
their
 
legal
 
depar tment
 
and
 
engineering,
 
of
 
course,
 
and
 
coding
 
of
 
course.
 
And
 
so
 
we
 
hope
 
that
 
every
 
company
 
is
 
doing
 
inference
 
24/7.
 
And
 
that
 
there
 
will
 
be
 
a
 
whole
 
bunch
 
of
 
AI
 
native
 
startups,
 
thousands
 
of
 
AI
 
native
 
startups
 
that
 
are
 
gener ating
 
tokens
 
and
 
gener ating
 
AI.
 
And
 
every
 
aspect
 
of
 
your
 
comput er
 
experience
 
from
 
using
 
Outlook
 
to
 
PowerP ointing
 
or
 
when
 
you'r e
 
sitting
 
there
 
with
 
Excel,
 
you'r e
 
constantly
 
gener ating
 
tokens.
 
And
 
every
 
time
 
you
 
read
 
a
 
PDF,
 
open
 
a
 
PDF,
 
it
 
gener ated
 
a
 
whole
 
bunch
 
of
 
tokens.
 
One
 
of
 
my
 
favorit e
 
applications
 
is
 
NotebookLM,
 
this
 
Google
 
application
 
that
 
came
 
out.
 
I
 
used
 
the
 
living
 
daylights
 
out
 
of
 
it
 
just
 
because
 
it's
 
fun.
 
And
 
I
 
put
 
every
 
PDF,
 
every
 
archived
 
paper
 
into
 
it
 
just
 
to
 
listen
 
to
 
it
 
as
 
well
 
as
 
scanning
 
through
 
it.
 
And
 
so
 
I
 
think
 
that's
 
the
 
goal
 
is
 
to
 
train
 
these
 
models
 
so
 
that
 
people
 
use
 
it.
 
And
 
there's
 
now
 
a
 
whole
 
new
 
era
 
of
 
AI,
 
if
 
you
 
will,
 
a
 
whole
 
new
 
genre
 
of
 
AI
 
called
 
physical
 
AI.
 
Just
 
those
 
large
 
language
 
models
 
understand
 
the
 
human
 
language
 
and
 
how
 
the
 
thinking
 
process,
 
if
 
you
 
will.
 
Physical
 
AI
 
understands
 
the
 
physical
 
world.
 
And
 
it
 
understands
 
the
 
meaning
 
of
 
the
 
structur e
 
and
 
understands
 
what's
 
sensible
 
and
 
what's
 
not
 
and
 
what
 
could
 
happen
 
and
 
what
 
won't.
 
And
 
not
 
only
 
does
 
it
 
understand
 
but
 
it
 
can
 
predict,
 
roll
 
out
 
a
 
short
 
future.
 
That
 
capability
 
is
 
incredibly
 
valuable
 
for
 
industrial
 
AI
 
and
 
robotics.
 
And
 
so
 
that's
 
ﬁred
 
up
 
so
 
many
 
AI
 
native
 
companies
 
and
 
robotics
 
companies
 
and
 
physical
 
AI
 
companies
 
that
 
you'r e
 
probably
 
hearing
 
about.
 
And
 
it's
 
really
 
the
 
reason
 
why
 
we
 
built
 
Omniverse.
 
Omniverse
 
is
 
so
 
that
 
we
 
can
 
enable
 
these
 
AIs
 
to
 
be
 
created
 
and
 
learn
 
in
 
Omniverse
 
and
 
learn
 
from
 
synthetic
 
data
 
gener ation
 
and
 
reinfor cement,
 
learning
 
physics
 
feedback
 
instead
 
of
 
just
 
a
 
human
 
feedback,
 
it's
 
now
 
physics
 
feedback.
 
To
 
have
 
these
 
capabilities,
 
Omniverse
 
was
 
created
 
so
 
that
 
we
 
can
 
enable
 
physical
 
AI.
 
And
 
so
 
that
 
--
 
the
 
goal
 
is
 
to
 
gener ate
 
tokens.
 
The
 
goal
 
is
 
to
 
inference,
 
and
 
we're
 
starting
 
to
 
see
 
that
 
growth
 
happening.
 
So
 
I'm
 
super
 
excited
 
about
 
that.
 
Now
 
let
 
me
 
just
 
say
 
1
 
more
 
thing.
 
Inference
 
is
 
super
 
hard.
 
And
 
the
 
reason
 
why
 
inference
 
is
 
super
 
hard
 
is
 
because
 
you
 
need
 
the
 
accur acy
 
to
 
be
 
high
 
on
 
the
 
1
 
hand.
 
You
 
need
 
the
 
throughput
 
to
 
be
 
high
 
so
 
that
 
the
 
cost
 
could
 
be
 
as
 
low
 
as
 
possible,
 
but
 
you
 
also
 
need
 
the
 
latency
 
to
 
be
 
low.
 
And
 
comput ers
 
that
 
are
 
high-thr oughput
 
as
 
well
 
as
 
low
 
latency
 
is
 
incredibly
 
hard
 
to
 
build.
 
And
 
these
 
applications
 
have
 
long
 
context
 
lengths
 
because
 
they
 
want
 
to
 
understand.
 
They
 
want
 
to
 
be
 
able
 
to
 
inference
 
within
 
understanding
 
the
 
context
 
of
 
what
 
they'r e
 
being
 
asked
 
to
 
do.
 
And
 
so
 
the
 
context
 
length
 
is
 
growing
 
larger
 
and
 
larger.
 
On
 
the
 
other
 
hand,
 
the
 
models
 
are
 
getting
 
larger.
 
They'r e
 
multimodality .
 
Just
 
the
 
number
 
of
 
dimensions
 
that
 
inference
 
is
 
innovating
 
is
 
incredible.
 
And
 
this
 
innovation
 
rate
 
is
 
what
 
makes
 
NVIDIA's
 
architectur e
 
so
 
great
 
because
 
we
 
--
 
our
 
ecosyst em
 
is
 
fantastic.
 
Everybody
 
knows
 
that
 
if
 
they
 
innovat e
 
on
 
top
 
of
 
CUD A
 
and
 
top
 
of
 
NVIDIA's
 
architectur e,
 
they
 
can
 
innovat e
 
more
 
quickly
 
and
 
they
 
know
 
that
 
everything
 
should
 
work.
 
And
 
if
 
something
 
were
 
to
 
happen,
 
it's
 
probably
 
likely
 
their
 
code
 
and
 
not
 
ours.
 
And
 
so
 
that
 
ability
 
to
 
innovat e
 
in
 
every
 
single
 
direction
 
at
 
the
 
same
 
time,
 
having
 
a
 
large
 
installed
 
base
 
so
 
that
 
what ever
 
you
 
create
 
could
 
land
 
on
 
an
 
NVIDIA
 
comput er
 
and
 
be
 
deployed
 
broadly
 
all
 
around
 
the
 
world
 
in
 
every
 
single
 
data
 
center
 
all
 
the
 
way
 
out
 
to
 
the
 
edge
 
into
 
robotic
 
systems,
 
that
 
capability
 
is
 
really
 
quite
 
phenomenal.
 
Oper ator
 
Your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Aaron
 
Rakers
 
of
 
Wells
 
Fargo.
 
Aaron
 
Rakers
 
I
 
want ed
 
to
 
ask
 
you,
 
as
 
we
 
kind
 
of
 
focus
 
on
 
the
 
Blackwell
 
cycle
 
and
 
think
 
about
 
the
 
Data
 
Cent er
 
business.
 
When
 
I
 
look
 
at
 
the
 
results
 
this
 
last
 
quarter,
 
Colett e,
 
you
 
mentioned
 
that
 
obviously,
 
the
 
networking
 
business
 
was
 
down
 
about
 
15%
 
sequentially .
 
But
 
then
 
your
 
comments
 
were
 
that
 
you
 
were
 
seeing
 
very
 
strong
 
demand.
 
You
 
mentioned
 
also
 
that
 
you
 
had
 
multiple
 
cloud
 
CFP
 
design
 
wins
 
for
 
these
 
large-scale
 
clust ers.
 
So
 
I'm
 
curious
 
if
 
you
 
could
 
unpack
 
what's
 
going
 
on
 
in
 
the
 
Networking
 
business
 
and
 
wher e
 
maybe
 
you've
 
seen
 
some
 
constr aints
 
and
 
just
 
your
 
conﬁdence
 
in
 
the
 
pace
 
of
 
Spectrum-X
 
progressing
 
to
 
that
 
multiple
 
billions
 
of
 
dollars
 
that
 
you
 
previously
 
had
 
talked
 
about.
 
Colett e
 
Kress
 
Let's
 
ﬁrst
 
start
 
with
 
the
 
networking.
 
The
 
growth
 
year-over -year
 
is
 
tremendous.
 
And
 
our
 
focus
 
since
 
the
 
beginning
 
of
 
our
 
acquisition
 
of
 
Mellano x
 
has
 
really
 
been
 
about
 
building
 
together
 
the
 
work
 
that
 
we
 
do
 
in
 
terms
 
of
 
in
 
the
 
data
 
center.
 
The
 
networking
 
is
 
such
 
a
 
critical
 
part
 
of
 
that.
 
Our
 
ability
 
to
 
sell
 
our
 
networking
 
with
 
many
 
of
 
our
 
systems
 
that
 
we
 
are
 
doing
 
in
 
data
 
center
 
is
 
continuing
 
to
 
grow
 
and
 
do
 
quite
 
well.
 
So
 
this
 
quarter
 
is
 
just
 
a
 
slight
 
dip
 
down
 
and
 
we're
 
going
 
to
 
be
 
right
 
back
 
up
 
in
 
terms
 
of
 
growing.
 
We're
 
getting
 
ready
 
for
 
Blackwell
 
and
 
more
 
and
 
more
 
systems
 
that
 
will
 
be
 
using
 
not
 
only
 
our
 
existing
 
networking
 
but
 
also
 
the
 
networking
 
that
 
is
 
going
 
to
 
be
 
incorpor ated
 
in
 
a
 
lot
 
of
 
these
 
large
 
systems
 
we
 
are
 
providing
 
them
 
to.
 
Oper ator
 
Your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Atif
 
Malik
 
of
 
Citi.
 
Atif
 
Malik
 
I
 
have
 
2
 
quick
 
ones
 
for
 
Colett e.
 
Colett e,
 
on
 
the
 
last
 
earnings
 
call,
 
you
 
mentioned
 
that
 
sover eign
 
demand
 
is
 
in
 
low
 
double-digit
 
billions.
 
Can
 
you
 
provide
 
an
 
updat e
 
on
 
that?
 
And
 
then
 
can
 
you
 
explain
 
the
 
supply-constr ained
 
situation
 
in
 
gaming?
 
Is
 
that
 
because
 
you'r e
 
shifting
 
your
 
supply
 
towar ds
 
data
 
center?
 
Colett e
 
Kress
 
So
 
ﬁrst
 
starting
 
in
 
terms
 
of
 
sover eign
 
AI,
 
such
 
an
 
impor tant
 
part
 
of
 
growth,
 
something
 
that
 
is
 
really
 
surfaced
 
with
 
the
 
onset
 
of
 
gener ative
 
AI
 
and
 
building
 
models
 
in
 
the
 
individual
 
countries
 
around
 
the
 
world.
 
And
 
we
 
see
 
a
 
lot
 
of
 
them,
 
and
 
we
 
talked
 
about
 
a
 
lot
 
of
 
them
 
on
 
the
 
call
 
today
 
and
 
the
 
work
 
that
 
they
 
are
 
doing.
 
So
 
our
 
sover eign
 
AI
 
and
 
our
 
pipeline
 
going
 
forward
 
is
 
still
 
absolut ely
 
intact
 
as
 
those
 
are
 
working
 
to
 
build
 
these
 
foundational
 
models
 
in
 
their
 
own
 
language,
 
in
 
their
 
own
 
cultur e,
 
and
 
working
 
in
 
terms
 
of
 
the
 
enterprises
 
within
 
those
 
countries.
 
And
 
I
 
think
 
you'll
 
continue
 
to
 
see
 
this
 
be
 
growth
 
oppor tunities
 
that
 
you
 
may
 
see
 
with
 
our
 
regional
 
clouds
 
that
 
are
 
being
 
stood
 
up
 
and/or
 
those
 
that
 
are
 
focusing
 
in
 
terms
 
of
 
AI
 
factories
 
for
 
many
 
parts
 
of
 
the
 
sover eign
 
AI.
 
This
 
is
 
areas
 
wher e
 
this
 
is
 
growing
 
not
 
only
 
in
 
terms
 
of
 
in
 
Europe,
 
but
 
you'r e
 
also
 
seeing
 
this
 
in
 
terms
 
of
 
growth
 
in
 
terms
 
of
 
in
 
the
 
Asia
 
Pac
 
as
 
well.
 
Let
 
me
 
ﬂip
 
to
 
your
 
second
 
question
 
that
 
you
 
asked
 
regarding
 
Gaming.
 
So
 
our
 
Gaming
 
right
 
now
 
from
 
a
 
supply,
 
we're
 
busy
 
trying
 
to
 
make
 
sure
 
that
 
we
 
can
 
ramp
 
all
 
of
 
our
 
diﬀer ent
 
products.
 
And
 
in
 
this
 
case,
 
our
 
gaming
 
supply,
 
given
 
what
 
we
 
saw
 
selling
 
through
 
was
 
moving
 
quite
 
fast.
 
Now
 
the
 
challenge
 
that
 
we
 
have
 
is
 
how
 
fast
 
could
 
we
 
get
 
that
 
supply
 
getting
 
ready
 
into
 
the
 
mark et
 
for
 
this
 
quarter.
 
Not
 
to
 
worry,
 
I
 
think
 
we'll
 
be
 
back
 
on
 
track
 
with
 
more
 
supply
 
as
 
we
 
turn
 
the
 
corner
 
into
 
the
 
new
 
calendar
 
year.
 
We're
 
just
 
going
 
to
 
be
 
tight
 
for
 
this
 
quarter.
 
Oper ator
 
Your
 
next
 
question
 
comes
 
from
 
the
 
line
 
of
 
Ben
 
Reitz es
 
of
 
Melius
 
Resear ch.
 
Benjamin
 
Reitz es
 
I
 
want ed
 
to
 
ask
 
Colett e
 
and
 
Jensen
 
with
 
regard
 
to
 
sequential
 
growth.
 
So
 
very
 
strong
 
sequential
 
growth
 
this
 
quarter,
 
and
 
you'r e
 
guiding
 
to
 
about
 
7%.
 
Do
 
your
 
comments
 
on
 
Blackwell
 
imply
 
that
 
we
 
reacceler ate
 
from
 
there
 
as
 
you
 
get
 
more
 
supply
 
just
 
in
 
the
 
ﬁrst
 
half,
 
there
 
would
 
be
 
some
 
catch-up?
 
So
 
I
 
was
 
wondering
 
how
 
prescriptive
 
you
 
could
 
be
 
there.
 
And
 
then
 
Jensen,
 
just
 
overall
 
with
 
the
 
change
 
in
 
administr ation
 
that's
 
going
 
to
 
take
 
place
 
here
 
in
 
the
 
U.S.
 
and
 
the
 
China
 
situation,
 
have
 
you
 
gotten
 
any
 
sense
 
or
 
any
 
conversations
 
about
 
tariﬀs
 
or
 
anything
 
with
 
regard
 
to
 
your
 
China
 
business?
 
Any
 
sense
 
of
 
what
 
may
 
or
 
may
 
not
 
go
 
on?
 
It's
 
probably
 
too
 
early
 
but
 
wondering
 
if
 
you
 
had
 
any
 
thoughts
 
there.
 
Jensen
 
Huang
 
We
 
guide
 
1
 
quarter
 
at
 
a
 
time.
 
Colett e
 
Kress
 
We
 
are
 
working
 
right
 
now
 
on
 
the
 
quarter
 
that
 
we're
 
in
 
and
 
building
 
what
 
we
 
need
 
to
 
ship
 
in
 
terms
 
of
 
Blackwell.
 
We
 
have
 
every
 
supplier
 
on
 
the
 
planet
 
working
 
seamlessly
 
with
 
us
 
to
 
that.
 
And
 
once
 
we
 
get
 
to
 
next
 
quarter,
 
we'll
 
help
 
you
 
understand
 
in
 
terms
 
of
 
that
 
ramp
 
that
 
we'll
 
see
 
to
 
the
 
next
 
quarter
 
going
 
after
 
that.
 
Jensen
 
Huang
 
What ever
 
the
 
new
 
administr ation
 
decides,
 
we'll,
 
of
 
course,
 
suppor t
 
the
 
administr ation.
 
And
 
that's
 
our
 
--
 
the
 
highest
 
mandat e.
 
And
 
then
 
after
 
that,
 
do
 
the
 
best
 
we
 
can
 
and
 
just
 
as
 
we
 
always
 
do.
 
And
 
so
 
we
 
have
 
to
 
simultaneously
 
and
 
we
 
will
 
comply
 
with
 
any
 
regulation
 
that
 
comes
 
along
 
fully
 
and
 
suppor t
 
our
 
customers
 
to
 
the
 
best
 
of
 
our
 
abilities
 
and
 
to
 
compet e
 
in
 
the
 
mark etplace.
 
We'll
 
do
 
all
 
of
 
these
 
3
 
things
 
simultaneously .
 
Oper ator
 
Your
 
ﬁnal
 
question
 
comes
 
from
 
the
 
line
 
of
 
Pierre
 
Ferragu
 
of
 
New
 
Street
 
Resear ch.
 
Pierre
 
Ferragu
 
Jensen,
 
you
 
mentioned
 
in
 
your
 
comments,
 
you
 
have
 
the
 
pretraining,
 
the
 
actual
 
language
 
models
 
and
 
you
 
have
 
reinfor cement
 
learning
 
that
 
becomes
 
more
 
and
 
more
 
impor tant
 
in
 
training
 
and
 
inference
 
as
 
well
 
and
 
then
 
you
 
have
 
inference
 
itself.
 
And
 
I
 
was
 
wondering
 
if
 
you
 
have
 
a
 
sense
 
like
 
a
 
high-level
 
typical
 
sense
 
of
 
out
 
of
 
an
 
overall
 
AI
 
ecosyst em,
 
like
 
maybe
 
one
 
of
 
your
 
clients
 
or
 
one
 
of
 
the
 
large
 
models
 
that
 
are
 
out
 
there
 
today,
 
how
 
much
 
of
 
the
 
comput e
 
goes
 
into
 
each
 
of
 
these
 
buck ets?
 
How
 
much
 
for
 
the
 
pretraining?
 
How
 
much
 
for
 
the
 
reinfor cement?
 
And
 
how
 
much
 
into
 
inference
 
today?
 
Do
 
you
 
have
 
any
 
sense
 
for
 
how
 
it's
 
splitting
 
and
 
wher e
 
the
 
growth
 
is
 
the
 
most
 
impor tant
 
as
 
well?
 
Jensen
 
Huang
 
Well,
 
today,
 
it's
 
vastly
 
in
 
pretraining
 
a
 
foundation
 
model
 
because,
 
as
 
you
 
know ,
 
post-tr aining,
 
the
 
new
 
technologies
 
are
 
just
 
coming
 
online.
 
And
 
what ever
 
you
 
could
 
do
 
in
 
pretraining
 
and
 
post-tr aining,
 
you
 
would
 
try
 
to
 
do
 
so
 
that
 
the
 
inference
 
cost
 
could
 
be
 
as
 
low
 
as
 
possible
 
for
 
everyone.
 
However ,
 
there
 
are
 
only
 
so
 
many
 
things
 
that
 
you
 
could
 
do
 
a
 
priority .
 
And
 
so
 
you'll
 
always
 
have
 
to
 
do
 
on-the-spot
 
thinking
 
and
 
in
 
context
 
thinking
 
and
 
a
 
reﬂection.
 
And
 
so
 
I
 
think
 
that
 
the
 
fact
 
that
 
all
 
3
 
are
 
scaling
 
is
 
actually
 
very
 
sensible
 
based
 
on
 
wher e
 
we
 
are.
 
And
 
in
 
the
 
area
 
foundation
 
model,
 
now
 
we
 
have
 
multimodality
 
foundation
 
models
 
and
 
the
 
amount
 
of
 
petabyt es
 
video
 
that
 
these
 
foundation
 
models
 
are
 
going
 
to
 
be
 
trained
 
on,
 
it's
 
incredible.
 
And
 
so
 
my
 
expectation
 
is
 
that
 
for
 
the
 
foreseeable
 
future,
 
we're
 
going
 
to
 
be
 
scaling
 
pretraining,
 
post-tr aining
 
as
 
well
 
as
 
inference
 
time
 
scaling
 
and
 
which
 
is
 
the
 
reason
 
why
 
I
 
think
 
we're
 
going
 
to
 
need
 
more
 
and
 
more
 
comput e.
 
And
 
we're
 
going
 
to
 
have
 
to
 
drive
 
as
 
hard
 
as
 
we
 
can
 
to
 
keep
 
increasing
 
the
 
performance
 
by
 
X
 
factors
 
out
 
of
 
time
 
so
 
that
 
we
 
can
 
continue
 
to
 
drive
 
down
 
the
 
cost
 
and
 
continue
 
to
 
increase
 
the
 
revenues
 
and
 
get
 
the
 
AI
 
revolution
 
going.
 
Oper ator
 
Thank
 
you.
 
I'd
 
like
 
to
 
turn
 
the
 
call
 
back
 
over
 
to
 
Jensen
 
Huang
 
for
 
closing
 
remarks.
 
Jensen
 
Huang
 
Thank
 
you.
 
The
 
tremendous
 
growth
 
in
 
our
 
business
 
is
 
being
 
fueled
 
by
 
2
 
fundamental
 
trends
 
that
 
are
 
driving
 
global
 
adoption
 
of
 
NVIDIA
 
computing.
 
First,
 
the
 
computing
 
stack
 
is
 
under going
 
a
 
reinvention,
 
a
 
platform
 
shift
 
from
 
coding
 
to
 
machine
 
learning,
 
from
 
executing
 
code
 
on
 
CPUs
 
to
 
processing
 
neural
 
networks
 
on
 
GPUs.
 
The
 
$1
 
trillion
 
installed
 
base
 
of
 
traditional
 
data
 
center
 
infrastructur e
 
is
 
being
 
rebuilt
 
for
 
Softwar e
 
2.0,
 
which
 
applies
 
machine
 
learning
 
to
 
produce
 
AI.
 
Second,
 
the
 
age
 
of
 
AI
 
is
 
in
 
full
 
steam.
 
Gener ative
 
AI
 
is
 
not
 
just
 
a
 
new
 
softwar e
 
capability
 
but
 
a
 
new
 
industr y
 
with
 
AI
 
factories
 
manufacturing
 
digital
 
intelligence,
 
a
 
new
 
industrial
 
revolution
 
that
 
can
 
create
 
a
 
multi-trillion-dollar
 
AI
 
industr y.
 
Demand
 
for
 
Hopper
 
and
 
anticipation
 
for
 
Blackwell,
 
which
 
is
 
now
 
in
 
full
 
production,
 
are
 
incredible
 
for
 
sever al
 
reasons.
 
There
 
are
 
more
 
foundation
 
model
 
makers
 
now
 
than
 
there
 
were
 
a
 
year
 
ago.
 
The
 
computing
 
scale
 
of
 
pretraining
 
and
 
post-tr aining
 
continues
 
to
 
grow
 
exponentially .
 
There
 
are
 
more
 
AI
 
native
 
startups
 
than
 
ever
 
and
 
the
 
number
 
of
 
successful
 
inference
 
services
 
is
 
rising.
 
And
 
with
 
the
 
introduction
 
of
 
ChatGPT
 
o1,
 
OpenAI
 
o1,
 
a
 
new
 
scaling
 
law
 
called
 
test
 
time
 
scaling
 
has
 
emer ged.
 
All
 
of
 
these
 
consume
 
a
 
great
 
deal
 
of
 
computing.
 
AI
 
is
 
transforming
 
every
 
industr y,
 
company,
 
and
 
countr y.
 
Enterprises
 
are
 
adopting
 
agentic
 
AI
 
to
 
revolutioniz e
 
workﬂows.
 
Over
 
time,
 
AI
 
cowork ers
 
will
 
assist
 
employees
 
in
 
performing
 
their
 
jobs
 
faster
 
and
 
better.
 
Investments
 
in
 
industrial
 
robotics
 
are
 
surging
 
due
 
to
 
breakthr oughs
 
in
 
physical
 
AI,
 
driving
 
new
 
training
 
infrastructur e
 
demand
 
as
 
resear chers
 
train
 
world
 
foundation
 
models
 
on
 
petabyt es
 
of
 
video
 
and
 
Omniverse
 
synthetically
 
gener ated
 
data.
 
The
 
age
 
of
 
robotics
 
is
 
coming.
 
Countries
 
across
 
the
 
world
 
recogniz e
 
the
 
fundamental
 
AI
 
trends
 
we
 
are
 
seeing
 
and
 
have
 
awak ened
 
to
 
the
 
impor tance
 
of
 
developing
 
their
 
national
 
AI
 
infrastructur e.
 
The
 
age
 
of
 
AI
 
is
 
upon
 
us
 
and
 
it's
 
large
 
and
 
diverse.
 
NVIDIA's
 
expertise,
 
scale,
 
and
 
ability
 
to
 
deliver
 
full
 
stack
 
and
 
full
 
infrastructur e
 
lets
 
us
 
serve
 
the
 
entire
 
multi-trillion-dollar
 
AI
 
and
 
robotics
 
oppor tunities
 
ahead
 
from
 
every
 
hyperscale
 
cloud,
 
enterprise
 
privat e
 
cloud
 
to
 
sover eign
 
regional
 
AI
 
clouds,
 
on-pr em
 
to
 
industrial
 
edge
 
and
 
robotics.
 
Thanks
 
for
 
joining
 
us
 
today,
 
and
 
catch
 
up
 
next
 
time.
 
Oper ator
 
This
 
concludes
 
today's
 
confer ence
 
call.
 
You
 
may
 
now
 
disconnect.
 
 
